"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nar\nX\n\niv\n:2\n\n10\n1.\n\n10\n19\n\n7v\n2 \n\n [\nco\n\nnd\n-m\n\nat\n.s\n\nup\nr-\n\nco\nn]\n\n  2\n6 \n\nJa\nn \n\n20\n21\n\nEffects of spin orbit coupling in superconducting proximity devices \u2013 application to\nCoSi2/TiSi2 heterostructures\n\nVivek Mishra,1 Yu Li,1 Fu-Chun Zhang,1, 2, \u2217 and Stefan Kirchner3, 4, \u2020\n\n1\nKavli Institute for Theoretical Sciences, University of Chinese Academy of Sciences, Beijing 100190, China\n\n2\nCAS Center for Excellence in Topological Quantum Computation,\n\nUniversity of Chinese Academy of Sciences, Beijing 100190, China\n3\nZhejiang Institute of Modern Physics & Department of Physics, Zhejiang University, Hangzhou 310027, China\n\n4\nZhejiang Province Key Laboratory of Quantum Technology and Device, Zhejiang University, Hangzhou 310027, China\n\n(Dated: January 27, 2021)\n\nMotivated by the recent findings of unconventional superconductivity in CoSi2/TiSi2 heterostruc-\ntures, we study the effect of interface induced Rashba spin orbit coupling on the conductance of\na three terminal \u201cT\u201d shape superconducting device. We calculate the differential conductance for\nthis device within the quasi-classical formalism that includes the mixing of triplet-singlet pairing\ndue to the Rashba spin orbit coupling. We discuss our result in the light of the conductance spectra\nreported by Chiu et al.for CoSi2/TiSi2 heterostructures.\n\nIntroduction\u2013 The search for platforms that can host\nMajorana zero modes (MZMs) has been one of the ma-\njor topics driving current condensed matter research as\nMZMs, being localized quasiparticles that obey non-\nAbelian braiding statistics, are the essential ingredient\nfor topological quantum computing1\u20136. Early proposals\nfor creating MZMs involve spin triplet superconductivity\nwhile almost all known superconductors belong to the\nspin singlet class with a few possible exceptions such as\nUPt3. As a result, a variety of ingenious heterostructures\nof superconducting nano-wires have been proposed and\nobserved to generate the required p-wave pairing compo-\nnent, taking advantage of broken time-reversal and inver-\nsion symmetry7\u201310. MZM has also been proposed inside a\nvortex of topological superconductor11; the experimental\nobservation of MZM in some of the iron-based supercon-\nductors is along this line12,13. Fu and Kane14 proposed\nthat proximity of s-wave superconductor on surface of\n3D topological insulator may serve for the same purpose\nto generate MZMs, due to the spin-momentum locking.\nTheir proposal has been confirmed experimentally15.\n\nTo distinguish between singlet and triplet supercon-\nductors, in addition to nuclear magnetic resonance16 and\n\u00b5-spin rotation probe, a T-shaped proximity structure\njunction was proposed to probe into the presence of\ntriplet superconductivity17. The proposed device con-\nsists of two normal metal wires combined to form the\nletter \u2018T\u2019. This three-terminal device is connected to a\nsuperconductor at the free end of the leg, see Fig. 1. As\nshown in Ref.17, a chiral p-wave or an ordinary p-wave\nstate give a zero bias conductance peak (ZBCP) in re-\nsponse to a bias voltage between the open ends of the\nbar of the \u2018T\u2019.\n\nRecent experimental results reported by Chiu et al.\nhave been argued to be consistent with the occurrence of\nchiral p-wave pairing in CoSi2/TiSi2 heterostructures\n\n18.\nChiu et al. support their claim with conductance\nspectroscopy data of CoSi2/TiSi2 superconductor-normal\nmetal (SN) tunnel junctions. In these heterostructures,\nCoSi2 is the superconducting component which becomes\n\nsuperconducting below 1.5 K. The conductivity of the SN\ntunnel junctions agrees with the theoretical calculations\nbased on the Blonder-Tinkham-Klapwijk (BTK) model\nfor a chiral p-wave superconductor19. However, there is\na sharp zero bias peak in the conductance spectra of the\nSN junction, which cannot be described within the BTK\ntheory.\n\nChiu et al. further substantiate their interpretation\nwith conductance spectra based on three terminal T-\nshaped proximity devices similar to the one sketched in\nFig. 1, which again show ZBCPs. As noted in Ref.18, a\ndistinction between ordinary and chiral p-wave supercon-\nductor solely based on experimental conductance spectra\nis hardly feasible. The observation of hysteresis behavior\nin the magnetoresistance below superconducting transi-\ntion temperature Tc of the CoSi2/TiSi2 junctions, how-\never, further vindicates their claim of chiral p-wave in the\nCoSi2/TiSi2 heterostructures. The findings of Ref. 18\nare intriguing for a few reasons. The superconductivity\nin CoSi2 was discovered in 1952\n\n20, the theoretical esti-\nmate of Tc based on phonon mediated pairing appears to\nagree well with the experimental Tc\n\n21. The specific heat\ndata below superconducting state suggests conventional\ns-wave pairing22. This material does not appear to be lo-\ncated in the vicinity of magnetism, therefore there is no\nreason to expect it to be a chiral p-wave superconductor,\nat least in the bulk limit.\n\nIt is worth noting that a strong spin-orbit coupling\n(SOC), exceeding the superconducting gap of CoSi2 by\nmore than a factor 30, has been reported in CoSi2 by\nthe same group18. Having in mind this strong SOC,\nwe propose the substrate induced Rashba SOC as a\nsource of p-wave pairing in this system. It is known\nthat SOC induced pairing does not break the time rever-\nsal symmetry23, and the presence of the SOC also leads\nto mixing of the triplet and the singlet components. In\nthe context of noncentrosymmetric superconductors, the\ntunneling conductance for SN junctions has been stud-\nied in systems with SOC24\u201328. However, the effect of the\nSOC and the conductance for a mixed parity supercon-\n\nhttp://arxiv.org/abs/2101.10197v2\n\n\n2\n\nFIG. 1. Schematic illustration of a T-shaped junction. The\nthree terminal proximity device consists of a diffusive normal\nmetal (NM) part attached to a superconductor (SC). The\nblue area indicates the diffusive normal metal (NM) part of\nthe device, while the orange area shows the SC part (color\nonline).\n\nductor in a T-junction device is not known. The origi-\nnal T-junction study did not include the SOC, and the\nZBCP for a chiral p-wave superconductor is expected to\nbe weak17.\n\nIn this paper, we investigate the effect of the singlet-\ntriplet mixing on the conductance spectra of the T-\nshaped junctions. In the context of CoSi2/TiSi2 het-\nerostructures, we focus on substrate induced SOC in a\nsuperconductor, which results in the \u201csp\u201d pairing state,\nwhere the singlet component has s-wave symmetry and\nthe triplet component has p-wave symmetry. In princi-\nple, triplet and singlet components can have anisotropic\nstructures due to the orbital form factors and because\nthe bands crossing the Fermi energy derive from the 3d\norbitals of Co21. Thus, we also consider the \u201cdf\u201d and\n\u201cdp\u201d pairing states, which have additional orbital form\nfactors compatible with dx2\u2212y2 and dxy functions, which\nlead to dx2\u2212y2 and dxy structures for the singlet compo-\nnents and effectively f -wave and p-wave like structures\nfor the triplet components, respectively. The df state has\nbeen proposed for a few heavy electron noncentrosym-\nmetric systems29,30 and the dp state has been suggested\nfor LaAlO3/SrTiO3 heterointerfaces\n\n26.\n\nModel & Formalism\u2013 We model the CoSi2/TiSi2 T-\nshaped junction of Ref.18 in terms of the two-dimensional\nproximity devices depicted schematically in Fig. 1. The\ntransport in the normal metal (NM) part is assumed to\nbe diffusive which is the experimentally relevant regime.\nThe height (d) and the width (w) are very small com-\npared to its length (Lx/y) in either direction and its di-\nmensions are assumed to be very small compared to the\ncoherence length \u03be0 \u2261 h\u0304vF /\u03c0\u2206 (i.e., w, d \u226a \u03be0), where\nvF and \u2206 are the Fermi velocity and the superconduc-\ntivity gap, respectively. Within the ambit of these as-\nsumptions, this structure can be thought of a set of two\none dimensional wires joined to form the shape of the\nletter \u2018T\u2019. The leg of this T-shaped junction is attached\nto a clean superconductor. The ends of the horizontal\n\nsection of this junction are subjected to a bias voltage\n(eV ). We consider the case where the SOC exists in the\nsuperconducting component of this structure due to its\nbroken inversion symmetry.\nThe kinetic part of the Hamiltonian reads,\n\nHk = \u03bek +HSOC,k. (1)\n\nHere \u03bek is the electronic dispersion relation for the\nfermions and the SOC term is,\n\nHSOC,k = \u03b1 (\u03c3 \u00d7 k) \u00b7 z\u0302 = \u03b1Ak \u00b7 \u03c3, (2)\nwhere \u03b1 is the Rashba SOC coupling constant, m is the\neffective mass and \u03c3 is (\u03c3x, \u03c3y, \u03c3z), where \u03c3x/y/z are the\nPauli matrices in spin space. We consider the Rashba\nSOC, that is induced along the growth direction, which\nis chosen to be the z\u0302 direction in the T-shaped junction.\nIn this case, the normal to the relevant interface is along\nthe z\u0302 axis and the SOC vector is,\n\nAk = (ky,\u2212kx, 0) = |k|(sin\u03c6k,\u2212 cos\u03c6k, 0). (3)\n\nwhere \u03c6k is the angle in the two dimensional momentum\nspace.\nDiagonalizing the Hamiltonian results in a splitting of\n\nthe original band into two helical bands with different\nspin structures. The energies of these two bands are\n\u03bek \u00b1 \u03b1|k|. The difference in the density of states and\nthe Fermi velocities are of the order of \u03b1pF /EF , where\npF and EF are the Fermi momentum and the Fermi en-\nergy of the original band. For realistic systems, the SOC\nenergy is generally very small compared to the Fermi\nenergy. Therefore, we ignore this difference in the den-\nsity of states and the Fermi velocities between the helical\nbands and we take these parameters to be the same as\nthe original band for our subsequent calculations.\nWe assume that the superconducting component is\n\nconfined in the two-dimensional plane and that it has\ndimensions that are very large compared to the coher-\nence length, hence we treat it like a homogeneous system,\nand ignore any kind of inverse proximity effect due to the\njunction formation. We adopt the quasi-classical Keldysh\nformalism to carry out the conductance calculations31,\nwhere the quasi-classical Green\u2019s function consists of re-\ntarded, advanced and Keldysh components. Each of\nthese components is a 4 \u00d7 4 matrix in the Nambu-spin\nspace. We denote the 4\u00d74 Green\u2019s function in this space\nwith .\u030c.. and .\u0302.. denotes the 2 \u00d7 2 Green\u2019s functions in\nthe spin basis. The advanced and Keldysh components\ncan be obtained from the retarded component, on which\nwe focus in the following. Following Ref. 32, the quasi-\nclassical retarded Green\u2019s function in a superconductor\nwithout inversion symmetry can be expressed as,\n\ng\u030c =\n\n(\n\ngI\u03c3I + gII\u03c3II \u2212 (fI\u03c3I + fII\u03c3II) i\u03c3y\n\u2212i\u03c3y\n\n(\n\nf\u0304I\u03c3I + f\u0304II\u03c3II\n)\n\n\u03c3y (g\u0304I\u03c3I + g\u0304II\u03c3II)\u03c3y\n\n)\n\n, (4)\n\nwhere \u03c3I/II = (\u03c30 \u00b1Ak \u00b7 \u03c3)/2, gI/II(\u03b5) = \u03b5/\n\u221a\n\n\u03b52 \u2212\u22062\nI/II\n\n,\n\nfI/II(\u03b5) = \u2206I/II/\n\u221a\n\n\u03b52 \u2212\u22062\nI/II\n\n, g\u0304I/II = \u2212gI/II and f\u0304I/II =\n\n\n\n3\n\nfI/II. The general gap structure for a system with the\n\nSOC is33,\n\n\u2206\u0302 = (\u2206s\u03a6s(\u03c6k) + \u2206t\u03a6t(\u03c6k)Ak \u00b7 \u03c3) i\u03c3y. (5)\n\nHere the SOC vector Ak acts like the d-vector, and \u2206s\n(\u2206t) is the gap magnitude of the singlet (triplet) com-\nponent. The gaps on two helical bands are \u2206I/II =\n\u2206s\u03a6s\u00b1\u2206t\u03a6t. The angular anisotropy of the gaps are em-\nbedded in \u03a6s and \u03a6t. The simplest case is \u03a6s = \u03a6t = 1,\nwhich is referred as sp-state, where the singlet compo-\nnent is an isotropic s-wave state, and the triplet com-\nponent has p-wave structure. Such states have been\nproposed for various non-centrosymmetric superconduc-\ntors. Apart from this, the other possibilities are the\ndf -state, where \u03a6s = \u03a6t = cos 2\u03c6k, and the dp-state with\n\u03a6s = \u03a6t = sin 2\u03c6k. We focus on sp-state, which is more\nrelevant in the context of CoSi2/TiSi2 heterostructures.\nThe gap function is parameterized as\n\n\u2206\u0302 = \u22060\n\n(\n\n1\u221a\n1 + r2\n\n+\nr\u221a\n\n1 + r2\nAk \u00b7 \u03c3\n\n)\n\ni\u03c3y. (6)\n\nwhere \u22060 is\n\u221a\n\n\u22062s +\u2206\n2\nt\n34. Here the parameter r \u2208 [0,\u221e]\n\nis the ratio of triplet to singlet component.\nThe Cooper pairs from the superconducting side can\n\ntunnel into the diffusive normal metal (NM), and this ef-\nfect is included through the boundary conditions, which\nare used to solve the Usadel equations on the NM side.\nWe treat the barrier between the NM and the supercon-\nductor as a spin-independent barrier. This assumption is\njustified because the SOC is very small compared to the\nFermi energy27. We first calculate the retarded compo-\nnent of the quasi-classical Green\u2019s function g\u030cRn , and then\nconstruct the advanced and the Keldysh components us-\ning g\u030cRn . The subscript n denotes the normal metal. The\nUsadel equations for g\u030cRn are\n\nD\u2202\u2113(g\u030c\nR\nn \u2202\u2113g\u030c\n\nR\nn ) + i\n\n[\n\n\u03b5\u03c4\u030c3, g\u030c\nR\nn\n\n]\n\n= 0. (7)\n\nwhere D is the diffusion constant of the normal\nmetal, \u2113 denotes the spatial directions x/y, and \u03c4\u030c3 is\ndiag(1, 1,\u22121,\u22121). The normalization condition for the\nquasi-classical Green\u2019s function is,\n\ng\u030cRn g\u030c\nR\nn = 1\u030c. (8)\n\nThese equations are supplemented by the boundary con-\nditions,\n\ng\u030cRn = \u03c4\u030c3, (9)\n\ng\u030cRn\u2207\u2202y g\u030cRn = 0\u030c, (10)\n\nwhere the last condition reflects current conservation35.\nThe boundary condition at (0, Ly) depends on the nature\nof the gap in the superconductor. We use the boundary\ncondition derived by Nazarov for interfaces with arbi-\ntrary transparency36, which was generalized for uncon-\nventional superconductors by Tanaka et al37\u201339. In this\n\napproach, the interface is modeled as a \u03b4 function po-\ntential barrier H\u03b4(y \u2212 Ly), which has the transmission\nprobability,\n\nT (\u03c6) =\n4 cos2 \u03c6\n\n4 cos2 \u03c6+ Z2\n, (11)\n\nwhere \u03c6 is the angle measured with respect to the nor-\nmal to the interface, which is y axis in the geometry we\nconsider, and Z is a dimensionless parameter given by\nZ = 2mH/k2F . Here m is the effective mass and kF is\nthe Fermi momentum. A large value of Z gives an inter-\nface with poor transparency, whereas Z = 0 characterizes\na transparent interface. The boundary condition at the\nSN interface can be expressed as,\n\nLy g\u030cn\n\u2202g\u030cn\n\u2202y\n\n\u2223\n\n\u2223\n\n\u2223\n\ny=Ly\n= 2\u0393\n\n\u2329\n\n[g\u030cn, B\u030c(\u03c6)]\n\u232a\n\n\u03c6\n, (12)\n\nwhere g\u030cn at the right hand side of Eq. (12) is the quasi-\nclassical Green\u2019s function in the NM region evaluated at\n(0, Ly), and \u0393 is the ratio of the normal metal resistance\nRN , and the interface resistance RB. The angular aver-\nage at the right hand side of Eq.(12) is defined as,\n\n\u3008...\u3009\u03c6 =\n\u222b \u03c0/2\n\n\u2212\u03c0/2\nd\u03c6(...) cos \u03c6\n\n\u222b \u03c0/2\n\n\u2212\u03c0/2\nd\u03c6T (\u03c6) cos\u03c6\n\n(13)\n\nNote, the angle \u03c6 in the boundary condition is measured\nwith respect to the interface normal. The matrix function\nB\u030c in Eq. (12) is,\n\nB\u030c(\u03c6) =\n[\n\n\u2212T \u2032[g\u030cn, H\u030c\u22121\u2212 ] + H\u030c\u22121\u2212 H\u030c+ \u2212 T \u20322g\u030cnH\u030c\u22121\u2212 H\u030c+\n]\u22121\n\n\u00d7\n[\n\n\u2212T \u2032(1\u030c+ H\u030c\u22121\u2212 ) + T \u20322g\u030cnH\u030c\u22121\u2212 H\u030c+\n]\n\n, (14)\n\nT \u2032(\u03c6) =\nT (\u03c6)\n\n2\u2212 T (\u03c6) + 2\n\u221a\n\n1\u2212 T (\u03c6)\n, (15)\n\nH\u030c\u00b1 =\n1\n\n2\n[g\u030c(\u03c6) \u00b1 g\u030c(\u03c0 \u2212 \u03c6)] . (16)\n\nNote, the boundary condition itself depends on the solu-\ntion at the boundary. To calculate the differential con-\nductance, we first calculate the current. For the current\ncalculation, we need the Keldysh component of the quasi-\nclassical Green\u2019s function,\n\ng\u030cK = g\u030cRh\u030c\u2212 h\u030cg\u030cA, (17)\n\nwhere the advanced component is,\n\ng\u030cA = \u2212\u03c4\u030c3(g\u030cR)\u2020\u03c4\u030c3, (18)\n\nand the spin resolved distribution function h\u030c is a diagonal\nmatrix diag(fL\u2191+fT\u2191, fL\u2193+fT\u2193, fL\u2191\u2212fT\u2191, fL\u2193\u2212fT\u2193)40,\nwhere fT\u03bd and fL\u03bd are the transverse and the longitu-\ndinal distribution functions and \u03bd is the spin index. In\nthe T-shaped junction, a bias voltage (V ) is applied at\nx = +Lx and at the other end the voltage is kept at\n\n\n\n4\n\n-0.1 -0.05 0 0.05 0.1\n1\n\n1.2\n\n1.4\n\n1.6\n\n1.8\n\n2\n\n2.2\n\n2.4\n\n-0.1 -0.05 0 0.05 0.1\n1\n\n1.2\n\n1.4\n\n1.6\n\n1.8\n\n2\n\n2.2\n\n2.4\n\n-0.1 -0.05 0 0.05 0.1\n1\n\n1.2\n\n1.4\n\n1.6\n\n1.8\n\n2\n\n2.2\n\n2.4\n\nFIG. 2. Differential conductance for a T-shaped junction attached to a superconductor under the influence of Rashba SOC.\nThe parameter r is the ratio of triplet to singlet component of the order parameter. The interface quality parameter \u0393 is set\nto 20. The ratio of lengths along the two spatial direction Ly/Lx is 0.1, 0.5 and 1 in the panels (a), (b) and (c), respectively.\nThe differential conductance is calculated in the zero temperature limit.\n\nzero. Therefore, the equilibrium spin-resolved distribu-\ntion functions at these two ends are,\n\nfT\u2191/\u2193\n\n\u2223\n\n\u2223\n\n\u2223\n\nx=Lx,y=0\n=\n\n1\n\n2\n\n[\n\nnf (\n\u03b5\u2212\n2T\n\n)\u2212 nf (\n\u03b5+\n2T\n\n)\n]\n\n, (19)\n\nfT\u2191/\u2193\n\n\u2223\n\n\u2223\n\n\u2223\n\nx=\u2212Lx,y=0\n= 0. (20)\n\nHere nf is the Fermi-Dirac distribution function and\n\u03b5\u00b1 = \u03b5\u00b1 eV . The transverse component of the distribu-\ntion function will be the same for both spin components\nat the normal electrode. The charge current density is,\n\nJE(x, T ) =\neN0D\n\n8\n\n\u222b \u221e\n\n\u2212\u221e\n\nd\u03b5Tr\n[\n\n\u03c4\u03023(g\u030c\nR\u2202xg\u030c\n\nK + g\u030cK\u2202xg\u030c\nA)\n\n]\n\n.\n\n(21)\nHere N0 is the total density of states at the Fermi level.\nThe differential conductance can be obtained by evalu-\nating the derivative of the charge current density w.r.t.\nthe bias voltage \u2013 we numerically solve the Usadel equa-\ntions in the normal metal and with the aforementioned\nboundary conditions. Since the boundary condition at\nthe SN interface involves the solutions at the interface,\nso we start with a guess solution and obtain the final\nsolution self-consistently.\nResults & Discussion\u2013 We consider a good interface\n\nbetween NM and SC and fix \u0393 at a value of 20. The\ninterface barrier parameter Z is set to 2. A larger value\nof \u0393 represents a good quality surface, which is essential\nfor the formation of a sizable proximity effect. Figure 2\nshows the differential conductance for the T-shaped de-\nvice, where the superconducting portion is under the in-\nfluence of the substrate induced Rashba SOC for several\nvalues of the parameter r indicating the relative strength\nof the triplet component. The magnitude of the gap is\n0.05Eth, where Eth is the Thouless energy for the half\nwire along the x\u0302 direction i.e. Eth \u2261 h\u0304D/L2x. For the\nproximity problem, the characteristic energy scale in the\nNM is the Thouless energy, which is inversely propor-\ntional to the square of the device length. A smaller de-\nvices is usually better for observing proximity effect re-\nlated physics.\n\nFor large values of r, the triplet component dominates.\nIn this regime we find that the differential conductance\nis similar to the p-wave case17. In general, for a three\ndimensional system, a z\u0302 Rashba SOC gives \u2206s\u00b1\u2206t sin \u03b8,\nwhere \u03b8 is the polar angle. Therefore, a triplet dominated\nsystem will have horizontal line nodes. However, in our\nstudy we consider a two-dimensional system, where the\ngaps on the two helical bands are \u2206s \u00b1\u2206t. Thus, in the\ntriplet dominant limit (r > 1), we have isotropic unequal\ngaps on two bands with opposite chiralities. A zero bias\nconductance peak (ZBCP) is expected for a chiral p-wave\nsuperconductor17, however it is expected to be weaker\nthan a p-wave system. We find that the height of the peak\nis comparable to that of a p-wave system. Unlike in a chi-\nral p-wave superconductors, the time-reversal symmetry\nis not broken in the case considered here. The origin of\nthe peak is the symmetry of the induced pairing in NM.\nIn the diffusive metal, the isotropic s-wave state can sur-\nvive due to impurity scattering, which kills any other kind\nof superconducting state. In the superconducting side\nof the junction, both triplet and singlet components are\neven functions of frequency. Therefore, the triplet com-\nponent leaks odd frequency, even parity and spin triplet\npairs, and the odd frequency nature of these induced\npair gives rise to a ZBCP41,42. In the case of two he-\nlical bands with opposite chirality triplet state, the spec-\ntral weight of the ZBCP is larger than that expected for\na chiral superconductor. The ZBCP becomes sharper as\nthe length of the leg (Ly) attached to the superconductor\nincreases, therefore a T-shaped junction with a shorter\nleg provides a better chance of ZBCP detection. We find\nthat a ZBCP forms as long as the triplet component is\nstronger. For the special case of r = 1, when triplet and\nsinglet components are equal, we still find a ZBCP in the\ndifferential conductance albeit with a reduced height and\nwidth. Since one of the bands has a zero gap in this limit,\nthe height of the peak decreases \u2013 the origin of the re-\nduced width in the ZBCP for smaller r is the presence of\neven frequency, spin singlet and even parity pairs, which\ncomes from the singlet component in the mixed parity\n\n\n\n5\n\n-0.01 -0.005 0 0.005 0.01\n1\n\n1.1\n\n1.2\n\n1.3\n\n1.4\n\n1.5\n\nFIG. 3. The differential conductance for a T-shaped junc-\ntion of a dp-superconductor for several values of r with larger\nsinglet component. The length along the y\u0302 direction is 0.5Ly .\n\nsuperconducting state. Such pairs reduce the density of\nstates at the Fermi level, which reduces the conductiv-\nity. However, induced pairs also increase conductivity in\nthe diffusive metal, this increase comes through Maki-\nThompson like process43. These two counter effects can-\ncel at zero energy. The finite energy maximum in the\nconductivity near the Thouless energy scale arises due to\ndifferent decay patterns of these two effects. The nega-\ntive contribution from the loss of density of states decays\nexponentially, while the Maki-Thompson like contribu-\ntion decays non-exponentially over the energy scale of\nEth. These two opposite contributions result in a dip\nin the limit of pure singlet superconductor (r \u226a 1) in\nthe T-shaped junction. In the singlet dominated regime\n(0 < r < 1), we find both a dip from the singlet com-\nponent and a weak ZBCP from the triplet component.\nFigure 3 shows evolution of the conductance peak to a\ndip in the strong singlet limit. The width and height\nof the ZBCP decreases rapidly with diminishing triplet\ncomponent.\n\nNext, we consider df and dp states which possess\nanisotropic orbital components. For the df -state, \u03a6s\nand \u03a6t are modeled by cos 2\u03c6k. For this gap function,\nthere are two line nodes at an angle \u00b1\u03c0/4 w.r.t. the\ninterface normal y\u0302 axis. Since we have already shown\nthat a T-shaped junction with shorter leg length is bet-\nter for observing the ZBCP, we fix the value of Ly at\n0.5Lx and consider a good quality interface with \u0393 = 20\nand Z = 2 for our differential conductance calculations\nwith anisotropic form factors. Fig. 4 shows the dif-\nferential conductance for a T-shaped junction attached\nto df -symmetry superconductor. We find qualitatively\nsimilar behavior for a df -superconductor to that of a\nsp-superconductor which was discussed above. This qual-\nitative similarity between sp and df superconductors can\nbe understood by examining the phase shift in the gap\n\n-0.1 -0.05 0 0.05 0.1\n1\n\n1.2\n\n1.4\n\n1.6\n\n1.8\n\n2\n\nFIG. 4. The differential conductance for a T-shaped junction\nof a df -superconductor for several values of r. The length\nalong the y\u0302 direction is 0.5Ly . The orientation of dx2\u2212y2\norbital form factor is shown in the main figure. The leg of the\nT-junction is taken along the y\u0302 direction, and the voltage is\napplied along the x\u0302 direction.\n\nfunctions of incoming and outgoing quasiparticle trajec-\ntories at the interface. For the dx2\u2212y2 orbital function,\nthe incoming \u2206(\u03c6) and the outgoing \u2206(\u03c0\u2212\u03c6) are qualita-\ntively the same as sp- superconductor. The nodal line of\nthe dx2\u2212y2 form factor is at an angle of \u00b1\u03c0/4, so there is\nno additional sign change due to this anisotropic factor,\nand the triplet component is effectively the same as for\nthe sp- superconductor. However, the dx2\u2212y2 form factor\nreduces the height of the ZBCP in the triplet dominated\nregime (r \u2265 1), and in the strong singlet regime r < 1,\nthe tiny peak that we find for the sp superconductors is\nsmeared and the lineshape is similar to a s-wave super-\nconductor.\nIn contrast, we find a qualitatively different behavior\n\nfor the dp state, as shown in the Fig. 5, there is a split-\nting of the ZBCP with reduced heights. For a dxy orbital\nfactor, the nodal line is along the interface normal, there-\nfore for all the incoming gap functions dxy form factor\ngives a sign change to the outgoing gap function. The\ntriplet component has an additional chiral p-wave factor,\nwhich also gives a sign change between incoming and\noutgoing gap functions, which gets canceled by the sign\nchange from the dxy factor, hence there is no overall sign\nchange. This is qualitatively equivalent to an extended\ns-wave state. For triplet dominant cases, a ZBCP with\nsplitting is the outcome of this lack of sign change. In\ncase of a s-wave superconductor, two opposite contribu-\ntions to the conductivity exist: while the loss of density\nof states reduces the conductivity, Maki-Thompson like\nprocesses result in an enhancement. For isotropic s-wave\nsystems, these two effects cancel at zero energy. How-\never, the additional anisotropy from the orbital form fac-\ntor may not give an exact cancellation, this could lead to\nan increased conductivity at the zero energy in compari-\n\n\n\n6\n\nFIG. 5. The differential conductance for a T-shaped junction\nof a dp-superconductor for several values of r. The length\nalong the y\u0302 direction is 0.5Ly . The orientation of dxy orbital\nform factor is shown in the main figure. The leg of the T-\njunction is taken along the y\u0302 direction, and the voltage is\napplied along the x\u0302 direction.\n\nson with a pure isotropic s-wave superconductor. In the\nlimit of strong singlet component, we find a featureless\nconductivity. This is expected because the nodal line in\nparallel to the interface normal, and in such orientation\nno proximity effect occur44.\n\nConcluding Remarks\u2013In this paper, we have studied\nthe conductance of T-shaped junctions connected to a su-\nperconductor under the influence of a strong Rashba SOC\ngenerated by the underlying substrate. The d-vector in\nthe superconducting state is determined by the SOC. The\nsuperconducting state is a mixed parity state with both\nsinglet and triplet components. We calculated the tun-\nneling conductance for this system within the quasiclassi-\ncal formalism. The effect of the superconducting order is\nincluded through Nazarov-Tanaka boundary conditions.\nWe looked at the effect of the device size on the zero-\nbias conductance peak. In agreement with earlier work,\nwe find that smaller device dimensions result in larger\nFWHM of the ZBCPs. Moreover, we showed that both\ntriplet and singlet components affect the conductance.\n\nSpecifically, we considered sp, df and dp pairing states.\nThe sp and df states produce ZBCPs, whenever the\ntriplet component is stronger than the singlet one. The\npeak is weaker in the case of df superconductors due to\nanisotropic dx2\u2212y2 orbital form factor. In the strong sin-\nglet limit, we find a dip structure in the conductance\nspectrum. For the sp-state, in the regime where a finite\nbut small triplet component co-exists with a large sin-\nglet component, we predict a weak ZBCP on top of the\ndip structure. This ZBCP disappears quickly with the\ndecreasing triplet strength. For the df -state, the weak\nZBCP disappears rapidly already when the triplet com-\nponent becomes smaller than the singlet component. In\ncontrast, we find a ZBCP splitting for the dp-state, which\n\nhappens because the triplet component does not cause a\nsign change of the incoming and outgoing gaps. Thus,\nwe conclude that making interfaces with different crys-\ntallographic orientations of the superconductor will be\nuseful for drawing concrete conclusions in systems, where\nanisotropic orbital form factors are likely to be present.\nIn the context of the recent experimental results on\n\nCoSi2/TiSi2 heterostructures\n18, we believe that the sp\n\nstate is consistent with the experiments. We have per-\nformed our calculations for device sizes that are compa-\nrable to the experimental setup. We found that in the\ntriplet dominant regime, the opposite chirality supercon-\nductivity on the two helical bands gives a ZBCP in the\nconductance of the T-junction. This peak is quite robust\nand stronger than the peak expected for a usual chi-\nral p-wave superconductor17. Therefore, we think that\nthe CoSi2/TiSi2 heterostructure is a triplet dominant\n(\u2206singlet < \u2206triplet) superconductor. The conductance\nfor an SN junction comprised of such a triplet domi-\nnant mixed parity sp superconductor and a normal metal\njunction has been studied earlier24, and agrees with the\nCoSi2/TiSi2 tunnel junction data barring the sharp fea-\nture at the zero energy.\nOne of the major issue with our description of the\n\nCoSi2/TiSi2 heterostructures is the lack of the time rever-\nsal symmetry breaking (TRSB) that has been observed\nup to Tc. The TRSB in the mixed parity superconductors\nhas been predicted earlier,45,46 however it is expected to\nhappen at a lower temperature below Tc. Twin bound-\naries can also cause TRSB, if the triplet and the sin-\nglet components are comparable in magnitudes47. An-\nother possible explanation for the hysteresis observed in\nthe magnetoresistance data is the Zeeman field induced\nsupercurrent. In a superconductor with broken inver-\nsion symmetry, an in-plane Zeeman field gives rise to a\nsupercurrent flow along the direction perpendicular to\nit48,49. We think that \u00b5SR experiments on CoSi2/TiSi2\nheterostructures will provide an ubiquitous evidence for\nTRSB.\nWe have considered a simple one band model for the\n\nCoSi2 for qualitatively understanding the CoSi2/TiSi2\nheterostructures. However, it is a multiband system,\nwhich can be a possible origin of the TRSB. We leave\nthis issue of TRSB for future study. We conclude that\nthe CoSi2/TiSi2 heterostructure is a s + p mixed par-\nity superconducting state with a dominant p-wave com-\nponent. Such mix parity superconductor with a domi-\nnant triplet component is a topologically nontrivial sys-\ntem and is similar to a quantum spin hall system50,51.\nIt hosts topologically protected Andreev bound states,\nwhich carry spin currents, therefore constituting an im-\nportant platform for further research.\n\nACKNOWLEDGMENTS\n\nThe authors are grateful to Shao-Pin Chiu and Juhn-\nJong Lin for helpful discussions. VM, YL and FCZ are\n\n\n\n7\n\npartially supported by NSFC grant 11674278 and by the\npriority program of the Chinese Academy of Sciences\ngrant No. XDB28000000, and by the China Postdoctoral\nScience Foundation under grant No. 2020M670422 (YL).\n\nWork at Zhejiang University was in part supported by\nthe National Key R&D Program of the MOST of China,\ngrant No. 2016YFA0300202 and the National Science\nFoundation of China, grant No. 11774307.\n\n\u2217 fuchun@ucas.ac.cn\n\u2020 stefan.kirchner@correlated-matter.com\n1 N. Read and D. Green, Phys. Rev. B 61, 10267 (2000).\n2 A. Y. Kitaev, Physics-Uspekhi 44, 131 (2001).\n3 D. A. Ivanov, Phys. Rev. Lett. 86, 268 (2001).\n4 A. Kitaev, Annals of Physics 303, 2 (2003).\n5 F. Wilczek, Nature Physics 5, 614 (2009).\n6 C. Nayak, S. H. Simon, A. Stern, M. Freedman, and\nS. Das Sarma, Rev. Mod. Phys. 80, 1083 (2008).\n\n7 R. M. Lutchyn, J. D. Sau, and S. Das Sarma, Phys. Rev.\nLett. 105, 077001 (2010).\n\n8 Y. Oreg, G. Refael, and F. von Oppen, Phys. Rev. Lett.\n105, 177002 (2010).\n\n9 V. Mourik, K. Zuo, S. M. Frolov, S. R. Plissard, E. P.\nA. M. Bakkers, and L. P. Kouwenhoven, Science 336, 1003\n(2012).\n\n10 S. Nadj-Perge, I. K. Drozdov, J. Li, H. Chen, S. Jeon,\nJ. Seo, A. H. MacDonald, B. A. Bernevig, and A. Yazdani,\nScience 346, 602 (2014).\n\n11 G. E. Volovik, The Universe in a Helium Droplet (Oxford\nUniversity Pres, 2003).\n\n12 D. Wang, L. Kong, P. Fan, H. Chen, S. Zhu, W. Liu,\nL. Cao, Y. Sun, S. Du, J. Schneeloch, R. Zhong, G. Gu,\nL. Fu, H. Ding, and H.-J. Gao, Science 362, 333 (2018).\n\n13 Q. Liu, C. Chen, T. Zhang, R. Peng, Y.-J. Yan, C.-H.-P.\nWen, X. Lou, Y.-L. Huang, J.-P. Tian, X.-L. Dong, G.-W.\nWang, W.-C. Bao, Q.-H. Wang, Z.-P. Yin, Z.-X. Zhao, and\nD.-L. Feng, Phys. Rev. X 8, 041056 (2018).\n\n14 L. Fu and C. L. Kane, Phys. Rev. Lett. 100, 096407 (2008).\n15 H.-H. Sun, K.-W. Zhang, L.-H. Hu, C. Li, G.-Y. Wang, H.-\n\nY. Ma, Z.-A. Xu, C.-L. Gao, D.-D. Guan, Y.-Y. Li, C. Liu,\nD. Qian, Y. Zhou, L. Fu, S.-C. Li, F.-C. Zhang, and J.-F.\nJia, Phys. Rev. Lett. 116, 257003 (2016).\n\n16 A. P. Mackenzie and Y. Maeno, Rev. Mod. Phys. 75, 657\n(2003).\n\n17 Y. Asano, Y. Tanaka, A. A. Golubov, and S. Kashiwaya,\nPhys. Rev. Lett. 99, 067005 (2007).\n\n18 S.-P. Chiu, C. C. Tsuei, S.-S. Yeh, F.-C. Zhang, S. Kirch-\nner, and J.-J. Lin, Observation of triplet superconductivity\nin CoSi2/TiSi2 heterostructures (2020), arXiv:2012.13679.\n\n19 G. E. Blonder, M. Tinkham, and T. M. Klapwijk, Phys.\nRev. B 25, 4515 (1982).\n\n20 B. T. Matthias, Phys. Rev. 87, 380 (1952).\n21 L. F. Mattheiss and D. R. Hamann, Phys. Rev. B 37, 10623\n\n(1988).\n22 K. Tsutsumi, S. Takayanagi, and T. Hirano, Physica B:\n\nCondensed Matter 237-238, 310 (1997), proceedings of\nthe Yamada Conference XLV, the International Conference\non the Physics of Transition Metals.\n\n23 L. P. Gor\u2019kov and E. I. Rashba, Phys. Rev. Lett. 87,\n037004 (2001).\n\n24 C. Iniotakis, N. Hayashi, Y. Sawa, T. Yokoyama, U. May,\nY. Tanaka, and M. Sigrist, Phys. Rev. B 76, 012501 (2007).\n\n25 A. B. Vorontsov, I. Vekhter, and M. Eschrig, Phys. Rev.\nLett. 101, 127003 (2008).\n\n26 Y. Tanaka, Y. Mizuno, T. Yokoyama, K. Yada, and\nM. Sato, Phys. Rev. Lett. 105, 097002 (2010).\n\n27 M. Eschrig, C. Iniotakis, and Y. Tanaka, Non-Centrosym-\nmetric Superconductors, edited by E. Bauer and M. Sigrist,\nLecture Notes in Physics, Vol. 847 (Springer Berlin Heidel-\nberg, 2012) pp. 313\u2013357.\n\n28 S. Tamura and Y. Tanaka, Phys. Rev. B 99, 184501 (2019).\n29 Y. Tada, N. Kawakami, and S. Fujimoto, Journal of the\n\nPhysical Society of Japan 77, 054707 (2008).\n30 Y. Yanase and M. Sigrist, Journal of the Physical Society\n\nof Japan 77, 124711 (2008).\n31 K. D. Usadel, Phys. Rev. Lett. 25, 507 (1970).\n32 N. Hayashi, K. Wakabayashi, P. A. Frigeri, and M. Sigrist,\n\nPhys. Rev. B 73, 092508 (2006).\n33 P. A. Frigeri, D. F. Agterberg, A. Koga, and M. Sigrist,\n\nPhys. Rev. Lett. 92, 097001 (2004); P. A. Frigeri, D. F.\nAgterberg, I. Milat, and M. Sigrist, arXiv e-prints , cond-\nmat/0505108 (2005), arXiv:cond-mat/0505108 [cond\u2013\nmat.supr-con].\n\n34 G. Annunziata, D. Manske, and J. Linder, Phys. Rev. B\n86, 174514 (2012).\n\n35 A. Zaitsev, Physics Letters A 194, 315 (1994).\n36 Y. V. Nazarov, Superlattices and Microstructures 25, 1221\n\n(1999).\n37 Y. Tanaka, Y. V. Nazarov, and S. Kashiwaya, Phys. Rev.\n\nLett. 90, 167003 (2003).\n38 Y. Tanaka, Y. V. Nazarov, A. A. Golubov, and S. Kashi-\n\nwaya, Phys. Rev. B 69, 144519 (2004); Phys. Rev. B 70,\n219907 (2004).\n\n39 Y. Tanaka, Y. Asano, A. A. Golubov, and S. Kashiwaya,\nPhys. Rev. B 72, 140503 (2005); Phys. Rev. B 73, 059901\n(2006).\n\n40 J. P. Morten, A. Brataas, and W. Belzig, Phys. Rev. B 72,\n014510 (2005).\n\n41 Y. Tanaka, Y. Asano, A. A. Golubov, and S. Kashiwaya,\nPhys. Rev. B 72, 140503 (2005); Phys. Rev. B 73, 059901\n(2006).\n\n42 Y. Tanaka and A. A. Golubov, Phys. Rev. Lett. 98, 037003\n(2007).\n\n43 A. F. Volkov and H. Takayanagi, Phys. Rev. Lett. 76, 4026\n(1996); Phys. Rev. B 56, 11184 (1997).\n\n44 Y. Asano, Phys. Rev. B 64, 014511 (2001).\n45 C. Timm, S. Rex, and P. M. R. Brydon, Phys. Rev. B 91,\n\n180503 (2015).\n46 Y. Wang and L. Fu, Phys. Rev. Lett. 119, 187003 (2017).\n47 E. Arahata, T. Neupert, and M. Sigrist, Phys. Rev. B 87,\n\n220504 (2013).\n48 S. K. Yip, Phys. Rev. B 65, 144508 (2002).\n49 V. M. Edelstein, Phys. Rev. B 67, 020505 (2003).\n50 Y. Tanaka, T. Yokoyama, A. V. Balatsky, and N. Nagaosa,\n\nPhys. Rev. B 79, 060505 (2009).\n51 M. Sato and S. Fujimoto, Phys. Rev. B 79, 094504 (2009).\n\nmailto:fuchun@ucas.ac.cn\nmailto:stefan.kirchner@correlated-matter.com\nhttps://doi.org/10.1103/PhysRevB.61.10267\nhttps://doi.org/10.1070/1063-7869/44/10s/s29\nhttps://doi.org/10.1103/PhysRevLett.86.268\nhttps://doi.org/https://doi.org/10.1016/S0003-4916(02)00018-0\nhttps://doi.org/10.1038/nphys1380\nhttps://doi.org/10.1103/RevModPhys.80.1083\nhttps://doi.org/10.1103/PhysRevLett.105.077001\nhttps://doi.org/10.1103/PhysRevLett.105.177002\nhttps://doi.org/10.1126/science.1222360\nhttps://doi.org/10.1126/science.1259327\nhttps://doi.org/10.1126/science.aao1797\nhttps://doi.org/10.1103/PhysRevX.8.041056\nhttps://doi.org/10.1103/PhysRevLett.100.096407\nhttps://doi.org/10.1103/PhysRevLett.116.257003\nhttps://doi.org/10.1103/RevModPhys.75.657\nhttps://doi.org/10.1103/PhysRevLett.99.067005\nhttps://arxiv.org/abs/2012.13679\nhttps://doi.org/10.1103/PhysRevB.25.4515\nhttps://doi.org/10.1103/PhysRev.87.380\nhttps://doi.org/10.1103/PhysRevB.37.10623\nhttps://doi.org/https://doi.org/10.1016/S0921-4526(97)00188-9\nhttps://doi.org/10.1103/PhysRevLett.87.037004\nhttps://doi.org/10.1103/PhysRevB.76.012501\nhttps://doi.org/10.1103/PhysRevLett.101.127003\nhttps://doi.org/10.1103/PhysRevLett.105.097002\nhttps://doi.org/10.1007/978-3-642-24624-1\nhttps://doi.org/10.1103/PhysRevB.99.184501\nhttps://doi.org/10.1143/JPSJ.77.054707\nhttps://doi.org/10.1143/JPSJ.77.124711\nhttps://doi.org/10.1103/PhysRevLett.25.507\nhttps://doi.org/10.1103/PhysRevB.73.092508\nhttps://doi.org/10.1103/PhysRevLett.92.097001\nhttps://arxiv.org/abs/cond-mat/0505108\nhttps://doi.org/10.1103/PhysRevB.86.174514\nhttps://doi.org/https://doi.org/10.1016/0375-9601(94)91257-2\nhttps://doi.org/10.1006/spmi.1999.0738\nhttps://doi.org/10.1103/PhysRevLett.90.167003\nhttps://doi.org/10.1103/PhysRevB.69.144519\nhttps://doi.org/10.1103/PhysRevB.70.219907\nhttps://doi.org/10.1103/PhysRevB.72.140503\nhttps://doi.org/10.1103/PhysRevB.73.059901\nhttps://doi.org/10.1103/PhysRevB.72.014510\nhttps://doi.org/10.1103/PhysRevB.72.140503\nhttps://doi.org/10.1103/PhysRevB.73.059901\nhttps://doi.org/10.1103/PhysRevLett.98.037003\nhttps://doi.org/10.1103/PhysRevLett.76.4026\nhttps://doi.org/10.1103/PhysRevB.56.11184\nhttps://doi.org/10.1103/PhysRevB.64.014511\nhttps://doi.org/10.1103/PhysRevB.91.180503\nhttps://doi.org/10.1103/PhysRevLett.119.187003\nhttps://doi.org/10.1103/PhysRevB.87.220504\nhttps://doi.org/10.1103/PhysRevB.65.144508\nhttps://doi.org/10.1103/PhysRevB.67.020505\nhttps://doi.org/10.1103/PhysRevB.79.060505\nhttps://doi.org/10.1103/PhysRevB.79.094504\n\n"
"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEPJ manuscript No.\n(will be inserted by the editor)\n\nPrecision measurements of differential cross sections and\nanalyzing powers in elastic deuteron-deuteron scattering at 65\nMeV/nucleon\n\nR. Ramazani-Sharifabadi1,2a, A. Ramazani-Moghaddam-Arani3b, H.R. Amir-Ahmadi2, C. D. Bailey4c, A. Deltuva5,\nM. Eslami-Kalantari6, N. Kalantar-Nayestanaki2, St. Kistryn7, A. Kozela8, M. Mahjour-Shafiei1, H. Mardanpour2,\nJ.G. Messchendorp2, M. Mohammadi-Dadkan2,9, E. Stephan10, E. J. Stephenson4, and H. Tavakoli-Zaniani2,6\n\n1 Department of Physics, University of Tehran, Tehran, Iran\n2 KVI-CART, University of Groningen, Groningen, The Netherlands\n3 Department of Nuclear Physics, Faculty of Physics, University of Kashan, Kashan, Iran\n4 Center for Exploration of Energy and Matter, Indiana University, Bloomington, IN 47408 USA\n5 Institute of Theoretical Physics and Astronomy, Vilnius University, Vilnius, Lithuania\n6 Department of Physics, School of Science, Yazd University, Yazd, Iran\n7 Institute of Physics, Jagiellonian University, Krako\u0301w, Poland\n8 Institute of Nuclear Physics, PAS, Krako\u0301w, Poland\n9 Department of Physics, University of Sistan and Baluchestan, Zahedan, Iran\n\n10 Institute of Physics, University of Silesia, Chorzo\u0301w, Poland\n\nReceived: date / Revised version: date\n\nAbstract. We present measurements of differential cross sections and analyzing powers for the elastic\n2H(~d, d)d scattering process. The data were obtained using a 130 MeV polarized deuteron beam. Cross\nsections and spin observables of the elastic scattering process were measured at the AGOR facility at\nKVI using two independent setups, namely BINA and BBS. The data harvest at setups are in excellent\nagreement with each other and allowed us to carry out a thorough systematic analysis to provide the most\naccurate data in elastic deuteron-deuteron scattering at intermediate energies. The results can be used to\nconfront upcoming state-of-the-art calculations in the four-nucleon scattering domain, and will, thereby,\nprovide further insights in the dynamics of three- and four-nucleon forces in few-nucleon systems.\n\nKey words. deuteron-deuteron scattering \u2013 elastic channel \u2013 vector and tensor analyzing powers \u2013 nuclear\nforces\n\nPACS. 21.30.-x; 21.30.Fe; 21.45.+v; 21.45.Ff; 25.10.+s\n\n1 Introduction\n\nUnderstanding the degrees of freedom that describe nu-\nclear forces is of great importance to make progress in nu-\nclear physics. The first major breakthrough came in 1935\nwhen Yukawa presented the description of the nucleon-\nnucleon force by the exchange of massive mesons [1] in\nanalogy to the exchange of massless photons describ-\ning successfully the electromagnetic interaction. More re-\ncently, various phenomenological nucleon-nucleon (NN)\npotentials have been derived based on Yukawa\u2019s idea.\nSome of these potentials were successfully linked to the un-\nderlying fundamental theory of quantum chromodynam-\n\na reza ramazani@ut.ac.ir\nb ramezamo@kashanu.ac.ir\nc Present address: American Physical Society, 1 Physics El-\n\nlipse, College Park, MD 20240 USA\n\nics [2,3]. Precision measurements obtained from nucleon-\nnucleon scattering data are strikingly well described by\nthese modern NN potentials [4].\n\nIt is compelling to apply the high-precision NN po-\ntentials to systems composed of at least three nucleons.\nRigorous Faddeev calculations of the binding energy of\nthe simplest three-nucleon system, triton, underestimate\nthe experimental data [5]. This observation shows that\ncalculations based solely on NN potentials are not suffi-\ncient to describe systems that involve more than two nu-\ncleons. This has led to the notion of the three-nucleon\nforce (3NF), a concept that was introduced already in\nthe early days of nuclear physics by Primakoff and Hol-\nstein [6]. Green\u2019s function Monte Carlo calculations based\non the AV18 NN potential complemented with the IL7\nthree-nucleon potential demonstrated the necessity of the\n3NF to describe the experimental data for the binding\n\nar\nX\n\niv\n:2\n\n10\n1.\n\n03\n73\n\n1v\n1 \n\n [\nnu\n\ncl\n-e\n\nx]\n  1\n\n1 \nJa\n\nn \n20\n\n21\n\n\n\n2 R. Ramazani-Sharifabadi et al.: Title Suppressed Due to Excessive Length\n\nenergies of light nuclei [7]. Moreover, rigorous Faddeev\ncalculations based on modern NN potentials show large\ndiscrepancies with cross section data in elastic nucleon-\ndeuteron scattering. The inclusion of 3NF effects partly\nresolves these deficiencies [8]. There are, by now, a large\nnumber of evidences revealing the importance of 3NF ef-\nfects.\n\nIn the last decades, many nucleon-deuteron elas-\ntic [9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,\n26,27] and breakup [28,29,30,31,32,33,34,35,36,37] scat-\ntering experiments at various energies below the pion-\nproduction threshold have provided an extensive database\nfor the study 3NF effects. The addition of 3NF effects,\nin particular the role of the \u2206 resonance, reduces signifi-\ncantly the discrepancies between differential cross-section\ndata and corresponding calculations excluding 3NF ef-\nfects. The situation for spin observables is vastly different.\nFor instance, the inclusion of 3NF effects for the vector\nanalyzing power of the elastic channel at the intermedi-\nate energies gives a better agreement between data and\ntheory, while for the tensor analyzing power, Re(T22), the\ndiscrepancies are not removed by adding 3NF effects in\nthe model [39]. The inclusion of 3NF effects even deteri-\norates the agreement between model predictions and the\ndata for the vector analyzing power of the proton in the\nproton-deuteron breakup reaction at configurations that\ncorrespond to small relative energies between the two out-\ngoing protons [39]. These observations imply that spin-\ndependent parts of 3NF effects are not yet well under-\nstood [38,39].\n\nAlthough the three-nucleon (3N) system is the clean-\nest system to study 3NF effects since only NN and 3N\nforces can contribute and observables can be calculated in\nan ab-initio manner, the influence of 3NF effects are in\ngeneral small in a 3N system. Only at specific parts of the\nphase space in three-nucleon scattering processes, 3NF ef-\nfects become significant. A well-known example of such\na phase space appears at scattering angles corresponding\nto the minimum of the differential cross section in elas-\ntic Nd scattering [8,40]. In spin observables, a significant\n3NF effect can also be seen for pd break-up configura-\ntions corresponding to small relative energies between the\ntwo outgoing protons [39]. Alternatively, and this is the\nfocus of this paper, one may investigate the four-nucleon\n(4N) system in which 3NF effects could be significantly\nenhanced [39]. Deuteron-deuteron scattering, as a 4N sys-\ntem, is a rich laboratory to study 3NF effects because of its\nvariety of final states, observables, and kinematical con-\nfigurations. Compared to the amount of available data in\nthe 3N scattering domain, the database in the 4N system\nis very limited. Most of the 4N data cover the very low-\nenergy regime, below the three- and four-body breakup\nthreshold [41,42,43]. Although, calculations at these low\nenergies are very reliable, the effect of the 3NF is very\nsmall. Therefore, the low-energy realm is not the most at-\ntractive regime to study rigorously the dynamics of 3NFs.\n\nAb-initio theoretical calculations for four nucleon\nsystems are still limited to beam energies below 40\nMeV [44,45,46,47,48,49,50,51]. At intermediate energies,\n\nbelow the pion-production threshold, the 4N experimental\ndatabase is very scarce [52,53,54]. Despite the fact that\nab-initio calculations are still in development in this en-\nergy regime, the prospects of studying the structure of 3N\nforces, and possibly higher-order four-nucleon force effects,\nlook promising [55,56]. Recent theoretical approximations\nfor deuteron-deuteron scattering are able to reasonably\npredict the experimental results in the quasi-free (QF)\nregime [57,58]. However, one should consider the final-\nstate interactions of spectator neutrons to identify the QF\nlimit correctly [34]. Besides, charge symmetry breaking\nstudies (CSB) in d + d \u2192 4He + \u03c00 reaction reveal the\nnecessity of theoretical calculations in dd elastic scatter-\ning process to provide an unambiguous formulation of the\ninitial-state interaction. In this energy regime, a single-\nscattering approximation is used in which one nucleon\nscatters from the opposite deuteron before it recombines\nto reform the original deuteron [59,60].\n\nThis paper presents measurements of the differen-\n\ntial cross section and spin observables in the 2H(~d, d)d\nelastic scattering process for a deuteron-beam energy of\n65 MeV/nucleon. The data were obtained by making use\nof a vector- and tensor-polarized deuteron beam that was\nprovided by the AGOR facility at KVI in Groningen,\nthe Netherlands. Two experimental equipments, located\nat two different beam lines, were used to measure inde-\n\npendently the various observables in 2H(~d, d)d scatter-\ning, namely the Big-Bite Spectrometer (BBS) and the\nBig Instrument for Nuclear-Polarization Analysis (BINA).\nThese setups bring complementary features: one (BINA)\ncovering large phase space, particularly in \u03c6, using a liquid\ndeuterium target leading to less background. The other\n(BBS) possesses an excellent momentum resolution, but\nwith moderate coverage, using a solid CD2 target with\nmore precise knowledge on the target thickness at the cost\nof a larger background. These two sets of measurements\ncombined have provided a good experimental database\nthat can be used as a benchmark for future ab-initio cal-\nculations. This paper addresses the analysis of these two\nindependent datasets. The results presented here are the\n\nmost precise and accurate data of the 2H(~d, d)d process at\nintermediate energies.\n\n2 Experimental setups\n\nThis experiment was performed with two different setups,\nBINA and BBS. In the following, details of both setups\nrelevant for the present paper will be presented. Detailed\ndiscriptions are presented in [61,62], respectively.\n\n2.1 Common source and accelerator facilties\n\nThe two experiments were conducted using AGOR facility\nat KVI. The measurement on BINA took place the week\nafter the BBS data taking. BINA has the ability to iden-\ntify and measure all reaction channels of the deuteron-\ndeuteron scattering process simultaneously, while BBS\n\n\n\nR. Ramazani-Sharifabadi et al.: Title Suppressed Due to Excessive Length 3\n\nmeasures the hadronic channels with particles emerg-\ning from the two-body final states. Vector- and tensor-\npolarized (unpolarized) deuteron beams were produced\nby the atomic Polarized Ion Source (POLIS) [63,64] with\nnominal polarization values between 60-80% of the the-\noretical values and accelerated by the AGOR cyclotron\nto energies of 65 MeV/nucleon. The polarization of the\ndeuteron beam was monitored for different periods of the\nexperiment and found to be stable within statistical un-\ncertainties [65].\n\n2.2 BINA\n\nFigure 1 shows a sketch of BINA. The setup consists\nof two parts, a forward wall and a backward ball.\nThe forward wall consists of a multi-wire proportional\nchamber (MWPC) to determine the scattering angles\nof the particles, twelve-vertically mounted plastic \u2206E-\nscintillators with a thickness of 2 mm, and ten horizontally\nmounted E-scintillators with a thickness of 12 cm. The\nE-scintillators are placed in a cylindrical shape where the\ntarget is positioned on the axial symmetry of the cylinder.\nAlthough, the \u2206E-E hodoscope provides the possibility to\nperform particle identification, the information from the\n\u2206E detector was not used in this experiment. In a visual\ninspection after the experiment, these scintillators were\nobserved to be damaged. Therefore, the \u2206E-E detectors\ncould not provide the PID information for all scattering\nangles. Photomultiplier tubes (PMTs) were mounted\non both sides of each E-scintillator. Signals from these\nPMTs are used to extract the energy and time-of-flight\n(TOF) of each scattered particle. The TOF information\nis used to perform PID. The MWPC covers scattering\nangles between 10\u25e6 and 32\u25e6 with a full azimuthal angle\ncoverage and up to 37\u25e6 with a limited azimuthal angle\ncoverage. The MWPC has a resolution of 0.4\u25e6 for the\npolar angle and between 0.6\u25e6 and 2.0\u25e6 for the azimuthal\nangle depending on the scattering angle. The detection\nefficiency of the MWPC for deuteron with energies\ncorresponding to the reaction of interest is typically\n98\u00b1 1% [69]. The backward ball of BINA is made of 149\nphoswich scintillators that were simultaneously used as\ndetector and scattering chamber with a scattering-angle\ncoverage between 40\u25e6 and 165\u25e6 and nearly full azimuthal\ncoverage. For more details on BINA, we refer to [61,66].\n\nThe deuteron beam, with a typical current of 4 pA,\nbombarded a liquid-deuterium target that was mounted\ninside the scattering chamber of BINA [67]. The thickness\nof the target cell was 3.85 mm with an uncertainty of\n5%. The scattering angles, energies, and (partly) time\nof flights of the final-state deuterons were measured\nby the multi-wire proportional chamber (MWPC) and\nscintillators of BINA. A Faraday cup was mounted at\nthe end of the beam line to monitor the beam current\nthroughout the experiment. The current meter of the\nFaraday cup was calibrated using a current source with\nan uncertainty of 2% [65]. A small offset in the readout\nof the current was observed with a value around 0.28 \u00b1\n0.13 pA, see Sec. 5.\n\nForward WallTarget\n\nBeam\n\nBackward Ball\n E\n\nMWPC\n\n\u0394\u0395\n\n29.7 cm\n\n12 cm\n\nBeam pipe\n\nFig. 1. A sketch of the various components of the BINA setup.\nThe elements on the right side show a side view of the forward\npart of BINA, including the multi-wire proportional cham-\nber (MWPC), an array of twelve thin plastic \u2206E-scintillators\nfollowed by ten thick segmented E-scintillators mounted in a\ncylindrical shape. On the left side, the backward part of BINA\nis depicted composed of 149 phoswich scintillators glued to-\ngether to form the scattering chamber.\n\n2.3 BBS\n\nThe Big-Bite Spectrometer (BBS) is a QQD-type mag-\nnetic spectrometer with a K-value of 430 MeV and a solid\nangle of up to 13 msr. By changing the position of the\nquadrupole doublet with respect to the dipole magnet,\nwhile the distance between the object (target) and the\ndipole remains the same, the momentum-bite acceptance\ncan be changed from 13 to 25%, the solid angle changes\nfrom 13 to 7 msr, simultaneously. The BBS consists of a\nscattering chamber containing a target ladder, a large slit\nwheel containing several entrance apertures (including a\nsieve slit for angle reconstruction), two sets of quadrupole\nmagnets for beam focusing, a large dipole magnet for mo-\nmentum selection, two sets of x-u plane wire-chamber de-\ntectors, and a scintillator plane which is used to generate\nthe event trigger. A diagram of the BBS is shown in Fig. 2.\n\nIn the BBS setup, different thick or thin sets of CD2\nand carbon targets were used for different ranges of lab an-\ngles. The carbon targets were used in the forward range of\nspectrometer angles to be able to subtract the background\ngenerated by deuterons elastically scattered from carbon\nin the CD2 target. For large angles (\u2265 15\u25e6), several layers\nof solid CD2 were combined, resulting in a total thickness\nof 45.15 \u00b1 2.26 mg/cm2. For small angles (4\u25e6 and 6\u25e6)\n\n\n\n4 R. Ramazani-Sharifabadi et al.: Title Suppressed Due to Excessive Length\n\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\n\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\nMWPCs D1\u2212D4\n\nCarbon analyzer C\n\nScintillators S1, S2\n\nVDCs 1 and 2\n\n2107\n\n621 270324 270\n\nFPPFPDS\n\nQ2\n\nD\n\n2\n2\n0\n cm\n\ntarget\n\nQ1\n\nFig. 2. A sketch of the main features of the BBS setup.\n\nthe CD2 target thickness was 10.49 \u00b1 0.52 mg/cm2. The\nthickness of the carbon target for large angles (\u2265 15\u25e6)\nwas 46.80 \u00b1 0.65 mg/cm2; for small angles it was 14.2\n\u00b1 0.20 mg/cm2. The scattering chamber of the BBS con-\nsisted of a large cylindrical chamber containing the targets\nand essentially forming the pivot point around which the\ndevice covers the scattering angles between 4\u25e6 and 48\u25e6\n\nduring the data taking. For the beam integration, a large\ncopper Faraday cup was used for the angles larger than\n15\u25e6, where the unscattered beam could hit the wall of the\nscattering chamber. For small scattering angles (less than\n15\u25e6), the unscattered beam was within the acceptance of\nBBS and entered the region of the quadrupole magnets.\nTherefore, a separate Faraday cup is mounted between the\nquadrupole magnets Q1 and Q2 for the beam integration\nin this region. A detailed description of the BBS setup is\npresented in [68].\n\n3 Event selection and data analysis method\n\nIn this section, the analysis procedures of both experi-\nments related to the BINA and BBS setups are described\nseparately. Detailed discriptions are presented in [61,62],\nrespectively.\n\n3.1 BINA\n\nDuring data taking with BINA, various hardware trig-\ngers with different down-scale factors were implemented\nthat were dedicated to a specific hadronic final state in\n\ndeuteron-deuteron scattering. To select events originat-\ning from elastically scattered deuterons, two triggers were\nof importance. The first one, referred to as the coinci-\ndence trigger, registered events for which there was at\nleast one signal from the forward wall scintillators in coin-\ncidence with at least one signal originating from the back-\nward wall. This trigger was down-scaled by a factor two.\nThe hardware thresholds for detection of a particle were\ntypically set around 1 MeV. Although, with the coinci-\ndence trigger we were able to cover a large part of the\n\nangular distribution of the 2H(~d, d)d reaction, whereby\nboth deuterons in the final state were detected, we ob-\nserved a significant drop in the detection efficiency for\nlow-energetic deuterons that scatter towards the backward\nball due to energy losses of those particles in the liquid-\ndeuterium target. The data selected with the coincidence\ntrigger were used to extract the spin observables, since\ndetection inefficiencies cancel out in the analysis. To ex-\ntract the differential cross section, we exploited a second\ntrigger the so-called, \u201csingle trigger\u201d. This trigger, down-\nscaled by a factor 256, was built from a logical OR of all\nthe discriminated signals of the scintillators of BINA, and,\nthereby, not biased on the response of the backward ball.\n\nThe data from the coincidence trigger were calibrated\nand further preprocessed by requiring that the relative an-\ngles of the reconstructed particles hitting the forward wall\nand backward ball match the correlation that is expected\nfrom kinematical considerations for the elastic deuteron-\ndeuteron scattering process. Cuts were applied to meet a\nrelative opening angle of 83\u25e6 and a coplanar configura-\ntion with respect to the azimuthal angles. After applying\nthese angular cuts with a window of \u00b120\u25e6, a major reduc-\ntion (around 75%) of backgrounds from other hadronic fi-\nnal states, such as breakup and nucleon-transfer reactions,\nwas obtained. Figure 3 shows the correlation between en-\nergy and scattering angle of deuterons detected in the for-\nward wall of BINA after the aforementioned event selec-\ntion. The solid line represents the expected kinematical lo-\ncus for the elastic deuteron-deuteron scattering. As seen,\nelastically scattered deuterons can easily be observed and\ndistinguished from background channels. The data below\nthe elastic events reveal another clear correlation which\nhas been identified as events belonging to the neutron-\n\ntransfer channel, 2H(~d, 3H)p.\nTo count the number of events that originate from\n\nthe elastic process, the center-of-mass energy for each re-\nconstructed particle is calculated from its energy deposit\nand scattering angle, and a corresponding histogram is\ngenerated in intervals of 2\u25e6 of the scattering angle and\nseparated for the various polarization states of the beam.\nFigure 4 depicts the center-of-mass energy distribution\nthat has been obtained using the single trigger. The upper\nspectrum shows the raw response after calibration and for\nparticles that scatter to 26 \u00b1 1\u25e6 but without any further\nconditions. For the lower spectrum, a coincidence with the\nbackward ball was required in addition using the kine-\nmatical cuts discussed earlier but from data taken with\nthe single trigger. The solid lines are the result of a fit\nthrough the data based on a Gaussian-distributed signal\n\n\n\nR. Ramazani-Sharifabadi et al.: Title Suppressed Due to Excessive Length 5\n\n [deg]\u03b8\n15 20 25 30 35\n\n [\nM\n\ne\nV\n\n]\nd\n\nE\n\n0\n\n50\n\n100\n\n0\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\nCounts\n\nFig. 3. The correlation between the reconstructed energy\nand scattering angle of the particles that were detected in the\nforward part of BINA with a coincidence requirement with the\nbackward ball. The solid line represents the kinematical locus\nfor the elastic deuteron-deuteron scattering process.\n\n [MeV]c.m.E\n24 26 28 30 32 34 36\n\nC\no\n\nu\nn\n\nts\n [\n\n1\n~\n\nM\ne\n\nv/\nb\n\nin\n]\n\n0\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n400\n\nFig. 4. Spectrum of the center-of-mass energy of particles hit-\nting the E detectors of the forward wall. Data are obtained\nusing the single trigger. The scattered particles are confined\nto polar angles of 26 \u00b1 1\u25e6. For the lower spectrum, a coinci-\ndence condition is imposed in the event selection. The solid\nlines show the results of a least-\u03c72 fit based on a Gaussian\n(signal) and a 5th-order polynomial (background) distribution.\nThe background contribution is indicated by the dashed lines.\nThe \u03c72/64 of the fit is 1.4 for the upper spectrum and 1.2 for\nthe lower one.\n\ncombined with a 5th-order polynomial representing the\nbackgrounds. The background component of the fit is indi-\ncated by the dashed lines. A clear peak can be observed in\nboth cases, corresponding unambiguously to the channel\nof interest. The difference between the integrals of the sig-\nnal distributions before and after applying the coincidence\ncondition excluding inefficiencies of the ball is less than\n2%. The coincidence requirement reduces significantly the\nbackground contribution. Monte Carlo simulations showed\nthat the remaining background is mostly due to hadronic\ninteractions of elastically scattered deuterons in the scin-\ntillator.\n\nTo extract cross sections, the number of counts pass-\ning the kinematical criteria has been corrected for effi-\nciencies of the system such as live-time, MWPC efficien-\ncies, hadronic interactions, and the down-scale factor that\ncomes from triggers. The average live-time of the data ac-\n\nquisition of BINA is around 40%.\nEvents from the elastic reaction that suffered from\n\nhadronic interactions do not give a clear peaking struc-\nture in the energy spectrum, and are, therefore, not eas-\nily separated from other background channels, we did not\ncount these events and corrected for their loss. Using a\nGEANT3-based Monte Carlo simulation, we estimated a\nloss of 16\u00b1 2% for the energy range of interest. The cross\nsections are corrected for this effect accordingly.\n\nVector and tensor polarized beams make it possible to\nmeasure spin observables. Using parity conservation, the\n\ncross section for 2H(~d, d)d reaction is given by the follow-\ning equation [72]:\n\n\u03c3(\u03b8, \u03c6)\n\n\u03c30(\u03b8)\n= k\n\n[\n1 +\n\n3\n\n2\npZAy(\u03b8) cos(\u03c6)\u2212\n\n1\n\n4\npZZAzz(\u03b8)\n\n+\n1\n\n4\npZZ\n\n(\nAzz(\u03b8) + 2Ayy(\u03b8)\n\n)\ncos(2\u03c6)\n\n]\n, (1)\n\nwhere \u03b8 and \u03c6 are polar and azimuthal angles of the scat-\ntered deuteron, respectively. Ay is the vector analyzing\npower, while Azz and Ayy are the tensor analyzing powers.\npZ (pZZ) represents the vector (tensor) polarization of\nthe beam. \u03c3 (\u03c30) is the effective cross section obtained for\ndata taken with (un)polarized beam. These effective cross\nsections correspond to the number of counts normalized\nby the accumulated and dead-time corrected charge.\nPlease note that in first order, the efficiencies cancel\nby taking the ratio between \u03c3(\u03b8, \u03c6) and \u03c30(\u03b8). Finally,\nk is a normalization factor and should be equal to one\nin the ideal case. Considering k as a free parameter, it\nfluctuates around one with a value of k = 1.00\u00b1 0.03 that\nis considered as a systematic uncertainty for the normal-\nization procedure. Experimentally, however, we evaluated\npossible systematical differences in the extraction of the\neffective cross sections \u03c3(\u03b8, \u03c6) and \u03c30(\u03b8) accommodated\nin k. These may be due to small differences in detection\nefficiencies or beam-current measurements between data\ntaken with unpolarized and polarized beams. For the\nextraction of the analyzing powers, we analyzed data\ntaken with the coincidence trigger and enforcing the\nselection criteria as described above. We note that the\nbackground using the coincidence conditions is very\nsmall.\n\nWe extracted the analyzing powers with two different\nmethods which both lead to compatible results within\nthe uncertainties. In the first method; we assume that the\nbeam polarization is purely vector (pZ 6= 0 and pZZ = 0)\nor purely tensor (pZ = 0 and pZZ 6= 0). Therefore,\nin Eq. 1, the corresponding terms are kept and the\nother terms are set to zero. As can be seen in Eq. 1, the\nasymmetry ratio of polarized to un-polarized cross section\nis a function of cos\u03c6 (cos 2\u03c6) for the case of pure vector\n(tensor) polarized beam, see Fig. 5. Therefore, vector\nanalyzing power, Ay, is extracted from the amplitude of\ncos\u03c6. In the same way, tensor analyzing powers of Azz and\nAyy are extracted from the off-set of cos 2\u03c6 from one and\nits amplitude, respectively. To estimate the systematic\nuncertainty due to the possible impurity in the vector-\n\n\n\n6 R. Ramazani-Sharifabadi et al.: Title Suppressed Due to Excessive Length\n\n0 50 100 150 200 250 300 350\n\n0\n\u03c3/\n\n\u03c3\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n1.2\n\n1.3\n\n [deg]\u03c6\n0 50 100 150 200 250 300 350\n\n0\n\u03c3/\n\n\u03c3\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n1.2\n\n1.3\n\nFig. 5. Asymmetry ratio of cross section for polarized over un-\npolarized beam as a function of \u03c6 for a pure-vector polarized\nbeam (top panel) and pure tensor polarized beam (bottom\npanel. Scattering angle of elastically scattered deuteron is 26\u00b1\n1\u25e6. The reduced \u03c72 for the top (bottom) panel is 1.04 (0.97).\n\n(tensor-)polarized beam, the second method is applied.\nIn the second method, we suppose that the pure-vector\n(tensor) polarized beam is not actually a pure-vector\n(tensor), (pZ 6= 0 and pZZ 6= 0). In other words, the\npure-vector (tensor) polarized beam is contaminated with\nanother polarization, say the tensor (vector) polarization.\nTherefore, Eq. 1 including all the terms is used to fit to\nthe asymmetry ratio of polarized to un-polarized cross\nsection beam for vector and tensor analyzing powers.\nAs described before, the analyzing powers can again be\nextracted from the amplitudes of the cos\u03c6 and cos 2\u03c6\nfunctions as well as the offset of cos 2\u03c6 function from one.\nThe difference between the two results is considered as\nthe systematic uncertainty due to the possible impurity\nin the beam polarization. The results of the first method\nis considered as the final results, see Sec. 5.\n\nTo verify the procedure of extracting the differential\n\ncross sections and analyzing powers of the 2H(~d, d)d\n\nreaction, we measured and analyzed the H(~d, dp) reaction\nas well. The same procedure was used to analyze the data\n\nof the well-studied H(~d, dp) reaction which were obtained\nusing a CH2 target and with the same setup and beam\n\nconditions as was applied in the study of the 2H(~d, d)d\nreaction.\n\nDifferential cross sections and analyzing powers for the\n\nreaction H(~d, dp) are presented in Fig. 6. In each panel,\nthe results of this analysis are represented by filled circles.\nThe error bars indicate the statistical uncertainties and\nthe gray bands represent the systematical errors. A de-\ntailed description of the related systematic uncertainties\nis presented in [26]. The open triangles show the results\nof cross sections measured at RCNP [23]. The open cir-\n\ncles and filled triangles show the analyzing powers data\ntaken at KVI, [26,22], and open rectangles are those taken\nat RIKEN [21]. The solid curves show the results of a\ncoupled-channel calculation by the Hannover-Lisbon the-\nory group based on the CD-Bonn potential including the\nCoulomb interaction and an intermediate \u2206-isobar [73].\nThe dotted lines represent results of a similar calcula-\ntions by excluding the \u2206-isobar. We note that the 3NF\neffects are predicted to be small and, therefore, the re-\nsults of the presented Faddeev calculations based on the\nhigh-precision NN potential are expected to accurately\ndescribe the experimental data. In addition, the results\nare compared with the results of a rough approximation\nbased on the lowest-order terms in the Born series expan-\nsion of the Alt-Grassberger-Sandhas (AGS) equation for\na three-nucleons interaction using CD-Bonn+\u2206 potential\n(the dashed lines). The comparison shows that the Born\napproximation is not very good in three-body systems at\nthis energy, and therefore, we do not expect that such an\napproach will provide a good description in the four-body\nscattering process; see Sec. 5. It is worth noting that the\nquality of the Born approximation improves with increas-\ning the energy and/or at small scattering angles as the\nlowest-order terms become dominant in all observables.\nOur measurements for the H(~d, dp) reaction are in excel-\nlent agreement with previously published data and with\nstate-of-the-art calculations, lending, thereby, confidence\nin the analysis procedure and our estimates of systematic\nuncertainties.\n\n3.2 BBS\n\nIn the following, the analysis procedure of the BBS data\nis described. Details of the analysis methods related the\nBBS data are presented in [62].\n\nThe differential cross section and spin observables were\nextracted at various scattering angles by counting elasti-\ncally scattered deuterons for various polarization states of\nthe beam. To access different scattering angles, the spec-\ntrometer was moved around the target. The quadrupole\nand dipole fields were changed according to the kinemat-\nics of the related reaction to focus and bend the particles\nof interest and bring them to the detector plane. In this\ncase, one focal point was produced via a combination of\nquadrupole and dipole fields for a scattered particle with\na given momentum. Therefore, the solid angle spanned by\nparticles, as they scatter from the target inside the scat-\ntering chamber, were determined by a defining aperture in\nfront of the spectrometer. For this purpose, a \u201cseive slit\u201d,\nan aperture fitted into the slit wheel of the BBS contain-\ning several pre-drilled holes, was used during several runs\nof the experiment. With this slit system, the optical coef-\nficients of BBS were fitted and the system was, therefore,\ncalibrated for various settings.\n\nThe main background sources are the events including\ndeuterons elastically or inelastically scattered from Car-\nbon. These events are appeared in the detector plane along\nwith the events of interest. To subtract the background, we\napplied two techniques. For the runs with no discernible\n\n\n\nR. Ramazani-Sharifabadi et al.: Title Suppressed Due to Excessive Length 7\n\n2\n\n3\n\n4\n\n5\n\n6\n\n2\n\n3\n\n4\n\n5\n\n6\n\n-0.4\n\n-0.2\n\n0.0\n\n0.2\n\nCDBonn+C+\nCDBonn+C\nBorn Approx.\nBINA\nRCNP\nRIKEN\nKVI 2008\nKVI 2007\n\n-0.2\n\n-0.1\n\n0.0\n\n0.1\n\n0.2\n\n50 60 70 80\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n50 60 70 80\n\nc.m. [deg]\n\nd\n/d\n\n[m\nb\n\n/s\nr]\n\nA\ny\n\nA\nzz\n\nA\nyy\n\nFig. 6. Differential cross section and analyzing powers of the elastic channel of the reaction H(~d, dp) that were taken with\na deuteron beam of 65 MeV/nucleon. In each panel, the data taken with BINA are indicated with filled circles whereby the\nerror bars are statistical. The open triangles show the cross section results obtained at RCNP [23]. The open circles and filled\ntriangles show the analyzing powers data taken at KVI [26,22], and open rectangles are those obtained at RIKEN [21]. The solid\ncurves show the results of a coupled-channel calculation by the Hannover-Lisbon theory group based on the CD-Bonn potential\nincluding the Coulomb interaction and an intermediate \u2206-isobar [73]. The dotted lines represent results of a similar calculation\nby excluding the \u2206-isobar. The dashed lines represent the results of a rough approximation based on the lowest-order terms in\nthe Born series expansion of the Alt-Grassberger-Sandhas (AGS) equation using CD-Bonn+\u2206 potential. The gray band shows\nthe systematic error (2\u03c3) in each panel,\n\nbackground structure due to the Carbon in the CH2 tar-\nget, the procedure of background subtraction is similar to\nthat described for the event selection in BINA. For the\nruns in which a clear background structure due to the\nCarbon in the CH2 target was present, the separate Car-\nbon data from the Carbon target which were taken during\nthe experiment were used. For each of these runs the cor-\nresponding Carbon data (i.e. data which were taken with\nexactly the same spectrometer settings and beam energy,\nbut with a solid carbon target) were analyzed using the\nsame parameters as the reaction data of interest. Finally,\nto obtain the differential cross section for each of the five\nbeam polarization states, the extracted number of counts\nafter background subtraction is corrected for the efficien-\ncies of the system such as live-time, and wire chamber\nefficiencies.\n\nBy knowing the polarized and unpolarized cross sec-\ntions for each of the five beam polarization values, we\ncould then calculate the unpolarized cross section and an-\nalyzing powers Ay and Ayy using Eq. 1 through a simple\nmatrix inversion. We have five equations and only three\nunknowns: the unpolarized cross section \u03c30, Ay, and Ayy.\nTherefore, the analyzing powers are obtained from the po-\nlarized cross sections using a matrix inversion, and their\nstatistical errors determined using standard error prop-\n\nagation techniques. Generally, there were almost always\nfive good polarized cross sections available, and therefore,\nthis was an over-determined system; however for a few\nruns only three or four polarization states were available,\nin which case the matrix inversion was reduced to only\ninclude the existing polarized cross sections [62].\n\n4 Systematic uncertainties\n\nThe common systematic uncertainties between the two ex-\nperiments as well as those specifically for BINA and BBS\nsetups are separately presented in the following subsec-\ntions.\n\n4.1 Common sources of systematic errors\n\nThe main common source of systematic error comes from\n\nthe uncertainty in the Ay measurements in the H(~d, dp)\nreaction to extract the polarization that is around 4.5%.\nThe results of Ayy measurements obtained from BBS\nare also used to estimate an offset in the readout of the\ncurrent. The offset was determined by minimizing the\nreduced \u03c72 whereby an offset in the current is introduced\n\n\n\n8 R. Ramazani-Sharifabadi et al.: Title Suppressed Due to Excessive Length\n\nas a free parameter using the comparison between the\nresults of the Ayy from the elastic channel of dd scat-\ntering coming from the BINA and those coming from\nthe BBS [62]. The error is obtained by evaluating the \u03c72\n\ndistribution as a function of the offset. The intersection\npoint of this distribution with a \u03c72 value that is one unit\nlarger than its minimum has been used to determine the\nuncertainty in the offset. The systematic error arising\nfrom the measurement of the beam current using a\nFaraday cup leads to a small offset of 0.28 \u00b1 0.13 pA in\nthe readout of the current.\n\n4.2 BINA\n\nOne of the systematic uncertainty in the cross section\nmeasurement is attributed to the thickness of the liquid\ndeuterium target. We estimated a corresponding error\nof 5% due to the bulging of the cell. The size of bulging\nwas first estimated via a measurement of the target\nthickness as a function of pressure at room temperature.\nThe actual target thickness was obtained by comparing\nthe cross section measurements at KVI between solid\nand liquid targets and the difference is considered as the\nuncertainty due to the thickness measurement. Other\nsystematic uncertainties come from the beam luminosity\nusing a precision current source (2%), the MWPC\nefficiency for deuterons which was obtained using an\nunbiased and nearly background-free data sample of the\npd elastic scattering process (1%), and the errors in the\ncorrection factor for losses due to hadronic interactions\nin the detector. For deuterons, this error is extracted\nfrom the difference between the measured and simulated\ndeposition of deuteron energy in the forward wall of\nBINA (2%) [65]. The uncertainty of the extraction of\nthe differential cross sections due to the offset current\nis around 5%. To estimate the systematic uncertainty\ndue to the background model, we used the 3th and 7th\n\norders of polynomial fit-functions instead of the 5th order\npolynomial representing the backgrounds. The maximum\ndifference between the results are considered as the\nsystematic uncertainty due to the background model\nwhich is around 4.5%.\n\nThe polarization of deuteron beam was monitored\nwith a Lamb-Shift Polarimeter (LSP) [71] at the low-\nenergy beam line and measured with BINA after beam\nacceleration at the high-energy beam line by mea-\nsuring the asymmetry in the elastic deuteron-proton\nscattering process [70]. The vector and tensor polariza-\ntions of the deuteron beam of BINA were found to be\npZ = \u22120.601 \u00b1 0.029 and pZZ = \u22121.517 \u00b1 0.032, respec-\ntively, whereby the errors include uncertainties in the\nanalyzing powers in elastic deuteron-proton scattering. It\nshould be remarked that only negative polarization states\nwere used, since the number of events obtained with that\npolarization state is much larger than those obtained with\nthe opposite polarization state. The errors were extracted\nemploying a constant-line fit through the measured\npolarization values as a function of center-of-mass angle.\n\nIn the case of measuring analyzing powers, a systematic\nuncertainty comes from the normalization procedure by\nconsidering the k factor in Eq. 1 as a free parameter.\nThis error turned out to be around 3%. Moreover, the\nmaximum shift in the results of Ay, Ayy, and Azz due\nto the offset current is around 0.01, 0.035, and 0.08, re-\nspectively, while the measured values of these observables\nvary between \u22120.07 to +0.35, \u22120.04 to +0.22, and \u22120.06\nto +0.3, respectively. The systematic error due to the\npossible impurity in the beam polarization is negligible\nfor the vector analyzing powers and estimated to be\nabout 0.01 (absolute) for the tensor analyzing powers.\n\n4.3 BBS\n\nSystematic errors in the measurement of the differential\ncross sections originate mainly from the errors in the\nknowledge of the target thickness and the calibration of\nthe Faraday cup. As was already stated, the error in the\ntarget thickness for the elastic dd reaction was around 5%.\nThe errors in the areal density measurements, which is the\nmass of the material divided by its area with the unit of\nmg/cm2, come from both mass measurement errors and\nthose from the measurements of the size of the target. The\nerror in the calibration of the Faraday cup was estimated\nto be 0.5%. These components, added in quadrature, were\napplied as an overall scale factor systematic for the cross\nsections (yielding a total normalization error of about 5%\nfor the elastic dd reaction).\n\nThe polarization states of the deuteron beam of the\nBBS were measured with the Ion-Beam Polarimeter (IBP)\nand found to be as follows: vector plus (0.538\u00b10.029), vec-\ntor minus (\u22120.621\u00b10.030), tensor plus (0.671\u00b10.04), and\ntensor minus (\u22121.633\u00b10.035). The main source of system-\natic uncertainty in this method comes from the analyzing\npowers measurements in the elastic d + p reaction while\nusing IBP. The polarization values for each state were\nmeasured at different beam energy ranges and found to\nbe consistent within the statistical uncertainties [62]. The\nmain sources of systematic error for the analyzing powers\ninclude the uncertainty due to beam polarization measure-\nments (py and pyy), and the total calibration error. The\ncalibration errors for (Ay, Ayy) are found to be around\n(1.2%, 1.7%). These errors introduce an overall scale fac-\ntor, since the beam polarization and initial polarimetry\ncalibration apply to all angles. Details of systematic stud-\nies are presented in [62].\n\n5 Experimental results\n\nFigure 7 shows the measured differential cross sections\nand analyzing powers for the elastic deuteron-deuteron\n\nscattering, 2H(~d, d)d. The results of BINA data are pre-\nsented as filled circles and the results of data taken by BBS\nsetup are shown as open circles [62]. The light (dark) gray\nband in each panel shows the systematic uncertainty of the\n\n\n\nR. Ramazani-Sharifabadi et al.: Title Suppressed Due to Excessive Length 9\n\n0.4\n0.8\n1.2\n1.6\n2.0\n2.4\n2.8\n3.2\n3.6\n4.0\n4.4\n\n0.0\n\n0.05\n\n0.1\n\n0.15\n\n0.2\n\n0.25\n\n0.3\n\n0.35\n\n-0.4\n\n-0.2\n\n0.0\n\n0.2\n\n0.4\n\n35 40 45 50 55 60 65 70 75\n\nBINA\nBBS\nCDBonn+\nBINA Sys Err (2 )\nBBS Sys Err (2 )\n\n-0.1\n\n0.0\n\n0.1\n\n0.2\n\n0.3\n\n35 40 45 50 55 60 65 70 75\n\nc.m. [deg]\n\nd\n/d\n\n[m\nb\n\n/s\nr]\n\nA\ny\n\nA\nzz\n\nA\nyy\n\nFig. 7. Differential cross section and analyzing powers of the elastic channel of the reaction 2H(~d, d)d are shown with statistical\nerrors for each point. The total systematic uncertainty related to BINA (BBS) results is shown with a light (dark) gray\nband for each panel. The results of BINA data are shown as filled circles and those for the BBS data are presented by open\ncircles [62]. The solid lines are the result of a calculation based on the lowest-order terms in the Born series expansion of the\nAlt-Grassberger-Sandhas equation for a four-nucleons interaction using CD-Bonn+\u2206 potential [60,57,58].\n\nBINA (BBS) data, and the error bars represent the sta-\ntistical errors which are smaller than the symbol size for\nmost of the data points. As discussed before, the results\nof the Ayy measurement obtained from BBS were used\nto normalize the offset of the current readout and hence,\nthe corresponding systematic error is the same for both se-\ntups. Therefore, just one gray band is shown in Fig. 7. The\nsolid lines are the results of a rough approximation based\non the lowest-order terms in the Born series expansion of\nthe Alt-Grassberger-Sandhas equation for a four-nucleons\ninteraction using CD-Bonn+\u2206 potential [60,57,58].\n\nThe comparison between the results of the two exper-\niments, namely data taken from BINA and BBS setups,\nindicates that both data sets are in very good agreement\nwithin the uncertainties. But, comparing the experimen-\ntal data with the theoretical approximation shows contra-\ndictions specially in the results of the analyzing powers.\nAside from the normalization in the results of the differen-\ntial cross section, the theoretical prediction follows at least\nthe shape of the experimental data. In the case of analyz-\ning powers, the comparison shows contradictory results\n\nindicating defects in the spin parts of theoretical calcula-\ntions of the scattering amplitude. As already mentioned,\nthe comparison between the results of exact calculations\nand those coming from Born approximation in Fig. 6, in-\ndicates that this approximation is not very suitable for the\n\nH(~d, dp) reaction in this energy range, and therefore, we\nexpect to observe discrepancies between Born approxima-\n\ntion and the experimental data in the 2H(~d, d)d reaction\nin Fig. 7. In fact, Born approximation may provide a rea-\nsonable estimation for observables at higher energies and\nsmall angles, but, it is not reliable in the considered en-\nergy and angle regime in this paper. It indicates that exact\ntheoretical calculations of four-body systems are a neces-\nsity to do a reasonable comparison with the experimental\ndata.\n\n6 Summary\n\nIn summary, we have analyzed the elastic chan-\n\nnel of deuteron-deuteron scattering, 2H(~d, d)d, at\n\n\n\n10 R. Ramazani-Sharifabadi et al.: Title Suppressed Due to Excessive Length\n\n65 MeV/nucleon. Two experiments were performed with\ntwo independent setups, namely BINA and BBS, which\nwere located at KVI in Groningen, the Netherlands. Cross\nsections and analyzing powers were obtained for a large\nangular range of the phase space. An excellent agreement\nis found between the measured differential cross sections\nand analyzing powers of both experiments for the angular\nrange at which they overlap. The experimental results\nare also compared with a theoretical approximation\nbased on lowest-order terms in the Born series expansion\nusing CD-Bonn+\u2206 potential. The very poor agreement\nbetween the experimental data and theoretical approx-\nimations shows the necessity of ab-initio calculations in\nthe four-body systems at intermediate energies.\n\n7 ACKNOWLEDGMENT\n\nThe authors acknowledge the work by the cyclotron and\nion-source groups at KVI for delivering a high-quality\nbeam used in these measurements. The present work has\nbeen performed with financial support from the \u201cNed-\nerlandse Organisatie voor Wetenschappelijk Onderzoek\u201d\n(NWO). This work was partly supported by Iran National\nScience Foundation (INSF) as a research project under No\n98028747.\n\nReferences\n\n1. H. Yukawa, Proc. Phys. Math. Soc. Jpn. 17, 48 (1935).\n2. S.A. Coon et al., Phys. Rev. C 48, 2559 (1993).\n3. E. Epelbaum et al., Phys. Rev. C 66, 064001 (2002).\n4. M. C. M. Rentmeester et al., Phys. Rev. Lett. 82, 4992\n\n(1999).\n5. R. B. Wiringa, R. A. Smith, and T. L. Ainsworth, Phys.\n\nRev. C 29, 1207 (1984).\n6. H. Primakoff and T. Holstein Phys. Rev. 55, 1218 (1939).\n7. S. Pieper, V. Pandharipande, and R. Wiringa, Phys. Rev.\n\nC 64, 1 (2001).\n8. H. Witala et al., Phys. Rev. Lett. 81, 11 (1998).\n9. K. Ermisch et al., Phys. Rev. Lett. 86, 5862 (2001).\n10. K. Ermisch et al., Phys. Rev. C 68, 051001(R) (2003).\n11. K. Ermisch et al., Phys. Rev. C 71, 064004 (2005).\n12. H. Sakai et al., Phys. Rev. Lett. 84, 5288 (2000).\n13. K. Sekiguchi et al., Phys. Rev. C 65, 034003 (2002).\n14. K. Sekiguchi et al., Phys. Rev. Lett. 95, 162301 (2005).\n15. H. Postma and R. Wilson, Phys. Rev. 121, 1129 (1961).\n16. H. Amir-Ahmadi et al., Phys. Rev. C 75, 041001(R)\n\n(2007).\n17. K. Kuroda et al., Nucl. Phys. 88, 33 (1966).\n18. P. Mermod et al., Phys. Rev. C 72, 061001(R) (2005).\n19. G. Igo et al., Nucl. Phys. A 195, 33 (1972).\n20. R. E. Adelberger and C. N Brown, Phys. Rev. D 5, 2139\n\n(1972).\n21. H. Mardanpour et al., Eur. Phys. J. A 31, 383 (2007).\n22. E. Stephan et al., Phys. Rev. C 76, 057001 (2007).\n23. H. Shimizu et al., Nucl. Phys. A 382, 242 (1982).\n24. K. Hatanaka et al., Eur . Phys. J. A 18, 293 (2003).\n25. E. J. Stephenson et al., Phys. Rev. C 60, 061001 (1999).\n\n26. A. Ramazani-Moghaddam-Arani et al., Phys. Rev. C 78,\n014006 (2008).\n\n27. A. Ramazani-Moghaddam-Arani et al., Few-Body Syst 44,\n27 (2008).\n\n28. St. Kistryn et al., Phys. Rev. C 68, 054004 (2003).\n29. St. Kistryn et al., Phys. Rev. C 72, 044006 (2005).\n30. St. Kistryn et al., Phys. Lett. B 641, 23 (2006).\n31. N. Kalantar-Nayestanaki et al., Nucl. Instrum. and Meth.\n\nin Phys. Res. A 444, 591 (2001).\n32. H. Mardanpour et al., Phys. Lett. B 687, 149 (2010).\n33. E. Stephan et al., Phys. Rev. C 82, 014003 (2010).\n34. R. Ramazani-Sharifabadi et al., Eur. Phys. J. A 55, 177\n\n(2019).\n35. I. Ciepa l et al., Phys. Rev. C 100, 024003 (2019).\n36. R. Ramazani-Sharifabadi et al., Eur. Phys. J. A 56, 221\n\n(2020).\n37. H. Tavakoli-Zaniani et al., Eur. Phys. J. A 56, 62 (2020).\n38. S. Shimizu et al., Phys. Rev. C 52, 1193 (1995).\n39. N. Kalantar-Nayestanaki et al., Rep. Prog. Phys. 75,\n\n016301 (2012).\n40. S. Nemoto et al., Phys. Rev. C 58, 2599 (1998).\n41. T. W. Phillips et al., Phys. Rev. C 22, 384 (1980).\n42. M. Viviani et al., Phys. Rev. Lett. 86, 3739 (2001).\n43. B. M. Fisher et al., Phys. Rev. C 74, 034001 (2006).\n44. A. Deltuva et al., Phys. Rev. C 86, 011001(R) (2012).\n45. A. Deltuva et al., Phys. Rev. C 87, 054002(R) (2013).\n46. A. Deltuva et al., Phys. Rev. Lett. 113, 102502 (2014).\n47. A. Deltuva et al., Phys. Rev. C 90, 044002 (2014).\n48. R. Lazauskas, Phys. Rev. C 91, 041001(R) (2015).\n49. A. Deltuva et al., Phys. Rev. C 91, 034001 (2015).\n50. A. Deltuva et al., Phys. Lett. B 742, 285-289 (2015).\n51. A. Deltuva et al., Phys. Rev. C 95, 024003 (2017).\n52. V. Bechtold et al., Nucl. Phys. A 288, 189 (1977).\n53. C. Alderliesten et al., Phys. Rev. C 18, 2001 (1978).\n54. M. Garcon et al., Nucl. Phys. A 458, 287 (1986).\n55. C. Elster et al., Phys. Rev. C 55, 1058 (1997).\n56. E. Uzu et al., Phys. Rev. C 68, 061001 (2003).\n57. A. Deltuva et al., Phys. Rev. C 92, 024001 (2015).\n58. A. Deltuva et al., Phys. Rev. C 93, 044001 (2016).\n59. E. J. Stephenson et al., Phys. Rev. Lett. 91, 142302 (2003).\n60. A. M. Micherdzinska et al., Phys. Rev. C 75, 054001\n\n(2007).\n61. R. Ramazani-Sharifabadi, Ph.D. thesis, University of\n\nGroningen, (2020).\n62. C. D. Bailey, Ph.D. thesis, Indiana University (2009).\n63. L. Friedrich et al., Polarized beams and polarized gas tar-\n\ngets (World Scientific, Singapore) , p. 198, (1995).\n64. H. R. Kremers et al., Polarized Gas Targets and Polarized\nBeams, AIP Conj Proc. 421, p. 507 (1997).\n\n65. A. Ramazani-Moghaddam-Arani et al., Phys. Rev. C 83,\n024004 (2011).\n\n66. H. Mardanpour, Ph.D. thesis, University of Groningen,\n(2008).\n\n67. N. Kalantar-Nayestanaki, J. Mulder, and J. Zijlstra Nucl.\nInstrum. and Meth. in Phys. Res. A 417, 215 (1998).\n\n68. A.M. van den Berg, Nucl. Instrum. and Meth. in Phys.\nRes. B 99, 637-640 (1995).\n\n69. A. Ramazani-Moghaddam-Arani, Ph.D. thesis, University\nof Groningen, (2009).\n\n70. R. Bieber et al., Nucl. Instrum. and Meth. in Phys. Res.\nA 457, 12 (2001).\n\n71. H. R. Kremers et al., Nucl. Instrum. and Meth. in Phys.\nRes. A 516, 209 (2004).\n\n72. G. G. Ohlsen, Nucl. Instr. Meth. 179, 283 (1981).\n73. A. Deltuva et al., Phys. Rev. C 68, 024005 (2003).\n\n\n\t1 Introduction\n\t2 Experimental setups\n\t3 Event selection and data analysis method\n\t4 Systematic uncertainties\n\t5 Experimental results\n\t6 Summary\n\t7 ACKNOWLEDGMENT\n\n"
"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nar\nX\n\niv\n:2\n\n10\n1.\n\n05\n58\n\n3v\n1 \n\n [\nm\n\nat\nh.\n\nN\nT\n\n] \n 1\n\n4 \nJa\n\nn \n20\n\n21\n\nMOCK MODULAR FORMS WITH INTEGRAL FOURIER\n\nCOEFFICIENTS\n\nYINGKUN LI AND MARKUS SCHWAGENSCHEIDT\n\nAbstract. In this note, we explicitly construct mock modular forms with integral Fourier\ncoefficients by evaluating regularized Petersson inner products involving their shadows,\nwhich are unary theta functions of weights 1\n\n2\nand 3\n\n2\n. In addition, we also improve the known\n\nbounds for the denominators of the coefficients of mock modular forms whose shadows are\nholomorphic weight one cusp forms constructed by Hecke.\n\n1. Introduction\n\nIn his groundbreaking thesis [25], Zwegers discovered the modular property of Ramanujan\u2019s\nmock theta functions by completing them with real-analytic functions, whose images under\nthe lowering operator are essentially complex conjugates of weight 3\n\n2\nunary theta functions.\n\nIn the sense of Bruinier and Funke [8], these completions are harmonic Maass forms of weight\n\nk = 1\n2\n, whose images under the differential operator \u03bek := 2iv\n\nk \u2202\n\u2202\u03c4\n\nare unary theta functions\n\nof weight 3\n2\n. The holomorphic part and \u03be-image of a harmonic Maass form are called a mock\n\nmodular form and its shadow (see [23]).\nTo establish the modular property of Ramanujan\u2019s classical mock theta functions, Zwegers\n\ngave three different constructions of these weight 1\n2\nharmonic Maass forms using Appell-Lerch\n\nsums, indefinite theta functions, and Fourier coefficients of meromorphic Jacobi forms, re-\nspectively. Bringmann and Ono [5] proved that generating series of certain partition numbers\nyield mock modular forms of weight 1\n\n2\nwhose shadows are linear combinations of unary theta\n\nfunctions. In the case of weight 3\n2\nmock modular forms with holomorphic theta functions as\n\nshadows, the prominent example is the generating series of Hurwitz class numbers that ap-\npeared in the seminal work of Hirzebruch and Zagier [19]. Building on the work of Zwegers,\nthe paper [4] also constructed such mock modular forms and related them to q-series.\n\nFor an arbitrary unary theta function of weight 1\n2\nor 3\n\n2\n, Bruinier and the second author [11]\n\ngave a different construction of its \u03be-preimage using regularized theta lifts, and expressed\nthe Fourier coefficients of the holomorphic part in terms of CM values of modular functions.\nFrom these constructions, one can apply the theory of complex multiplication to show that\nthese Fourier coefficients are rational. A drawback of the construction from [11] is the fact\nthat it does not yield an explicit bound on the denominators of the coefficients.\n\nDate: January 15, 2021.\n1\n\nhttp://arxiv.org/abs/2101.05583v1\n\n\n2 YINGKUN LI AND MARKUS SCHWAGENSCHEIDT\n\nIn some cases, Charollois and the first author [13] constructed mock modular forms whose\nshadows are weight 3\n\n2\nunary theta functions, and whose coefficients are simple finite sums of\n\nrational numbers. In particular, they obtained explicit bounds on the denominators of the\ncoefficients. In the present article, we give another construction of \u03be-preimages of weight 1\n\n2\n\nand 3\n2\nunary theta functions, and show that their Fourier coefficients are rational numbers\n\nwith explicitly bounded denominators. Let us explain our results in more detail.\nFor N \u2208 N we denote by C[Z/2NZ] the group ring spanned by the formal basis symbols\n\neh for h \u2208 Z/2NZ. Then for \u03bd \u2208 {0, 1} the vector-valued unary theta function\n\n\u03b8N(\u03c4 ; \u03bd) :=\n\u2211\n\nh (mod 2N)\n\n\u03b8N,h(\u03c4 ; \u03bd)eh, \u03b8N,h(\u03c4 ; \u03bd) :=\n\u2211\n\nn\u22082NZ+h\nn\u03bdq\n\nn2\n\n4N ,(1.1)\n\nwith q := e2\u03c0i\u03c4 , is a holomorphic modular form of weight \u03bd + 1\n2\nfor the Weil representation\n\n\u03c1N of Mp2(Z) (see Section 2). Our first main result is as follows.\n\nTheorem 1.1. Let \u03b8N (\u03c4 ; \u03bd) be as in (1.1).\n\n(1) There exists a mock modular form \u03b8\u0303+N (\u03c4 ; 1) of weight\n1\n2\nwith shadow 1\u221a\n\nN\n\u03b8N (\u03c4 ; 1) such\n\nthat 48N\u03b8\u0303+N (\u03c4 ; 1) has integral Fourier coefficients.\n\n(2) There exists a mock modular form \u03b8\u0303+N (\u03c4 ; 0) of weight\n3\n2\nwith shadow\n\n\u221a\nN\n\u03c0\n\u03b8N (\u03c4 ; 0) such\n\nthat 72N\u03b8\u0303+N (\u03c4 ; 0) has integral Fourier coefficients.\n\nRemark 1.2. As usual, \u03b8\u0303+N (\u03c4 ; \u03bd) is the holomorphic part of a harmonic Maass form \u03b8\u0303N(\u03c4 ; \u03bd)\nof weight 3\n\n2\n\u2212 \u03bd for the complex conjugate \u03c1N of the Weil representation \u03c1N on Mp2(Z).\n\nRemark 1.3. Here is a trick to reduce the denominator. Given two mock modular forms f1, f2\nwith the same shadow g such that Mjfj has integral Fourier coefficients for Mj \u2208 N, we can\nfind aj \u2208 Z such that a1M1 + a2M2 = gcd(M1,M2). Then a1M1f1+a2M2f2gcd(M1,M2) is mock modular\nwith shadow g, and the denominators of its coefficients are bounded by gcd(M1,M2).\n\nThe denominator bounds we obtained above can be used to give a denominator bound of\nthe Weyl vectors for Borcherds products in signature (1, 2) (see [11]), which is closely related\nto the orders of the multiplier systems of such Borcherds products1. Also, the rational Fourier\ncoefficients in Theorem 1.1 appear in other formulas, such as the Fourier coefficients of mock\nmodular forms with binary theta functions as shadows [15], and CM values of higher Green\u2019s\nfunctions [7,20]. A denominator bound as we have proved here would make precise the fields\nof definition of the algebraic values appearing in those formulas.\n\nFor the proof of Theorem 1.1 we will compute, for any weakly holomorphic modular form\ng of weight \u03bd + 1\n\n2\nfor \u03c1N , the regularized Petersson inner product\n\n(g(\u03c4), \u03b8N(\u03c4 ; \u03bd))\nreg := lim\n\nT\u2192\u221e\n\n\u222b\n\nFT\n\u3008g(\u03c4), \u03b8N(\u03c4 ; \u03bd)\u3009v\u03bd+\n\n1\n2\ndudv\n\nv2\n, (\u03c4 = u+ iv \u2208 H),\n\n1The order of the multiplier system of a Borcherds product is always finite (see [3]).\n\n\n\nMOCK MODULAR FORMS 3\n\nwhere FT is the usual fundamental domain for SL2(Z) truncated at the height T , and \u3008\u00b7, \u00b7\u3009\ndenotes the natural hermitian pairing on C[Z/2NZ] (see Section 2). From this we will\n\nreconstruct \u03b8\u0303+N(\u03c4 ; \u03bd) using Serre duality (see Proposition 2.5). The basic idea to compute\nthe regularized Petersson inner product is to use the identity\n\n\u3008g(\u03c4), \u03b8N(\u03c4 ; \u03bd)\u3009 = \u3008g(\u03c4)\u03b7(\u03c4)\u22124, \u03b8N (\u03c4 ; \u03bd)\u03b7(\u03c4)4\u3009\n\nand realize \u03b8N (\u03c4 ; \u03bd)\u03b7(\u03c4)4 as a special value of a signature (1, 4) theta function. Then the\ninner product (g(\u03c4), \u03b8N(\u03c4 ; \u03bd))\n\nreg can be viewed as a special value of a regularized theta\nlift of the weakly holomorphic modular form g(\u03c4)\u03b7(\u03c4)\u22124, which can be evaluated using the\nmethods developed by Borcherds [1] and Bruinier [6]. To make the argument rigorous, we will\nimplement this idea for vector-valued forms. This technicality causes us to have the factors\n48 and 72 in the theorem above, which could be reduced in certain cases (see Remark 4.1).\nWe refer the reader to Section 4 for the details of the proof of Theorem 1.1. Note that this\nidea has been used when \u03b8N (\u03c4 ; \u03bd) is replaced by a holomorphic binary theta function to\nproduce harmonic Maass forms of weight one [15, 21].\n\nThe mock modular form \u03b8\u0303N (\u03c4 ; \u03bd) from Theorem 1.1 will be constructed explicitly in the\nproof of the theorem, though it is not canonical and involves choosing a suitable lattice L,\na primitive isotropic vector \u2113 \u2208 L and its dual \u2113\u2032 \u2208 L\u2032 (see e.g. (4.2)). By slightly modifying\nthe method above, we will also give simpler formulas for mock modular forms with shadow\n\u03b8N (\u03c4 ; \u03bd) in Section 6. To give an impression, we include a special case in the introduction.\n\nProposition 1.4. Let H(n) be the Hurwitz class numbers (with H(0) := \u2212 1\n12\n). Then\n\n(1.2)\n\u2211\n\nn\u22650\nH(n)qn =\n\n1\n\n24\u03b7(4\u03c4)\n\n\u2211\n\na\u2282Z[\n\u221a\n6]\n\n\u03c66(a)q\nNm(a)/6 =\n\n1\n\n24\u03b7(4\u03c4)3\n\n\u2211\n\na\u2282Z[\n\u221a\n2]\n\n\u03c62(a)q\nNm(a)/2\n\nwith the functions \u03c66, \u03c62 defined by\n\n\u03c66(a) :=\n\n{(\n12\n\nTr(\u03bb)/2\n\n)\nTr\n(\n\n|\u03bb|\n2\u2212\n\n\u221a\n6\n\n)\n, if a = (\u03bb) satisfies 49\u2212 20\n\n\u221a\n6 < \u03bb\n\n\u03bb\u2032\n\u2264 1,\n\n0, otherwise.\n,\n\n\u03c62(a) :=\n\n{\n\u2212\n(\n\n\u22124\nTr(|\u03bb|/2)\n\n)\nTr\n(\n\n\u03bb2\n\u221a\n2\n\n3\u2212\n\u221a\n2\n\n)\n, if a = (\u03bb) satisfies 17\u2212 12\n\n\u221a\n2 < \u03bb\n\n\u03bb\u2032\n\u2264 1,\n\n0, otherwise.\n\n(1.3)\n\nThe proposition follows from Propositions 6.1 and 6.7 below. There we construct for any\n\nN \u2208 N an explicit mock modular form with shadow\n\u221a\nN\n\u03c0\n\u03b8N (\u03c4 ; 0). Specializing to N = 1,\n\nand using that the generating series of Hurwitz class numbers is a mock modular form with\nshadow \u2212 8\n\n\u03c0\n\u03b81(\u03c4 ; 0) (see [22]), we easily obtain Proposition 1.4 by comparing the first few\n\nFourier coefficients. The first identity can probably be derived from the classical results in\n\u00a72.3 of [19], whereas the second seems new to us. Note that for N = 2, Proposition 6.1 also\n\n\n\n4 YINGKUN LI AND MARKUS SCHWAGENSCHEIDT\n\nproduces the following well-known mock modular form (see [14])\n\n\u03b8\u0303+2 (\u03c4) :=\nE2(\u03c4)/24\u2212 F (2)2 (\u03c4)\n\n\u03b7(\u03c4)3\n= \u2212\n\nq\u22121/8\n\n24\n(\u22121 + 45q + 231q2 + 770q3 +O(q4)),\n\nE2(\u03c4) := 1\u2212 24\n\u2211\n\nn\u22651\n\u03c31(n)q\n\nn, F\n(2)\n2 (\u03c4) :=\n\n\u2211\n\nb>a>0\nb\u2212a odd\n\na(\u22121)bqab/2,\n(1.4)\n\nwhose shadow is 1\n2\n\u221a\n2\n\u03b7(\u03c4)3.\n\nUsing the same idea as in the proof of Theorem 1.1, we can also improve a denominator\nbound for certain weight one harmonic Maass forms. Let m \u2282 OF be an integral ideal in\na real quadratic field F = Q(\n\n\u221a\nD) with ring of integers OF , and \u03d5 an odd ray class group\n\ncharacter of conductor m \u00b7 \u221e1 with Nm(m) = M , which we view as a function on integral\nideals by extending with 0. Hecke associated the holomorphic weight one, level N = DM\neigenform\n\nf\u03d5(\u03c4) :=\n\u2211\n\na\u2282OF\n\n\u03d5(a)qNm(a)\n\nto \u03d5 in [18]. In [13], Charollois and the first author showed that there is a mock modular\nform f+\u03d5 (\u03c4) =\n\n\u2211\nn\u226b\u2212\u221e c\n\n+\n\u03d5 (n)q\n\nn with shadow f\u03d5 satisfying\n\n(1.5) c+\u03d5 (n)\u2212\n\u2211\n\n[a]\u2208ClF\n\n\u03d5(a)\n\u2211\n\n(\u03bb)\u2282a\nNm((\u03bb)a\u22121)=n\n\n\u03d5(\u03bb) log\n\n\u2223\u2223\u2223\u2223\n\u03bb\n\n\u03bb\u2032\n\n\u2223\u2223\u2223\u2223 \u2208\n1\n\n\u03ba\nZ[\u03d5] \u00b7 log \u03b5F\n\nwith \u03ba = 96DM3\n\u220f\n\np|2M prime(1 + p\n\u22121).2 Here \u03b5F \u2208 O\u00d7F is the fundamental unit, Z[\u03d5] \u2282 C is\n\nthe subring generated by the values of \u03d5, and we have chosen a set of representatives of the\nclass group ClF . The constant \u03ba comes from bounding the denominator of a mixed mock\nmodular form of weight 1 in Theorem 5.1 of [13]. We can now improve it as follows.\n\nTheorem 1.5. In the notations above, we can take \u03ba = 96N = 253DM .\n\nExample 1.6. The constant 96N above can sometimes be reduced. LetD = 12,m = 2\n\u221a\n3OF\n\nand define \u03d5(a) = sgn(\u03bb) \u00b7\u03d50(\u03bb) for an integral ideal a = (\u03bb) with \u03d50 : OF \u2192 {\u00b11} given by\n\n(1.6) \u03d50(\u03bb) :=\n\n\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f3\n\n1, if \u03bb \u2261 1, 2 +\n\u221a\n3 (mod 2\n\n\u221a\n3),\n\n\u22121, if \u03bb \u2261 5, 4 +\n\u221a\n3 (mod 2\n\n\u221a\n3),\n\n0, otherwise.\n\n2There is a mistake in (4.2.7) of [13], where A3 should be replaced by A2D. This affects Theorem 1.1,\n5.1, 6.5, where M3\u03c6(2M) should be replaced by DM2\u03c6(2M).\n\n\n\nMOCK MODULAR FORMS 5\n\nThen f\u03d5(\u03c4/12) =\n\u2211\n\na\u2282OF \u03d5(a)q\nNm(a)/12 = \u03b7(\u03c4)2. Furthermore, we define\n\n(1.7) \u03d5\u0303(a) := \u03d5(a) log\n\n\u2223\u2223\u2223\u2223\n\u03bb\n\n\u03bb\u2032\n\n\u2223\u2223\u2223\u2223\n\nif a = (\u03bb) with 2\u2212\n\u221a\n3 <\n\n\u2223\u2223 \u03bb\n\u03bb\u2032\n\n\u2223\u2223 \u2264 2 +\n\u221a\n3. Then John Duncan asked if the function\n\n(1.8) \u03d1\u0303+(\u03c4) := 4 log(2 +\n\u221a\n3)\u03b7(\u03c4)\u03b8\u0303+2 (\u03c4) +\n\n\u2211\n\na\u2282OF\n\n\u03d5\u0303(a)qNm(a)/12\n\nis a mock modular form with shadow \u03b7(\u03c4)2. This is indeed the case and follows from the\nresults in [13]. We will give the details in Section 5.\n\nThe paper is organized as follows. We start with a section on the necessary preliminaries\nabout vector-valued harmonic Maass forms for the Weil representation, unary theta functions\nand their connection to the Dedekind eta function, and the relation between the evaluation of\nregularized inner products and mock modularity. In Section 3 we evaluate several regularized\ntheta lifts in signature (1, n). Sections 4 and 5 are devoted to the proofs of Theorem 1.1\nand Theorem 1.5. Finally, in Section 6 we give explicit constructions of mock modular forms\nwhose shadows are unary theta functions.\nAcknowledgement: We thank John Duncan for sharing the observation of Example 1.6.\n\n2. Preliminaries\n\n2.1. Modular forms for the Weil representation. We recall some facts about harmonic\nweak Maass forms for the Weil representation associated with an even lattice from [6, 8].\nLet Mp2(R) be the metaplectic two-fold cover of SL2(R) consisting of elements (A, \u03c6) with\nA = ( a bc d ) \u2208 SL2(R) and \u03c6 : H \u2192 C holomorphic with \u03c6(\u03c4)2 = c\u03c4 + d. Let Mp2(Z) \u2282\nMp2(R) denote the inverse image of SL2(Z) under the covering map Mp2(R) \u2192 SL2(R). It is\ngenerated by T := (( 1 10 1 ) , 1) and S := ((\n\n0 \u22121\n1 0 ) ,\n\n\u221a\n\u03c4 ). We let \u0393\u0303\u221e be the subgroup generated\n\nby T .\nLet L be an even lattice with quadratic form Q of signature (b+, b\u2212), the dual lattice L\u2032\n\nand the associated finite quadratic module (or discriminant form)\n\n(2.1) AL := L\n\u2032/L,\n\non which Q becomes a quadratic form valued in Q/Z. Let (\u00b7, \u00b7) be the associated bilinear\nform. Moreover, we let \u0393L be the discriminant kernel of L, which is the subgroup of the\northogonal group O(L) which acts trivially on AL. For convenience, we use L\n\n\u2212 to denote\nthe lattice L with the quadratic form \u2212Q.\n\n\n\n6 YINGKUN LI AND MARKUS SCHWAGENSCHEIDT\n\nThe group ring C[AL] :=\n\u2295\n\nh\u2208AL Ceh is naturally an Mp2(Z)-module via the Weil repre-\nsentation \u03c1L defined by\n\n\u03c1L(T )(eh) := e(Q(h))eh,\n\n\u03c1L(S)(eh) :=\ne((b\u2212 \u2212 b+)/8)\u221a\n\n|AL|\n\u2211\n\n\u00b5\u2208AL\n\ne(\u2212(h, \u00b5))e\u00b5,\n(2.2)\n\nwhere we put e(x) := e2\u03c0ix for x \u2208 C. Despite the notation, \u03c1L only depends on the\nfinite quadratic module AL. There is a natural hermitian pairing \u3008\u00b7, \u00b7\u3009 on C[AL] given by\n\u3008eh1, eh2\u3009 := 1 if h1 = h2 and zero otherwise. With respect to this pairing \u03c1L is a unitary\nrepresentation.\n\nA real-analytic function f : H \u2192 C[AL] is called modular of weight k \u2208 12Z with respect\nto \u03c1L if\n\n(2.3) (f |k,L(A, \u03c6))(\u03c4) := \u03c6(\u03c4)\u22122k\u03c1\u22121L ((A, \u03c6))f(Az) = f(\u03c4)\n\nfor all (A, \u03c6) \u2208 Mp2(Z) and \u03c4 \u2208 H. We denote the spaces of harmonic Maass, weakly\nholomorphic, holomorphic, and cuspidal forms of weight k for \u03c1L by\n\nHk,L \u2283 M !k,L \u2283 Mk,L \u2283 Sk,L,\n\nrespectively. More generally, for any representation \u03c1 of Mp2(Z), the subscript L in the\nnotation above will be replaced by \u03c1.\n\nEvery f \u2208 Hk,L can be written uniquely as f = f+ + f\u2212, where f+ is holomorphic and\nhas a Fourier expansion of the form\n\nf+(\u03c4) =\n\u2211\n\nh\u2208AL\n\n\u2211\n\nm\u2208Z+Q(h)\nm\u226b\u2212\u221e\n\naf (h,m)q\nm\neh(2.4)\n\nwith coefficients af (h,m) \u2208 C. We may assume that 2k \u2261 sgn(L) (mod 2) since other-\nwise the action \u03c1L(S\n\n2)eh = i\n\u2212sgn(L)\n\ne\u2212h implies that Hk,L is trivial. Moreover, under this\nassumption the coefficients above satisfy the symmetry\n\naf (\u2212h,m) = (\u22121)k\u2212\nsgn(L)\n\n2 af (h,m)\n\nfor every h \u2208 AL and m \u2208 Z+ Q(h).\nThe antilinear differential operator \u03bek = 2iv\n\nk \u2202\n\u2202\u03c4\n\ndefines a surjective map Hk,L \u2192 M !2\u2212k,L\u2212\n(see Theorem 3.7 in [8]). A holomorphic C[AL]-valued q series as in (2.4) is called a mock\nmodular form of weight k with shadow g if it is the holomorphic part f+ of a harmonic\nMaass form f \u2208 Hk,L which satisfies g = \u03bekf .\n\nExamples of harmonic Maass forms can be constructed as special values of Maass Poincare\u0301\nseries. Let k \u2264 0. Following Section 1.3 in [6], for h \u2208 AL and m \u2208 Z+Q(h) with m < 0 we\n\n\n\nMOCK MODULAR FORMS 7\n\nconsider the Maass Poincare\u0301 series\n\nFh,m(\u03c4, s) :=\n1\n\n2\u0393(2s)\n\n\u2211\n\n(A,\u03c6)\u2208\u0393\u0303\u221e\\Mp2(Z)\n\nMs(4\u03c0|m|v)e(mu)eh|k,L(A, \u03c6),\n\nwhere Ms(v) := v\u2212\nk\n2M\u2212 k\n\n2\n,s\u2212 1\n\n2\n(v) with the usual M-Whittaker function. We put3\n\nFh,m(\u03c4) := Fh,m\n\n(\n\u03c4, 1\u2212 k\n\n2\n\n)\n= qm(eh + e\u2212h) +O(1),(2.5)\n\nwhich defines a harmonic Maass form in Hk,L that maps to a cusp form under \u03bek.\n\n2.2. Unary theta series and the eta function. For N \u2208 N we consider the lattice\n(2.6) Z[N ] := (Z, Nx2).\n\nWe can identify its discriminant form with Z/2NZ, and by a slight abuse of notation, we\nput AN := C[Z/2NZ] and write \u03c1N for the Weil representation associated with Z[N ]. The\nunary theta function \u03b8N (\u03c4 ; \u03bd) defined in (1.1) is a holomorphic modular form of weight \u03bd+\n\n1\n2\n\nfor \u03c1N . For \u03bd = 1 it is a cusp form.\nFamiliar modular forms can be expressed in terms of unary theta functions. For example,\n\n\u03b7(\u03c4) = \u03b86,1(\u03c4 ; 0)\u2212 \u03b86,5(\u03c4 ; 0), \u03b7(\u03c4)3 = \u03b82,1(\u03c4).(2.7)\nTo generalize this situation, it is convenient to phrase these identities in terms of eigenvectors\nof the Weil representation. Define\n\nv3 := e(0,1) + e(1,0) + e(0,\u22121) + e(\u22121,0) \u2212 (e(3,2) + e(2,3) + e(3,\u22122) + e(\u22122,3)) \u2208 C[A3 \u2295A3],\nv2 := e1 \u2212 e3 \u2208 C[A2], v6 := e1 \u2212 e5 \u2212 e7 + e11 \u2208 C[A6], v4 := v3 \u2297 v3 \u2208 C[A43].\n\n(2.8)\n\nHere we write (\n\u2211\n\n\u00b5 a\u00b5e\u00b5)\u2297 (\n\u2211\n\n\u03bd b\u03bde\u03bd) :=\n\u2211\n\n\u00b5,\u03bd a\u00b5b\u03bde(\u00b5,\u03bd).\n\nLemma 2.1. In the notations above, the vector v2 (resp. v3, v6, v4) generates a 1-dimensional\nsubspace invariant under \u03c12 (resp. \u03c13 \u2297 \u03c13, \u03c16, \u03c1\u229743 ), which acts on the space via \u03c73 (resp.\n\u03c72, \u03c7, \u03c74). Here \u03c7 : Mp2(Z) \u2192 C\u00d7 is the character of Mp2(Z) defined by\n(2.9) \u03c7(T ) = e2\u03c0i/24, \u03c7(S) = 1.\n\nFurthermore, we have\n\n\u3008\u03b86(\u03c4 ; 0), v6\u3009 = 2\u03b7(\u03c4),\n\u3008\u03b82(\u03c4 ; 1), v2\u3009 = 2\u03b7(\u03c4)3,\n\u3008\u03b83(\u03c4 ; 0)2, v3\u3009 = 4\u03b7(\u03c4)2,\n\u3008\u03b83(\u03c4 ; 0)4, v4\u3009 = 16\u03b7(\u03c4)4.\n\n(2.10)\n\n3Note that for k = 0 the Maass Poincare\u0301 series Fh,m(\u03c4, s) does not converge at s = 1, but it can be\nanalytically continued to s = 1 via its Fourier expansion.\n\n\n\n8 YINGKUN LI AND MARKUS SCHWAGENSCHEIDT\n\nRemark 2.2. Note that the first two equations in (2.10) are equivalent to (2.7), and the third\nequation has been known to Weber.\n\nRemark 2.3. For all 0 \u2264 r \u2264 23, the space Mr/2,\u03c7r = Sr/2,\u03c7r is 1-dimensional and spanned\nby \u03b7(\u03c4)r.\n\nProof of Lemma 2.1. The first claim can be checked locally at each prime p, as\n\nv6 = (e1 \u2212 e\u22121)\u2297 (e1 \u2212 e\u22121) \u2208 C[A6 \u2297 Z2]\u2297 C[A6 \u2297 Z3],\nv3 = (e(0,1) \u2212 e(1,0))\u2297 (e(0,1) \u2212 e(1,0) + e(0,\u22121) \u2212 e(\u22121,0)) \u2208 C[A23 \u2297 Z2]\u2297 C[A23 \u2297 Z3].\n\nThe second claim follows from Remark 2.3 and comparing Fourier coefficients. \ufffd\n\n2.3. Regularized inner product and pairing. Let L be an even lattice and k \u2208 1\n2\nZ\n\nsatisfying 2k \u2261 sgn(L) (mod 2). Denote\n(2.11)\n\nVk,L :=\n\n\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f3\n\n\u2211\n\nh\u2208AL\n\n\u2211\n\nm\u2208Z+Q(h)\nm\u226b\u2212\u221e\n\na(h,m)qmeh : a(\u2212h,m) = (\u22121)k\u2212\nsgn(L)\n\n2 a(h,m) for all h,m\n\n\uf8fc\n\uf8f4\uf8fd\n\uf8f4\uf8fe\n\n,\n\na vector space of formal Laurent series with values in C[AL], which contains the space M\n!\nk,L\n\nof weakly holomorphic modular forms. Furthermore, let V +k,L denote the subspace consisting\nof formal power series (supported on indices m \u2265 0), which contains the space Mk,L of\nholomorphic modular forms. On the space Vk,L \u00d7 V2\u2212k,L\u2212 we define the bilinear pairing\n\n(2.12) {g, f} := CTq=0\n\u2211\n\nh\u2208AL\n\nghfh.\n\nWhen restricted to M !k,L \u00d7 M !2\u2212k,L\u2212, this pairing vanishes identically. Furthermore, if g \u2208\nM !k,L, and f = G\u0303\n\n+ is the holomorphic part of a harmonic Maass form G\u0303 of weight 2 \u2212 k\nfor \u03c1L\u2212 such that \u03bekG\u0303 = G \u2208 Mk,L, an application of Stokes\u2019 theorem gives us (see e.g.\nProposition 3.5 in [8])\n\n(2.13) {g, G\u0303+} = (g,G)reg := lim\nT\u2192\u221e\n\n\u222b\n\nFT\n\u3008g(\u03c4), G(\u03c4)\u3009vkdudv\n\nv2\n,\n\nwhere FT is the usual fundamental domain for SL2(Z) truncated at the height T . It is clear\nthat the pairing vanishes on V +k,L \u00d7 S2\u2212k,L\u2212. Denote Wk,L \u2282 Vk,L the subspace spanned by\nV +k,L and M\n\n!\nk,L. By Serre duality (compare Theorem 3.1 in [2]), we know that\n\n(2.14) S2\u2212k,L\u2212 = W\n\u22a5\nk,L \u2282 V2\u2212k,L\u2212, Wk,L = S\u22a52\u2212k,L\u2212 \u2282 Vk,L,\n\nwhere the orthogonal complement is taken with respect to the pairing {\u00b7, \u00b7}. From this, we\ncan deduce the following result.\n\n\n\nMOCK MODULAR FORMS 9\n\nLemma 2.4. The pairing {\u00b7, \u00b7} induces a perfect pairing on Vk,L/M !k,L \u00d7M !2\u2212k,L\u2212.\n\nProof. Suppose f \u2208 (M !\n2\u2212k,L\u2212)\n\n\u22a5 \u2282 Vk,L. Then f \u2208 S\u22a52\u2212k,L\u2212 = Wk,L by (2.14), and we can\nwrite f = f1 + f2 with f1 \u2208 V +k,L and f2 \u2208 M !k,L \u2282 (M !2\u2212k,L\u2212)\u22a5. That means f1 is in\n(M !\n\n2\u2212k,L\u2212)\n\u22a5 \u2229 (V +\n\n2\u2212k,L\u2212)\n\u22a5, and hence in W\u22a5\n\n2\u2212k,L\u2212. Again by (2.14), we have f1 \u2208 Sk,L, hence\nf = f1 + f2 \u2208 M !k,L. \ufffd\n\nProposition 2.5. Let G \u2208 Mk,L, and suppose that G\u0303+ \u2208 V2\u2212k,L\u2212 satisfies\n\n(2.15) {g, G\u0303+} = (g,G)reg\n\nfor all g \u2208 M !k,L. Then G\u0303+ is a mock modular form with shadow G.\nProof. The case G = 0 follows directly from Lemma 2.4. More generally, we can subtract\n\nfrom G\u0303+ a known mock modular form with shadow G to reduce to the case G = 0. \ufffd\n\n3. Theta lifts\n\nIn this section, we compute some regularized theta lifts for lattices of signature (1, n) for\nn \u2265 1. The formulas in this section are special cases of the general results of Borcherds [1]\nand Bruinier [6], but we write down the simplifications for the convenience of the reader.\n\nThroughout this section, we let L be an even lattice of signature (1, n) with n \u2265 1, and we\nfix an isotropic vector \u2113 \u2208 VR := L\u2297 R. If L is isotropic, we will choose \u2113 to be a primitive\nisotropic vector in L. We let Gr(L) denote the Grassmannian of positive lines in VR. For\nz \u2208 Gr(L) we consider the polynomials\n\npz(\u03bb) :=\n\n(\n\u03bb,\n\n\u2113z\n|\u2113z|\n\n)\n, pz\u22a5(\u03bb) :=\n\n(\n\u03bb,\n\n\u2113z\u22a5\n\n|\u2113z\u22a5|\n\n)\n,\n\non VR, where we write \u03bbz for the projection of \u03bb \u2208 VR to the subspace z, and |\u03bb| :=\n\u221a\n|(\u03bb, \u03bb)|.\n\nFor m+, m\u2212 \u2208 {0, 1} we define the Siegel theta function\n\n\u0398\n(m+,m\u2212)\nL,\u2113 (\u03c4, z) := v\n\nn\n2\n+m\u2212\n\n\u2211\n\n\u03bb\u2208L\u2032\npm\n\n+\n\nz (\u03bb)p\nm\u2212\nz\u22a5 (\u03bb)e (Q(\u03bbz)\u03c4 +Q(\u03bbz\u22a5)\u03c4 ) e\u03bb+L(3.1)\n\non H \u00d7 Gr(L). Note that for m+, m\u2212 \u2208 {0, 1} and every fixed z \u2208 Gr(L) the polynomial\npm\n\n+\n\nz (\u03bb)p\nm\u2212\nz\u22a5 (\u03bb) is harmonic and homogeneous of degree (m\n\n+, m\u2212). Hence by Theorem 4.1\nin [1] the theta function transforms like a modular form of weight 1\u2212n\n\n2\n+ m+ \u2212 m\u2212 for \u03c1L\n\nin \u03c4 . Moreover, it is \u0393L-invariant in z. The corresponding regularized theta lift of a weakly\nholomorphic modular form f \u2208 M !1\u2212n\n\n2\n+m+\u2212m\u2212,L is defined by\n\n(3.2) \u03a6\n(m+,m\u2212)\nL,\u2113 (f, z) := lim\n\nT\u2192\u221e\n\n\u222b\n\nFT\n\n\u2329\nf(\u03c4),\u0398\n\n(m+,m\u2212)\nL,\u2113 (\u03c4, z)\n\n\u232a\nv\n\n1\u2212n\n2\n\n+m+\u2212m\u2212 dudv\n\nv2\n.\n\n\n\n10 YINGKUN LI AND MARKUS SCHWAGENSCHEIDT\n\nNote that when m\u2212 = 0 (resp. m\u2212 = 1 = n), we will fix generators of z (resp. z and z\u22a5) to\nremove the dependence on \u2113, and omit it from the subscripts.\n\nBy the general theory developed in [1,6], the theta lift converges for every z \u2208 Gr(L) and\nis real analytic up to singularities along the Heegner divisor\n\nZf :=\n\u2211\n\n\u03bb\u2208L\u2032\nQ(\u03bb)<0\n\naf (\u03bb,Q(\u03bb))\u03bb\n\u22a5,\n\nwhere \u03bb\u22a5 denotes the hypersurface consisting of all z \u2208 Gr(L) perpendicular to \u03bb. These\nhypersurfaces partition Gr(L) into infinitely many connected components, the so-called Weyl\nchambers corresponding to f .\n\n3.1. Theta lifts on isotropic lattices of signature (1, n). Let L be an isotropic even\nlattice of signature (1, n) with n \u2265 1. Let \u2113 \u2208 L be primitive isotropic and let M \u2208 N with\n(\u2113, L) = MZ. Choose some \u2113\u2032 \u2208 L\u2032 with (\u2113, \u2113\u2032) = 1, some \u03b6 \u2208 L with (\u2113, \u03b6) = M , and set\n\nK := L \u2229 \u2113\u22a5 \u2229 \u2113\u2032\u22a5.\n\nThen K has signature (0, n\u2212 1) and L = K \u2295 Z\u03b6 \u2295 Z\u2113 (see Proposition 2.2 in [6]). Let\n\nL\u20320 := {\u03bb \u2208 L\u2032 : (\u03bb, \u2113) \u2261 0 (mod M)},\n\nand let p : L\u20320 \u2192 K \u2032 be defined by\n\n(3.3) p(\u03bb) := \u03bbK \u2212\n(\u03bb, \u2113)\n\nM\n\u03b6K , \u03bbK := \u03bb+ ((\u03bb, \u2113)(\u2113\n\n\u2032, \u2113\u2032)\u2212 (\u03bb, \u2113\u2032))\u2113\u2212 (\u03bb, \u2113)\u2113\u2032 \u2208 K \u2297Q,\n\nwhich induces a surjection p : L\u20320/L \u2192 K \u2032/K.\nWe let Bk(x) denote the one-periodic function that agrees with the Bernoulli polynomial\n\nBk(x) for 0 \u2264 x < 1. Recall that the first few Bernoulli polynomials are given by\n\nB0(x) = 1, B1(x) = x\u2212\n1\n\n2\n, B2(x) = x\n\n2 \u2212 x+\n1\n\n6\n, B3(x) = x\n\n3 \u2212\n3\n\n2\nx2 +\n\n1\n\n2\nx.\n\nThe theta lift has the following Fourier expansion.\n\n\n\nMOCK MODULAR FORMS 11\n\nProposition 3.1. Let m+, m\u2212 \u2208 {0, 1} and let L be an even lattice of signature (1, n) with\na primitive isotropic vector \u2113 \u2208 L. For f \u2208 M !1\u2212n\n\n2\n+m+\u2212m\u2212,L and every z \u2208 Gr(L) we have\n\n\u03a6\n(m+,m\u2212)\nL,\u2113 (f, z) =\n\n|\u2113z|m\n+\u22121(\u2212|\u2113z\u22a5|)\u2212m\n\n\u2212\n\n\u221a\n2(4\u03c0)m\n\n+\n\u03a6\n\n(m+,m\u2212)\nK (f)\u2212\n\n\u221a\n2(\u22124\u03c0)1\u2212m+\n\n2\u2212m+ +m\u2212\n|\u2113z|1\u2212m\n\n++2m\u2212(\u2212|\u2113z\u22a5|)\u2212m\n\u2212\n\n\u00d7\n\u2211\n\n\u03bb\u2208K \u2032\n\n\u2211\n\n\u03b4\u2208L\u20320/L\np(\u03b4)=\u03bb+K\n\naf(\u03b4, Q(\u03bb))B2\u2212m++m\u2212\n\n(\npz(\u03bb)\n\n|\u2113z|\n+ (\u03b4, \u2113\u2032)\n\n)\n\n+\n(\u22124\u03c0)1\u2212m+\u221a\n\n2\n\n\u2211\n\n\u03bb\u2208L\u2032\nQ(\u03bb)<0\n(\u03bb,\u2113)6=0\n\naf(\u03bb,Q(\u03bb))p\n1\u2212m+\nz (\u03bb)p\n\nm\u2212\nz\u22a5 (\u03bb)\n\n(\nsgn(pz(\u03bb))\u2212 sgn((\u03bb, \u2113))\n\n)\n,\n\nwhere the constant \u03a6\n(m+,m\u2212)\nK (f) is given by\n\n\u03a6\n(m+,m\u2212)\nK (f) =\n\n\uf8f1\n\uf8f4\uf8f4\uf8f2\n\n\uf8f4\uf8f4\uf8f3\n\n\u22128\u03c0\n\u2211\n\nm\u22650\n\n\u2211\n\n\u03bb\u2208K \u2032\n\n\u2211\n\n\u03b4\u2208L\u20320/L\np(\u03b4)=\u03bb+K\n\naf (\u03b4,\u2212m+Q(\u03bb))\u03c31(m), if m+ = m\u2212,\n\n0, otherwise,\n\nwith \u03c31(m) :=\n\u2211\n\nd|m d for m \u2208 N and \u03c31(0) := \u2212\n1\n24\n.\n\nRemark 3.2. By the same arguments as in the proof of Theorem 3.3 in [12] one can show that\nthe sum in the third line in Proposition 3.1 is finite for every fixed z \u2208 Gr(L), and vanishes\nfor |\u2113z| small enough. Moreover, the second line encompasses the singularities of the theta\nlift along those \u03bb\u22a5 with (\u03bb, \u2113) 6= 0, whereas the first line gives those with (\u03bb, \u2113) = 0.\n\nRemark 3.3. For m+ = m\u2212 and n \u2265 2, the constant \u03a6(m\n+,m\u2212)\n\nK (f) can also be computed\nusing Theorem 2.14 in [6] by writing f as a linear combination of the Maass Poincare\u0301 series\ndefined in (2.5). This yields the alternative representation\n\n\u03a6\n(m+,m\u2212)\nK (f) =\n\n8\u03c0\n\n(n\u2212 1)\n\u2211\n\n\u03bb\u2208K \u2032\n\n\u2211\n\n\u03b4\u2208L\u20320/L\np(\u03b4)=\u03bb+K\n\naf (\u03b4, Q(\u03bb))|Q(\u03bb)|.(3.4)\n\nRemark 3.4. There is F\u2113,z \u2208 V 3+n\n2\n\n\u2212m++m\u2212,L\u2212 that allows us to rewrite Proposition 3.1 as\n\n\u03a6\n(m+,m\u2212)\nL,\u2113 = {f, F\u2113,z}\n\nfor all f \u2208 M !1\u2212n\n2\n\n+m+\u2212m\u2212,L (see e.g. (4.1) for n = 4, m\n+ = 1, m\u2212 = 0).\n\n\n\n12 YINGKUN LI AND MARKUS SCHWAGENSCHEIDT\n\nProof of Proposition 3.1. The formula follows from Theorem 10.24 in [1] (or a generalization\n\nof Proposition 3.1 in [6]), where the Fourier expansion of \u03a6\n(m+,m\u2212)\nL,\u2113 (f, z) was computed in\n\na fixed Weyl chamber of Gr(L). In order to extend the expansion to all of Gr(L), one can\nuse the shape of the singularities of the theta lift to determine its \u201cwall crossing\u201d behavior\nas a point z \u2208 Gr(L) moves across a hypersurface in the Heegner divisor Zf from one Weyl\nchamber to another (compare also Corollary 6.3 in [1]).\n\nFirst, by Theorem 10.2 in [1], for |\u2113z| small enough the Fourier expansion of the theta lift\nis given by the first three lines of the expression on the right-hand side in the proposition.\n\nThe constant \u03a6\n(m+,m\u2212)\nK (f) appearing in Theorem 10.2 in [1] vanishes if m\n\n+ 6= m\u2212, and for\nm+ = m\u2212 it is equal to the regularized integral\n\n\u03a6\n(m+,m\u2212)\nK (f) = lim\n\nT\u2192\u221e\n\n\u222b\n\nFT\n\n\u2329\nfK(\u03c4),\u0398K\u2212(\u03c4)\n\n\u232a dudv\nv2\n\n, fK(\u03c4) :=\n\u2211\n\n\u03b3\u2208K \u2032/K\n\n\u2211\n\n\u03b4\u2208L\u20320/L\np(\u03b4)=\u03b3\n\nf\u03b4(\u03c4)e\u03b3,(3.5)\n\nwhere \u0398K\u2212(\u03c4) =\n\u2211\n\n\u03bb\u2208K \u2032 e(\u2212Q(\u03bb)\u03c4)e\u03bb+K is the holomorphic theta function associated to\nthe positive definite lattice K\u2212. In particular, the integral in (3.5) can be viewed as the\nregularized average value of a weakly holomorphic modular form of weight 0 for SL2(Z).\nHence (3.5) can be evaluated as explained in Remark 4.9 in [9].\n\nMoreover, by Theorem 6.2 in [1] (or a generalization of Theorem 2.12 in [6]), the theta lift\nhas a singularity of type\n\n(\u22124\u03c0)1\u2212m+\u221a\n2\n\n\u2211\n\n\u03bb\u2208L\u2032\nQ(\u03bb)<0\n\u03bb\u22a5z0\n\naf(\u03bb,Q(\u03bb))p\n1\u2212m+\nz (\u03bb)p\n\nm\u2212\nz\u22a5 (\u03bb)sgn(pz(\u03bb))\n\nat a point z0 \u2208 Gr(L). Here we say that a function f has a singularity of type g at a point\nz0 if there exists a neighborhood U of z0 such that f and g are defined on U and f \u2212g is real\nanalytic on U . This definition slightly differs from the one used in [1] and [6], but it allows\nus to extend the Fourier expansion to points z \u2208 Gr(L) where the theta lift has singularities.\n\nIt is now easy to check that the expression on the right-hand side of the proposition has\n\nthe same singularities as \u03a6\n(m+,m\u2212)\nL,\u2113 (f, z). In particular, the difference of \u03a6\n\n(m+,m\u2212)\nL,\u2113 (f, z) and\n\nthe expression in the proposition defines a real-analytic function on all of Gr(L) and vanishes\nfor |\u2113z| small enough (see also Remark 3.2), and hence vanishes everywhere on Gr(L). This\nfinishes the proof. \ufffd\n\n3.2. Theta lifts on anisotropic lattices of signature (1, 1). We now compute the theta\n\nlift \u03a6\n(m+,m\u2212)\nL,\u2113 (f, z) defined in (3.2) for anisotropic lattices L of signature (1, 1). One can\n\ncompute the theta lift for anisotropic lattices of signature (1, n) for any n \u2265 1 in a similar\n4Note that there is a sign (\u22121)h\u2212 missing in the cited formula.\n\n\n\nMOCK MODULAR FORMS 13\n\nway, but the resulting formulas do not look as pleasing. Hence we confine ourselves with\nsignature (1, 1), which suffices for our applications. Another advantage is that we can fix\ngenerators of z, z\u22a5 and remove the isotropic vector \u2113 from the notations.\n\nProposition 3.5. Let m+, m\u2212 \u2208 {0, 1} but (m+, m\u2212) 6= (1, 0) and let L be an anisotropic\nlattice of signature (1, 1). For any f \u2208 M !\n\nm+\u2212m\u2212,L and z \u2208 Gr(L) we have\n\n\u03a6\n(m+,m\u2212)\nL (f, z)\n\n=\n2\n\n3\n2\n\u2212m+\u2212m\u2212\n\n\u03c0m\n+\u22121\n\n\u2211\n\n\u03bb\u2208L\u2032\nQ(\u03bb)<0\n\naf (\u03bb,Q(\u03bb))sgn(pz(\u03bb))\nm+sgn(pz\u22a5(\u03bb))\n\nm\u2212(|pz\u22a5(\u03bb)| \u2212 |pz(\u03bb)|)1\u2212m\n++m\u2212.\n\nRemark 3.6. For (m+, m\u2212) = (1, 0) the proof below does not work since there might be\nnon-trivial holomorphic modular forms of weight 1 for \u03c1L, so we cannot write f as a linear\ncombination of Maass Poincare\u0301 series (and, possibly, invariant vectors). Hence we exclude\nthis case from the above proposition.\n\nProof of Proposition 3.5. First note that we cannot use Theorem 10.2 in [1] since it requires\nthe existence of an isotropic vector in L. Instead, we will write\n\nf(\u03c4) =\n1\n\n2\n\n\u2211\n\nh\u2208AL\n\n\u2211\n\nm<0\n\naf(h,m)Fh,m(\u03c4) + c\n\nas a linear combination of the Maass Poincare\u0301 series Fh,m defined in (2.5) and (if m\n+ = m\u2212)\n\na \u03c1L-invariant vector c \u2208 C[AL]. Then we compute the lift of Fh,m and c using the unfolding\nargument as in the proof of Theorem 2.14 in [6].\n\nTo simplify the notation, we only treat the case m+ = m\u2212 = 1 here. The other cases\n\nare analogous. We first show that the theta lift \u03a6\n(1,1)\nL (c, z) of an invariant vector c \u2208 C[AL]\n\nvanishes identically. To this end, we use the simple fact that every invariant vector can be\nwritten as a linear combination of residues at s = 1 of Eisenstein series\n\nEh(\u03c4, s) :=\n\u2211\n\n(A,\u03c6)\u2208\u0393\u0303\u221e\\Mp2(Z)\n\nvseh|0,L(A, \u03c6)\n\ncorresponding to isotropic elements h \u2208 AL. By the usual unfolding argument one can show\nthat \u03a6\n\n(1,1)\nL (Eh(\u00b7, s), z) is a multiple of\n\n\u2211\n\n\u03bb\u2208(L+h)\\{0}\nQ(\u03bb)=0\n\npz(\u03bb)pz\u22a5(\u03bb)\n\n(4\u03c0|Q(\u03bbz\u22a5)|)s+\n1\n2\n\nfor \u211c(s) large enough. Since L is anisotropic, the sum over \u03bb \u2208 (L+ h) \\ {0} with Q(\u03bb) = 0\nis empty, so the theta lift of Eh(\u03c4, s) vanishes identically for \u211c(s) big enough. In particular,\n\n\n\n14 YINGKUN LI AND MARKUS SCHWAGENSCHEIDT\n\nits residue at s = 1 vanishes, as well. This shows that the theta lift of an invariant vector\nvanishes.\n\nHence it suffices to compute the lifts of Poincare\u0301 series Fh,m. Using the unfolding argument\nagain, we find for \u211c(s) big enough\n\n\u03a6\n(1,1)\nL (Fh,m(\u00b7, s), z)\n\n=\n2\n\n\u0393(2s)\n\n\u2211\n\n\u03bb\u2208L+h\nQ(\u03bb)=m\n\npz(\u03bb)pz\u22a5(\u03bb)\n\n\u222b \u221e\n\n0\n\nv\u2212\n1\n2M0,s\u2212 1\n\n2\n(4\u03c0|m|v) exp (\u22122\u03c0v(Q(\u03bbz)\u2212Q(\u03bbz\u22a5))) dv.\n\nThe integral is an inverse Laplace transform (see equation (11) on p. 215 of [16]) given by\n\n(4\u03c0|m|)s\u0393\n(\ns+ 1\n\n2\n\n)\n\n(4\u03c0|Q(\u03bbz\u22a5)|)s+\n1\n2\n\n2F1\n\n(\ns+\n\n1\n\n2\n, s, 2s;\n\n|m|\n|Q(\u03bbz\u22a5)|\n\n)\n.\n\nPlugging in s = 1 and using that 2F1\n(\n3\n2\n, 1, 2; x\n\n)\n= 21\u2212\n\n\u221a\n1\u2212x\n\nx\n\u221a\n1\u2212x we find\n\n\u03a6\n(1,1)\nL (Fh,m, z) =\n\n\u2211\n\n\u03bb\u2208L+h\nQ(\u03bb)=m\n\npz(\u03bb)pz\u22a5(\u03bb)\n\n\u221a\n|Q(\u03bbz\u22a5)| \u2212\n\n\u221a\nQ(\u03bbz)\u221a\n\n|Q(\u03bbz\u22a5)|\n\u221a\nQ(\u03bbz)\n\n.\n\nUsing Q(\u03bbz) =\n1\n2\npz(\u03bb)\n\n2 and Q(\u03bbz\u22a5) = \u221212pz\u22a5(\u03bb)\n2 we obtain the stated formula. \ufffd\n\nNote that, in contrast to the isotropic case, the sum on the right-hand side of Propo-\nsition 3.5 is not finite since the discriminant kernel \u0393L is infinite (as it corresponds to a\nnon-trivial subgroup of the units in a real quadratic field). However, one can obtain a finite\nevaluation of the theta lift by splitting the sum over \u03bb \u2208 L\u2032 modulo \u0393L. To this end, it is\nconvenient to view anisotropic lattices of signature (1, 1) as lattices in real quadratic fields,\nas we now explain.\n\nLet D > 1 be a non-square discriminant (not necessarily fundamental), let F = Q(\n\u221a\nD)\n\nbe the corresponding real quadratic field, and let OF be its ring of integers. We consider the\nsubring of OF given by\n\nOD := Z+\nD +\n\n\u221a\nD\n\n2\nZ.\n\nNotice that OD = OF if D is a fundamental discriminant. More generally, if D = f 2DF\nwith a fundamental discriminant DF and f \u2208 N, then OD is the order of discriminant D and\nconductor f in F . For an integral ideal a \u2286 OD with A := [OD : a] and a positive integer\nM \u2208 N we consider the lattice\n\n(3.6) (La,M , Qa,M) :=\n\n(\nMa,\u2212\n\nNmF/Q\nAM\n\n)\n,\n\n\n\nMOCK MODULAR FORMS 15\n\nwhere Nm denotes the norm. The associated bilinear form is Ba,M(\u03bb, \u00b5) = \u2212\nTrF/Q(\u03bb\u00b7\u00b5\u2032)\n\nAM\n, where\n\n\u00b5\u2032 denotes the conjugate of \u00b5. The lattice La,M is anisotropic of signature (1, 1). The dual\n\nlattice is given by ad\u22121D , where dD :=\n\u221a\nDOD, so the discriminant group of La,M is isomorphic\n\nto OD/MdD. The discriminant kernel \u0393La,M is generated by \u22121 and a totally positive unit\n\u03b5La,M > 1 in OD, which is an integral power of the smallest totally positive unit > 1 in OD.\n\nFor simplicity, in the following corollary we already evaluate the theta lift at a certain\nspecial point which is adjusted to our later applications. We view (La,M , Qa,M) as a sub-\nlattice of (R2, xy) by sending \u03bb \u2208 La,M to 1\u221aAM [\u03bb,\u2212\u03bb\n\n\u2032]. We will choose the generators\n\n(z1, z2), (z\n\u22a5\n1 , z\n\n\u22a5\n2 ) \u2208 R2 of z and z\u22a5 with z1, z2 > 0 and z\u22a51 > 0, z\u22a52 < 0.\n\nCorollary 3.7. Suppose that (L,Q) = (La,M , Qa,M) is an anisotropic lattice of the form\n(3.6). Let f \u2208 M !\n\nm+\u2212m\u2212,L and let w \u2208 Gr(L) be the positive line generated by\n1\u221a\n2\n[1, 1]. Then\n\nwe have\n\n\u03a6\n(1,1)\nL (f, w) = \u2212\n\n1\u221a\nAM\n\n\u2211\n\n\u03bb\u2208RL\n\naf(\u03bb,Q(\u03bb))sgn(\u03bb)TrF/Q\n\n(\n\u03bb\n\n1\u2212 \u03b5\u22121L\n\n)\n,\n\n\u03a6\n(0,0)\nL (f, w) =\n\n4\u03c0\u221a\nDAM\n\n\u2211\n\n\u03bb\u2208R\u2217\nL\n\naf (\u03bb,Q(\u03bb))sgn(\u03bb)TrF/Q\n\n( \u221a\nD\u03bb\n\n1\u2212 \u03b5\u22121L\n\n)\n,\n\n\u03a6\n(0,1)\nL (f, w) =\n\n2\n\u221a\n2\u03c0\u221a\n\nDAM\n\n\u2211\n\n\u03bb\u2208R\u2217\nL\n\naf (\u03bb,Q(\u03bb))sgn(\u03bb)TrF/Q\n\n( \u221a\nD\u03bb2\n\n1\u2212 \u03b5\u22122L\n\n)\n,\n\nwhere RL := {\u03bb \u2208 L\u2032 : \u03b5\u22122L < \u03bb/\u03bb\u2032 < 1} and R\u2217L := {\u03bb \u2208 L\u2032 : \u03b5\n\u22122\nL < \u03bb/\u03bb\n\n\u2032 \u2264 1}. The sums on\nthe right-hand sides are finite.\n\nProof. Again, we only treat the case (m+, m\u2212) = (1, 1) for simplicity. First note that we\nhave\n\npw(\u03bb) =\n1\u221a\n2AM\n\n(\u03bb\u2212 \u03bb\u2032), pw\u22a5(\u03bb) =\n1\u221a\n2AM\n\n(\u03bb+ \u03bb\u2032).\n\nUsing Proposition 3.5 we find\n\n\u03a6\n(1,1)\nL (f, w) =\n\n1\n\n2\n\u221a\nAM\n\n\u2211\n\n\u03bb\u2208L\u2032\nQ(\u03bb)<0\n\naf(\u03bb,Q(\u03bb))sgn (|\u03bb\u2032| \u2212 |\u03bb|)\n(\n|\u03bb+ \u03bb\u2032| \u2212 |\u03bb\u2212 \u03bb\u2032|\n\n)\n\n= \u2212 1\u221a\nAM\n\n\u2211\n\n\u03bb\u2208L\u2032\nQ(\u03bb)<0\n\naf(\u03bb,Q(\u03bb))sgn (|\u03bb\u2032| \u2212 |\u03bb|)min(|\u03bb|, |\u03bb\u2032|),\n\nwhere we used that |x + y| \u2212 |x \u2212 y| = \u22122min(|x|, |y|) for x, y \u2208 R with xy < 0, and\nsgn(\u03bb) = sgn(\u03bb\u2032) since Q(\u03bb) = \u2212 \u03bb\u03bb\u2032\n\nAM\n< 0. Let \u0393\u2032L be the subgroup of \u0393L consisting of\n\ntotally positive units. Note that the terms with \u03bb = \u03bb\u2032 contribute nothing. As a system of\n\n\n\n16 YINGKUN LI AND MARKUS SCHWAGENSCHEIDT\n\nrepresentatives for \u0393\u2032L\\{\u03bb \u2208 L\u2032 : Q(\u03bb) < 0, \u03bb 6= \u03bb\u2032} we choose the set RL. Then the set of\nall \u03bb \u2208 L\u2032 with Q(\u03bb) < 0 and \u03bb 6= \u03bb\u2032 is given by {\u03bb\u03b5nL : \u03bb \u2208 RL}. For \u03bb \u2208 RL and n \u2208 Z we\nhave\n\nmin(|\u03bb\u03b5nL|, |\u03bb\u2032\u03b5\u2212nL |) =\n{\n|\u03bb|\u03b5nL, if n \u2264 0,\n|\u03bb\u2032|\u03b5\u2212nL , if n > 0,\n\nsgn(|\u03bb\u2032\u03b5\u2212nL | \u2212 |\u03bb\u03b5\nn\nL|) =\n\n{\n1, if n \u2264 0,\n\u22121, if n \u2265 1.\n\nWe obtain\n\n\u2211\n\n\u03bb\u2208L\u2032\nQ(\u03bb)<0\n\naf(\u03bb,Q(\u03bb))sgn (|\u03bb\u2032| \u2212 |\u03bb|)min(|\u03bb|, |\u03bb\u2032|) =\n\u2211\n\n\u03bb\u2208RL\n\naf (\u03bb,Q(\u03bb))\n\n(\n|\u03bb|\n\u2211\n\nn\u22640\n\u03b5nL \u2212 |\u03bb\u2032|\n\n\u2211\n\nn\u22651\n\u03b5\u2212nL\n\n)\n\n=\n\u2211\n\n\u03bb\u2208RL\n\naf (\u03bb,Q(\u03bb))\n\n(\n|\u03bb|\n\n1\u2212 \u03b5\u2032L\n+\n\n|\u03bb\u2032|\n1\u2212 \u03b5L\n\n)\n.\n\nThis yields the stated formula. The sums on the right-hand side of the corollary are finite\nsince f has finite principal part and the intersection of RL (resp. R\n\n\u2217\nL) with the set of vectors\n\nof a fixed norm is finite. \ufffd\n\n4. Proof of Theorem 1.1\n\nWe consider the even lattice L = Z[N ] \u2295 (Z[3]\u2212)4 of signature (1, 4) and level 12N . By\nthe four-square theorem, we can find a primitive isotropic vector \u2113 = [3, \u2217] \u2208 L. Then we\nhave (\u2113, L) = MZ for some M \u2208 N satisfying M | 6N . Let \u03b6 \u2208 L and \u2113\u2032 = [\u2113\u20321, \u2217] \u2208 L\u2032 such\nthat (\u03b6, \u2113) = M and (\u2113, \u2113\u2032) = 1. Denote K = L \u2229 \u2113\u22a5 \u2229 \u2113\u2032\u22a5.\n\nIn order to show that the mock modular forms we are going to construct below have\nbounded denominators, we will need to bound the denominators of vectors in K \u2032. Namely,\nif \u00b5 = [\u00b51, \u2217] \u2208 K \u2032 \u2282 Q5, we claim that \u00b51 \u2208 12NZ. Indeed, for any \u03bb = [\u03bb1, \u2217] \u2208 L\n\n\u2032, the first\ncoordinate of \u03bbK is\n\n\u03bb1 + ((\u03bb, \u2113)(\u2113\n\u2032, \u2113\u2032)\u2212 (\u03bb, \u2113\u2032))3\u2212 (\u03bb, \u2113)\u2113\u20321 \u2208\n\n1\n\n2N\nZ\n\nas \u03bb1, \u2113\n\u2032\n1 \u2208 12NZ and L has level 12N . Using the surjection p : L\n\n\u2032\n0/L \u2192 K \u2032/K in (3.3), we\n\ncan find \u03bb = [\u03bb1, \u2217] \u2208 L\u20320 \u2282 L\u2032 such that \u00b5 = p(\u03bb) = \u03bbK \u2212\n(\u03bb,\u2113)\nM\n\n\u03b6K . Since\n(\u03bb,\u2113)\nM\n\n\u2208 Z, we have\n\u00b51 \u2208 12NZ, as claimed.\n\nNow we come to the construction of a mock modular form \u03b8\u0303+N (\u03c4 ; 1) whose shadow is given\nby 1\u221a\n\nN\n\u03b8N (\u03c4 ; 1). To this end, we will first compute the regularized Petersson inner product\n\n(g(\u03c4), \u03b8N(\u03c4 ; 1))\nreg for every g \u2208 M !3\n\n2\n,Z[N ]\n\n, using the theta lift studied in Section 3. Then\n\nwe can express this inner product in terms of the pairing {g, \u03b8\u0303+N(\u03c4 ; 1)} for some explicit\nLaurent series \u03b8\u0303+N (\u03c4 ; 1) \u2208 V 1\n\n2\n,Z[N ]\u2212, and finally apply Proposition 2.5 to obtain the desired\n\nmock modularity.\n\n\n\nMOCK MODULAR FORMS 17\n\nIt is easy to check that the theta function \u0398\n(1,0)\nL,\u2113 (\u03c4, z) defined in (3.1) splits at the line\n\ngenerated by\n\nw =\n1\u221a\n2N\n\n[1, 0, 0, 0, 0]\n\nas a tensor product\n\n\u0398\n(1,0)\nL,\u2113 (\u03c4, w) =\n\n1\u221a\n2N\n\n(\n\u03b8N (\u03c4 ; 1)\u2297 v2\u03b83(\u03c4 ; 0)\n\n4\n)\n.\n\nThen by (2.10) for each g \u2208 M !3\n2\n,Z[N ]\n\n, we have\n\n(g(\u03c4), \u03b8N(\u03c4 ; 1))\nreg\n\n=\n1\n\n16\n\n(\ng(\u03c4)\u2297 (\u03b7\u22124(\u03c4)v4), \u03b8N (\u03c4 ; 1)\u2297 v2\u03b83(\u03c4 ; 0)\n\n4\n)reg\n\n=\n\n\u221a\n2N\n\n16\n\u03a6\n\n(1,0)\nL,\u2113 (f, w),\n\nwhere f := g\u2297 (\u03b7\u22124v4) \u2208 M\u2212 1\n2\n,L with v4 defined in (2.10). From Proposition 3.1, we see that\n\n(g(\u03c4), \u03b8N(\u03c4, 1))\nreg\n\n=\n\n\u221a\n2N\n\n16\n\u03a6\n\n(1,0)\nL,\u2113 (f, w) =\n\n\u221a\nN\n\n16\n{f, F\u2113,w}\n\nwith the power series F\u2113,w(\u03c4) =\n\u2211\n\nm,h c(h,m)q\nm\neh \u2208 V 5\n\n2\n,L\u2212 defined by\n\n(4.1) c(h,m) = \u22122\n\u2211\n\n\u03bb\u2208K+p(h)\n\u2212Q(\u03bb)=m\u22650\n\nB1\n\n(\npw(\u03bb)\n\n|\u2113w|\n+ (h, \u2113\u2032)\n\n)\n+\n\n\u2211\n\n\u03bb\u2208L+h\n\u2212Q(\u03bb)=m>0\n\n(\u03bb,\u2113)6=0\n\n(sgn(pw(\u03bb))\u2212 sgn((\u03bb, \u2113))),\n\nwhere we understand that the first sum vanishes if h /\u2208 L\u20320/L. Note that\npw(\u03bb)\n|\u2113w| =\n\n\u03bb1\n3\n\u2208 1\n\n6N\nZ\n\nfor \u03bb = [\u03bb1, \u2217] \u2208 K \u2032 by the above discussion, and (h, \u2113\u2032) \u2208 16NZ since L has level 12N . Recall\nthat B1(x) is the one-periodic function that agrees with B1(x) = x\u2212 12 for 0 \u2264 x < 1. This\nimplies that F\u2113,w has coefficients in\n\n1\n3N\n\nZ.\nIf we write F\u2113,w =\n\n\u2211\n\u03b1\u2208AN , \u00b5\u2208A43\n\nF\u2113,w,\u03b1,\u00b5, then the Laurent series\n\n(4.2) \u03b8\u0303+N(\u03c4 ; 1) :=\n1\n\n16\n\u03b7\u22124(\u03c4)\n\n\u2211\n\n\u03b1\u2208AN\n\ne\u03b1\n\n\u2211\n\n\u00b5\u2208A43\n\n\u3008e\u00b5, v4\u3009F\u2113,w,\u03b1,\u00b5(\u03c4) \u2208 V 1\n2\n,Z[N ]\u2212\n\nhas coefficients in 1\n48N\n\nZ and satisfies\n\n{\ng(\u03c4), \u03b8\u0303+N(\u03c4 ; 1)\n\n}\n=\n\n(\ng(\u03c4),\n\n1\u221a\nN\n\u03b8N (\u03c4 ; 1)\n\n)reg\n\nfor any g \u2208 M !3\n2\n,Z[N ]\n\n. Hence, Proposition 2.5 implies that \u03b8\u0303+N (\u03c4 ; 1) is a mock modular form\n\nof weight 1\n2\nwith shadow 1\u221a\n\nN\n\u03b8N (\u03c4 ; 1). Moreover, 48N\u03b8\u0303\n\n+\nN (\u03c4 ; 1) has integral coefficients. This\n\nfinishes the proof of the first item in Theorem 1.1.\n\n\n\n18 YINGKUN LI AND MARKUS SCHWAGENSCHEIDT\n\nThe proof of the second item of Theorem 1.1 is analogous, now using the theta lift\n\n\u03a6\n(0,0)\nL (f, z) with a constant polynomial. In particular, using Proposition 3.1 and the ar-\n\nguments above we can construct a mock modular form whose coefficients have denominators\nwhich are bounded by 288N . On the other hand, if we use the alternative expression (3.4)\n\nfor the constant \u03a6\n(0,0)\nK (f), we can construct a mock modular form with rational coefficients\n\nwhose denominators are bounded by 216N . Combining these two mock modular forms using\nthe trick from Remark 1.3, we can bound the denominators by gcd(288N, 216N) = 72N .\n\nRemark 4.1. If 3N is the sum of two squares, then we can carry out the argument above\nwith L = Z[N ]\u2295 (Z[3]\u2212)2, and the factor 48 can be reduced to 12. If N \u2261 1 (mod 3), then\nthe factor 48 can be reduced to 16 by combining with Proposition 6.1 and Remarks 1.3, 6.2.\n\n5. Proof of Theorem 1.5 and Example 1.6\n\nFor any integral OD-ideal a co-prime to DM , denote R = La,M an anisotropic lattice of\nsignature (1, 1) as in defined in (3.6). Since the eigenform f\u03d5 is a linear combination of\ncomponents of the vector-valued cusp form\n\n(5.1) \u03d1R(\u03c4) :=\n\u2211\n\n\u03bb\u2208\u0393R\\R\u2032\nQ(\u03bb)>0\n\nsgn(\u03bb)qQ(\u03bb)e\u03bb \u2208 S1,R,\n\nit suffices to consider its \u03be-preimage. This is constructed in (5.2.1) in [13] as\n\n\u03d1\u0303R(\u03c4) := log \u03b5R \u00b7 \u0398\u0303R(\u03c4) + 2I \u2032(\u03c4, R\u2212),\nwhere I \u2032 is a deformed theta integral and \u0398\u0303R(\u03c4) is a real-analytic modular form of weight\none satisfying\n\n(5.2) \u03be1\u0398\u0303R(\u03c4) =\n\u221a\n2\u0398\n\n(1,0)\nR (\u03c4, z0),\n\nHere \u0398\n(1,0)\nR (\u03c4, z) denotes the theta function (3.1), which we evaluate at the point z0 =\n\n1\u221a\n2\n[1, 1].\n\nAs in the proof of Theorem 5.1 loc. cit., the number \u03baR defined in (4.2.7) is a bound on\n\nthe denominator of Fourier coefficients of the holomorphic part5 \u0398+R(\u03c4) of \u0398\u0303R(\u03c4). For this\npurpose, it is enough to produce a suitable \u0398+R(\u03c4) having rational Fourier coefficients with a\ngood denominator bound. To do this, we consider the lattice L = R\u2295 (Z[3]\u2212)4 and obtain\n\n\u2329\n\u0398\n\n(1,0)\nL (\u03c4, z0), v4\n\n\u232a\n= 16\u0398\n\n(1,0)\nR (\u03c4, z0)\u03b7(\u03c4)\n\n4.\n\nApplying Proposition 3.1 again gives us for any g \u2208 M !1,R\n\n8\n\u221a\n2\n(\ng,\u0398\n\n(1,0)\nR (\u03c4, z0)\n\n)reg\n=\n\n1\u221a\n2\n\u03a6\n\n(1,0)\nL,\u2113 (f, w) = {f, F\u2113,w},\n\n5The holomorphic function \u0398+\nR\n(\u03c4) is also called a mixed mock modular form in the sense of [14].\n\n\n\nMOCK MODULAR FORMS 19\n\nwhere f = g \u2297 (\u03b7\u22124v4), w = [1, 0, 0, 0, 0] and F\u2113,w =\n\u2211\n\nh\u2208AR,\u00b5\u2208A43\nF\u2113,w,h,\u00b5eh,\u00b5 \u2208 V3,L\u2212 defined\n\nin the same way as in (4.1) with \u2113 \u2208 L any primitive isotropic vector. As a is co-prime to\nDM , we can find \u03b1 \u2208 a such that Tr(\u03b1) = gcd(D, 2). By taking \u2113 = [3M\u03b1, \u2217] \u2208 L, we can\nconduct the same analysis as in the proof of Theorem 1.1 to see that 6DMF\u2113,w has integral\ncoefficients. A slight variant of Proposition 2.5 then implies that\n\n\u0398\u0303+R :=\n\u03b7\u22124\n\n8\n\n\u2211\n\nh\u2208AR\n\neh\n\n\u2211\n\n\u00b5\u2208A43\n\n\u3008e\u00b5, v4\u3009F\u2113,w,h,\u00b5\n\nis the holomorphic part of a weight 1 real-analytic modular form \u0398\u0303R satisfying (5.2). This\ngives the bound \u03baR = 48DM , which leads to the improvement of \u03ba = 96DM following the\nsame proofs of Theorems 5.1 and 6.5 in [13]. This finishes the proof of Theorem 1.5.\n\nFor Example 1.6, we take F = Q(\n\u221a\n12) and a = OF ,M = 2. Then R\u2032 = (2\n\n\u221a\n3)\u22121Z[\n\n\u221a\n3],\n\nAR = {a+b\n\u221a\n3\n\n2\n\u221a\n3\n\n: a \u2208 Z/12Z, b \u2208 Z/4Z} \u223c= A6 \u2295A2 and \u03b5R = (2+\n\u221a\n3)4. Furthermore, we have\n\n\u3008\u03d1R(\u03c4), v\u3009 = \u03b7(\u03c4)2, \u3008\u0398(1,0)R (\u03c4, z0), v\u3009 =\n1\n\n4\n\u03b7(\u03c4)3\u03b7(\u2212\u03c4 )\n\n\u221a\nv,\n\nv :=\n1\n\n8\nv6 \u2297 v4 \u2208 Q[A6]\u2297Q[A2] \u223c= Q[AR].\n\n(5.3)\n\nLet \u0398\u0303+R be as above and \u0398\u0303R its modular completion. Then \u3008\u0398\u0303\n+\nR, v\u3009 is in q\u22121/12QJqK and\n\nthe \u03be1-image of its modular completion \u3008\u0398\u0303R, v\u3009 is \u03b7(\u03c4)3\u03b7(\u2212\u03c4 )\n\u221a\nv/(2\n\n\u221a\n2). From (1.4) and\n\nRemark 2.3, we then know that \u03b7(\u03c4)\u03b8\u0303+2 \u2212 \u3008\u0398\u0303+R, v\u3009 is in the trivial space M1,\u03c72 with \u03c7 the\ncharacter defined in (2.9). Finally, Proposition 5.5 in [13] shows that the holomorphic part of\n\n\u30082I \u2032(\u03c4, R\u2212), v\u3009 is given by\n\u2211\n\na\u2282OF \u03d5\u0303(a)q\nNm(a)/12 \u2208 q11/12RJqK. Therefore, \u03d1\u0303+ defined in (1.8)\n\nis the holomorphic part of the harmonic Maass form \u30082I \u2032(\u03c4, R\u2212) + log \u03b5R\u0398\u0303R(\u03c4), v\u3009 \u2208 H1,\u03c72 ,\nwhose \u03be1-image is \u3008\u03d1R(\u03c4), v\u3009 = \u03b7(\u03c4)2 \u2208 S1,\u03c72 .\n\n6. Explicit constructions of mock modular forms\n\nIn this section we compute explicit mock modular forms of weight 1\n2\nand 3\n\n2\nusing the\n\nevaluations of theta lifts for lattices of signature (1, 1) given in Section 3.\n\n6.1. Mock modular forms of weight 1\n2\n. We construct mock modular forms \u03b8\u0303+N (\u03c4 ; 1) of\n\nweight 1\n2\nfor \u03c1N with shadow\n\n1\u221a\nN\n\u03b8N (\u03c4 ; 1) for every N \u2208 N.\n\n\n\n20 YINGKUN LI AND MARKUS SCHWAGENSCHEIDT\n\nProposition 6.1. (1) Suppose that 2N is a square. Then\n\n\u03b8\u0303+N (\u03c4 ; 1) =\n1\n\n\u03b7(\u03c4)3\n\n( \u2211\n\nx,y\u2208Z\ny>\n\n\u221a\n2|x|\u221a\nN\n\n(\n\u22124\ny\n\n)\nsgn(x)yq\n\ny2\n\n8\n\u2212 x\n\n2\n\n4N ex\n\n+\n\u2211\n\nb(2\n\u221a\n2N)\n\n(\n\u22124\nb\n\n)(\nE2(\u03c4)\n\n12\n\u221a\n2N\n\n\u2212\n\u221a\n2NB2\n\n(\nb\n\n2\n\u221a\n2N\n\n))\ne\nb\n\u221a\n\nN\n2\n\n)\n\nis a mock modular form of weight 1\n2\nfor \u03c1N with shadow\n\n1\u221a\nN\n\u03b8N (\u03c4 ; 1). Here B2(x) is\n\nthe one-periodic function that agrees with the Bernoulli polynomial B2(x) = x\n2\u2212x+ 1\n\n6\nfor 0 \u2264 x < 1, and E2(\u03c4) = 1 \u2212 24\n\n\u2211\nn\u22651 \u03c31(n)q\n\nn is the holomorphic (quasimodular)\nEisenstein series of weight 2.\n\n(2) Suppose that 2N is not a square. Let F = Q(\n\u221a\n2N) and let \u03b5N = a + b\n\n\u221a\n2N be the\n\nsmallest totally positive unit > 1 of F such that b is even and lcm(2N, 4) | (a \u2212 1).\nThen\n\n\u03b8\u0303+N (\u03c4 ; 1) =\n1\n\n\u03b7(\u03c4)3\n\n\u2211\n\nx,y\u2208Z\n\u03b5\u22122\nN\n\n<\n\u221a\nNy+\n\n\u221a\n2x\u221a\n\nNy\u2212\n\u221a\n2x\n\n<1\n\n(\n\u22124\ny\n\n)\nsgn(x)TrF/Q\n\n(\ny\n2\n+ x\u221a\n\n2N\n\n1\u2212 \u03b5\u22121N\n\n)\nq\n\ny2\n\n8\n\u2212 x\n\n2\n\n4N ex\n\nis a mock modular form of weight 1\n2\nfor \u03c1N with shadow\n\n1\u221a\nN\n\u03b8N (\u03c4 ; 1).\n\nProof. The proof is similar to the proof of Theorem 1.1, but we will give some details for the\nconvenience of the reader. Let us first assume that 2N is a square. Then the lattice\n\nL =\n\u221a\nN [1, 1]Z\u2295\n\n\u221a\n2[1,\u22121]Z\n\nin (R2, xy) has signature (1, 1) and is isotropic. Its dual lattice is given by\n\nL\u2032 =\n1\n\n2\n\u221a\nN\n[1, 1]Z\u2295\n\n1\n\n2\n\u221a\n2\n[1,\u22121]Z.\n\nWe choose the primitive isotropic vector \u2113 = [2\n\u221a\nN, 0] \u2208 L. Then we have (\u2113, L) = 2\n\n\u221a\n2N .\n\nThe theta function \u0398\n(1,1)\nL,\u2113 (\u03c4, z) considered in Section 3 splits at the special point w =\n\n1\u221a\n2\n[1, 1]\n\nas a tensor product\n\n\u0398\n(1,1)\nL,\u2113 (\u03c4, w) = \u2212\n\n1\n\n2\n\u221a\n2N\n\n\u03b8N(\u03c4 ; 1)\u2297 v\n3\n2\u03b82(\u03c4 ; 1).\n\nHence, using Lemma 2.1 we can write for any g \u2208 M !3\n2\n,Z[N ]\n\n(g(\u03c4), \u03b8N(\u03c4 ; 1))\nreg =\n\n1\n\n2\n\n(\ng(\u03c4)\u2297 \u03b7\u22123(\u03c4)v2, \u03b8N (\u03c4 ; 1)\u2297 v\n\n3\n2\u03b82(\u03c4 ; 1)\n\n)reg\n= \u2212\n\n\u221a\n2N\u03a6\n\n(1,1)\nL,\u2113 (f, w),\n\n\n\nMOCK MODULAR FORMS 21\n\nwhere f = g \u2297 (\u03b7\u22123v2) \u2208 M !0,L. On the other hand, Proposition 3.1 implies that the theta\nlift can be expressed in terms of the pairing {\u00b7, \u00b7} defined in (2.12) as\n\n\u03a6\n(1,1)\nL,\u2113 (f, w) = {f, F\u2113,w}\n\nwith the power series\n\nF\u2113,w(\u03c4) = \u2212\n\u2211\n\nb(2\n\u221a\n2N)\n\n(\u221a\n2E2(\u03c4)\n\n24|\u2113w\u22a5|\n\u2212\n\n|\u2113w|2\u221a\n2|\u2113w\u22a5|\n\nB2\n\n(\nb\n\n2\n\u221a\n2N\n\n))\neb\u2113/2\n\n\u221a\n2N\n\n+\n1\u221a\n2\n\n\u2211\n\n\u03bb\u2208L\u2032\nQ(\u03bb)<0\n\npw\u22a5(\u03bb)\n(\nsgn(pw(\u03bb))\u2212 sgn((\u03bb, \u2113))\n\n)\nq\u2212Q(\u03bb)e\u03bb+L.\n\nHere we used that the lattice K is trivial, and that a system of representatives for \u03b4 \u2208\nL\u20320/L is given by b\u2113/M , where b runs modulo M := (\u2113, L). In particular, it follows from\nProposition 2.5 that the Laurent series\n\n\u03b8\u0303+N (\u03c4 ; 1) := \u2212\n\u221a\n2\u03b7\u22123(\u03c4)\u3008F\u2113,w(\u03c4), v2\u3009 \u2208 V 1\n\n2\n,Z[N ]\u2212\n\nis a mock modular form of weight 3\n2\nwith shadow 1\u221a\n\nN\n\u03b8N (\u03c4 ; 1). Using |\u2113w| = |\u2113w\u22a5| =\u221a\n\n2N, pw(\u03bb) =\nx\u221a\n2N\n\nand pw\u22a5(\u03bb) = \u2212y2 for \u03bb = [\nx\n\n2\n\u221a\nN\n\n+ y\n2\n\u221a\n2\n, x\n2\n\u221a\nN\n\n\u2212 y\n2\n\u221a\n2\n] \u2208 L\u2032 it is easy to\n\nobtain the representation of \u03b8\u0303+N (\u03c4 ; 1) given in the proposition. We leave the details of the\nsimplification to the reader.\n\nThe proof for 2N not being a square is similar, so we will only give a sketch. In this case,\nwe use a certain lattice La,M of the shape (3.6). We let D = 2N if N is even and D = 8N if\nN is odd. We choose\n\na :=\n\n{\nOD, if N is even,\n2Z+\n\n\u221a\n2NZ, if N is odd,\n\nM :=\n\n{\n2, if N is even,\n\n1, if N is odd.\n\nNote that in the first case we have A = [OD : a] = 1 and in the second case we have A = 2.\nIn any case, we have\n\n(La,M , Qa,M) =\n\n(\n2Z+\n\n\u221a\n2NZ,\u2212\n\nNmF/Q\n2\n\n)\n.\n\nWe can now proceed similarly as in the case of 2N being a square, now using the evaluation\n\nof the theta lift \u03a6\n(1,1)\nL,\u2113 (f, w) at w =\n\n1\u221a\n2\n[1, 1] given in Corollary 3.7. It is easy to see that the\n\nunit \u03b5N := \u03b5La,M is characterized by the conditions given in the proposition. \ufffd\n\n\n\n22 YINGKUN LI AND MARKUS SCHWAGENSCHEIDT\n\nRemark 6.2. (1) For 2N being a square, in [13] the authors constructed the mock mod-\nular form\n\n1\n\n\u03b7(\u03c4)3\n\n( \u2211\n\nx,y\u2208Z\ny>\n\n\u221a\n2|x|\u221a\nN\n\n(\n\u22124\ny\n\n)\nsgn(x)\n\n(\ny \u2212\n\n\u221a\n2|x|\u221a\nN\n\n)\nq\n\ny2\n\n8\n\u2212 x\n\n2\n\n4N ex +\nE2(\u03c4)\n\n6\n\u221a\n2N\n\n\u2211\n\nb(2\n\u221a\n2N)\n\n(\n\u22124\nb\n\n)\ne\nb\n\u221a\n\nN\n2\n\n)\n\nof weight 1\n2\nwith shadow 1\u221a\n\nN\n\u03b8N(\u03c4 ; 1). The difference with the mock modular form\n\nconstructed in Proposition 6.1 is given by \u03b7\u22123 times an Eisenstein series of weight 2\nfor \u03c1N , which can for example be constructed as an integral of a Kudla-Millson theta\nfunction as in Theorem 6.4 in [17].\n\n(2) For fixed m \u2208 N the number of x, y \u2208 Z with 0 \u2264 Ny2 \u2212 2x2 \u2264 m and\n\n\u03b5\u22122N <\n\n\u221a\nNy +\n\n\u221a\n2x\u221a\n\nNy \u2212\n\u221a\n2x\n\n< 1\n\nis finite. Indeed, the above conditions imply that x, y satisfy the inequalities\n\u221a\nm\n\n2\n\u221a\nN\n(1 + \u03b5\u22121N ) < |y| <\n\n\u221a\nm\n\n2\n\u221a\nN\n(1 + \u03b5N),\n\n\u221a\nm\n\n2\n\u221a\n2\n(\u03b5\u22121N \u2212 \u03b5N) < sgn(y)x < 0.\n\nIn particular, the coefficient at qm in the inner sum in \u03b8\u0303+N (\u03c4 ; 1) in item (2) of Propo-\nsition 6.1 is given by a finite sum of rational numbers.\n\n(3) If 2N is a square, then the denominators of the coefficients of \u03b8\u0303+N(\u03c4 ; 1) are bounded\n\nby 6\n\u221a\n2N . If 2N is not a square, then we can write\n\nTrF/Q\n\n(\n\u03bb\n\n1\u2212 \u03b5\u22121N\n\n)\n=\n\nTrF/Q(\u03bb(1\u2212 \u03b5N))\nNmF/Q(1\u2212 \u03b5\u22121N )\n\n=\nTrF/Q(\u03bb(1\u2212 \u03b5N))\nTrF/Q(1\u2212 \u03b5N)\n\n,\n\nwhich implies that the denominators of the coefficients of \u03b8\u0303N(\u03c4 ; 1) are bounded by\nTrF/Q(1\u2212 \u03b5N ). In the numerical examples that we looked at the denominators of the\ncoefficients of \u03b8\u0303N (\u03c4 ; 1) were usually bigger than the bound 48N from Theorem 1.1.\n\nExample 6.3. Consider Ramanujan\u2019s classical mock theta functions of order 3,\n\nf(q) = 1 +\n\n\u221e\u2211\n\nn=1\n\nqn\n2\n\n(1 + q)2(1 + q2)2 \u00b7 \u00b7 \u00b7 (1 + qn)2\n= 1 + q \u2212 2q2 + 3q3 \u2212 3q4 + 3q5 + . . .\n\n\u03c9(q) =\n\n\u221e\u2211\n\nn=0\n\nq2n(n+1)\n\n(1\u2212 q)2(1\u2212 q3)2 \u00b7 \u00b7 \u00b7 (1\u2212 q2n+1)2\n= 1 + 2q + 3q2 + 4q3 + 6q4 + 8q5 + . . .\n\n\n\nMOCK MODULAR FORMS 23\n\nThe fundamental work of Zwegers [24] (see also Section 8.2 in [10]) shows that the C[Z/12Z]-\nvalued function\n\nF+(\u03c4) = q\u2212\n1\n24 f(q)(e1 \u2212 e5 + e7 \u2212 e11)\n\n+ 2q\n1\n3 (\u03c9(q\n\n1\n2 )\u2212 \u03c9(\u2212q\n\n1\n2 ))(\u2212e2 + e10)\n\n+ 2q\n1\n3 (\u03c9(q\n\n1\n2 ) + \u03c9(\u2212q\n\n1\n2 ))(\u2212e4 + e8)\n\nis the holomorphic part of a harmonic Maass form F (\u03c4) of weight 1\n2\nfor the dual Weil\n\nrepresentation associated with Z[6]. By comparing principal parts, we obtain that\n\nF+(\u03c4) = 2\u03b8\u03036(\u03c4 ; 1) + 2\u03b8\u03036(\u03c4 ; 1)\n\u03c33 ,\n\nwhere \u03b8\u03036(\u03c4 ; 1) is the mock modular form constructed in the second item of Proposition 6.1\nabove, and \u03c33 is the Atkin-Lehner involution on Z/12Z that interchanges 1 with 7, 3 with\n9, 5 with 11, and fixes all other elements. For example, this implies the identity\n\nq\u2212\n1\n24 f(q) = 2\u03b8\u03036,1(\u03c4 ; 1) + 2\u03b8\u03036,7(\u03c4 ; 1)\n\n=\n1\n\n\u03b7(\u03c4)3\n\n\u2211\n\nx,y\u2208Z\nx\u22611(6)\n\n\u03b5\n\u22122\n6 <\n\n\u221a\n3y+x\u221a\n3y\u2212x\n\n<1\n\n(\n\u22124\ny\n\n)\nsgn(x)TrF/Q\n\n(\ny + x\u221a\n\n3\n\n1\u2212 \u03b5\u221216\n\n)\nq\n\ny2\n\n8\n\u2212x\n\n2\n\n24 ,\n\nwhere F = Q(\n\u221a\n12) and \u03b56 = 97 + 28\n\n\u221a\n12.\n\nIf 6N is a square then we can use the theta lift \u03a6\n(1,0)\nL,\u2113 (f, z) on the isotropic lattice L =\u221a\n\nN [1, 1]Z\u2295\n\u221a\n6[1,\u22121]Z, and apply similar arguments as in the proof of Proposition 6.1 to\n\nobtain the following formula.\n\nProposition 6.4. Suppose that 6N is a square. Then\n\n\u03b8\u0303+N(\u03c4 ; 1) =\n1\n\n\u03b7(\u03c4)\n\n( \u2211\n\nx,y\u2208Z\ny>\n\n\u221a\n6|x|\u221a\nN\n\n(\n12\n\ny\n\n)\nsgn(x)q\n\ny2\n\n24\n\u2212 x\n\n2\n\n4N ex \u2212\n\u2211\n\nb(2\n\u221a\n6N)\n\n(\n12\n\nb\n\n)\nB1\n\n(\nb\n\n2\n\u221a\n6N\n\n)\ne\nb\n\u221a\n\nN\n6\n\n)\n\nis a mock modular form of weight 1\n2\nfor \u03c1N with shadow\n\n1\u221a\nN\n\u03b8N (\u03c4 ; 1). Here B1(x) is the\n\none-periodic function that agrees with the Bernoulli polynomial B1(x) = x\u2212 12 for 0 \u2264 x < 1.\nExample 6.5. Arguing as in Example 6.3, we obtain from Proposition 6.4 the identity\n\nq\u2212\n1\n24 f(q) =\n\n1\n\n\u03b7(\u03c4)\n\n(\n1 + 2\n\n\u2211\n\nx,y\u2208Z\nx\u22611(6)\ny>|x|\n\n(\n12\n\ny\n\n)\nsgn(x)q\n\ny2\n\n24\n\u2212x\n\n2\n\n24\n\n)\n\n\n\n24 YINGKUN LI AND MARKUS SCHWAGENSCHEIDT\n\nfor Ramanujan\u2019s order 3 mock theta function f(q). This identity is very similar to the ones\nfor Ramanujan\u2019s mock theta functions of order 5 and 7 given in Section 6 of [23].\n\nRemark 6.6. Unfortunately, if 6N is not a square, our method does not work as before. In\n\nthis case we would need to evaluate the theta lift \u03a6\n(1,0)\nL,\u2113 (f, z) on an anisotropic lattice L\n\nof signature (1, 1), where f is a weight 1 weakly holomorphic modular form for \u03c1L. Since\nthere might be holomorphic modular forms of weight 1 for \u03c1L, we cannot write f as a\nlinear combination of Maass Poincare\u0301 series to compute the theta lift of f by the unfolding\nargument as in Proposition 3.5. We wonder if our method can be adjusted to obtain a mock\n\nmodular form with shadow 1\u221a\nN\n\u03b8N(\u03c4 ; 1) using the theta lift \u03a6\n\n(1,0)\nL,\u2113 (f, z) for anisotropic L.\n\n6.2. Mock modular forms of weight 3\n2\n. We now give explicit mock modular forms of\n\nweight 3\n2\nwith shadow\n\n\u221a\nN\n\u03c0\n\u03b8N (\u03c4 ; 0). The construction works analogously as in the proof of\n\nProposition 6.1, so we omit the details for brevity.\n\nUsing the signature (1, 1) theta lift \u03a6\n(0,0)\nL,\u2113 (f, z) with a constant polynomial considered in\n\nSection 3 we obtain the following mock modular forms.\n\nProposition 6.7. (1) Suppose that 6N is a square. Then\n\n\u03b8\u0303+N (\u03c4 ; 0) =\n1\n\n\u03b7(\u03c4)\n\n(\n\u2212 2\n\n\u2211\n\nx,y\u2208Z\ny>\n\n\u221a\n6|x|\u221a\nN\n\n(\n12\n\ny\n\n)\n|x|q\n\ny2\n\n24\n\u2212 x\n\n2\n\n4N ex\n\n+\n\u2211\n\nb(2\n\u221a\n6N)\n\n(\n12\n\nb\n\n)(\nE2(\u03c4)\n\n12\n+ 2NB2\n\n(\nb\n\n2\n\u221a\n6N\n\n))\ne\nb\n\u221a\n\nN\n6\n\n)\n\nis a mock modular form of weight 3\n2\nfor \u03c1N with shadow\n\n\u221a\nN\n\u03c0\n\u03b8N (\u03c4 ; 0). Here B2(x) is\n\nthe one-periodic function that agrees with the Bernoulli polynomial B2(x) = x\n2\u2212x+ 1\n\n6\nfor 0 \u2264 x < 1, and E2(\u03c4) = 1 \u2212 24\n\n\u2211\nn\u22651 \u03c31(n)q\n\nn is the holomorphic (quasimodular)\nEisenstein series of weight 2.\n\n(2) Suppose that 6N is not a square. Let F = Q(\n\u221a\n6N) and let \u03b5N = a + b\n\n\u221a\n6N be the\n\nsmallest totally positive unit > 1 of F such that b is even and lcm(2N, 12) | (a\u2212 1).\nThen\n\n\u03b8\u0303+N (\u03c4 ; 0) =\n1\n\n\u03b7(\u03c4)\n\n\u2211\n\nx,y\u2208Z\n\u03b5\n\u22122\nN\n\n<\n\u221a\n\nNy+\n\u221a\n6x\u221a\n\nNy\u2212\n\u221a\n6x\n\n\u22641\n\n(\n12\n\ny\n\n)\nsgn(y)TrF/Q\n\n\uf8eb\n\n\uf8ed\n\u221a\nN\u221a\n6\ny + x\n\n1\u2212 \u03b5\u22121N\n\n\uf8f6\n\n\uf8f8 q\ny2\n\n24\n\u2212 x\n\n2\n\n4N ex\n\nis a mock modular form of weight 3\n2\nfor \u03c1N with shadow\n\n\u221a\nN\n\u03c0\n\u03b8N (\u03c4 ; 0).\n\nSimilarly, using the signature (1, 1) theta lift \u03a6\n(0,1)\nL,\u2113 (f ; z) with a degree (0, 1) polynomial,\n\nwe obtain the following result.\n\n\n\nMOCK MODULAR FORMS 25\n\nProposition 6.8. (1) Suppose that 2N is a square. Then\n\n\u03b8\u0303+N(\u03c4 ; 0) =\n1\n\n\u03b73(\u03c4)\n\n(\n\u2212 2\n\n\u2211\n\nx,y\u2208Z\ny>\n\n\u221a\n2|x|\u221a\nN\n\n(\n\u22124\ny\n\n)\n|x|yq\n\ny2\n\n8\n\u2212 x\n\n2\n\n4N ex\n\n+\n16N2\n\n3\n\u221a\n2N\n\n\u2211\n\nb(2\n\u221a\n2N)\n\n(\n\u22124\nb\n\n)\nB3\n\n(\nb\n\n2\n\u221a\n2N\n\n)\ne\nb\n\u221a\n\nN\n2\n\n)\n\nis a mock modular form of weight 3\n2\nfor \u03c1N with shadow\n\n\u221a\nN\n\u03c0\n\u03b8N(\u03c4 ; 0). Here B3(x) is the\n\none-periodic function that agrees with the Bernoulli polynomial B3(x) = x\n3\u2212 3\n\n2\nx2+ 1\n\n2\nx\n\nfor 0 \u2264 x < 1.\n(2) Suppose that 2N is not a square. Let F = Q(\n\n\u221a\n2N) and let \u03b5N = a + b\n\n\u221a\n2N be the\n\nsmallest totally positive unit > 1 of F such that b is even and lcm(2N, 4) | (a \u2212 1).\nThen\n\n\u03b8\u0303+N (\u03c4 ; 0) =\n1\n\n\u03b7(\u03c4)3\n\n\u2211\n\nx,y\u2208Z\n\u03b5\n\u22122\nN\n\n<\n\u221a\n\nNy+\n\u221a\n2x\u221a\n\nNy\u2212\n\u221a\n\n2x\n\u22641\n\n(\n\u22124\ny\n\n)\nsgn(y)TrF/Q\n\n(\u221a\n2N(y\n\n2\n+ x\u221a\n\n2N\n)2\n\n1\u2212 \u03b5\u22122N\n\n)\nq\n\ny2\n\n8\n\u2212 x\n\n2\n\n4N ex\n\nis a mock modular form of weight 3\n2\nfor \u03c1N with shadow\n\n\u221a\nN\n\u03c0\n\u03b8N (\u03c4 ; 0).\n\nReferences\n\n[1] R.E. Borcherds. Automorphic forms with singularities on Grassmannians. Invent. Math., 132(3):491\u2013\n562, 1998. 3, 9, 10, 12, 13\n\n[2] R.E. Borcherds. The Gross-Kohnen-Zagier theorem in higher dimensions. Duke Math. J., 97(2):219\u2013233,\n1999. 8\n\n[3] R.E. Borcherds. Correction to: \u201cThe Gross-Kohnen-Zagier theorem in higher dimensions\u201d [Duke Math.\nJ. 97 (1999), no. 2, 219\u2013233]. Duke Math. J., 105(1):183\u2013184, 2000. 2\n\n[4] K. Bringmann, A. Folsom, and K. Ono. q-series and weight 3/2 Maass forms. Compos. Math.,\n145(3):541\u2013552, 2009. 1\n\n[5] K. Bringmann and K. Ono. Dyson\u2019s ranks and Maass forms. Ann. Math., 171(1):419\u2013449, 2010. 1\n[6] J.H. Bruinier. Borcherds products on O(2, l) and Chern classes of Heegner divisors. Number 1780 in\n\nLecture Notes in Mathematics. Springer-Verlag, Berlin, 2002. 3, 5, 6, 9, 10, 11, 12, 13\n[7] J.H. Bruinier, S. Ehlen, and T. Yang. CM values of higher automorphic Green functions for orthogonal\n\ngroups. preprint, arXiv:1912.12084, 2019. 2\n[8] J.H. Bruinier and J. Funke. On two geometric theta lifts. Duke Math. J., 125(1):45\u201390, 2004. 1, 5, 6, 8\n[9] J.H. Bruinier and J. Funke. Traces of CM values of modular functions. J. Reine Angew. Math., 594:1\u201333,\n\n2006. 12\n[10] J.H. Bruinier and K. Ono. Heegner divisors, L-functions and harmonic weak Maass forms. Ann. Math.,\n\n172:2135\u20132181, 2010. 23\n[11] J.H. Bruinier and M. Schwagenscheidt. Algebraic formulas for the coefficients of mock theta functions\n\nand Weyl vectors of Borcherds products. J. Algebra, 478:38\u201357, 2017. 1, 2\n\n\n\n26 YINGKUN LI AND MARKUS SCHWAGENSCHEIDT\n\n[12] J.H. Bruinier and M. Schwagenscheidt. Theta lifts for Lorentzian lattices and coefficients of mock theta\nfunctions. Math. Zeit., 2020. 11\n\n[13] P. Charollois and Y. Li. Harmonic Maass forms associated to real quadratic fields. J. Eur. Math. Soc.\n(JEMS), 22(4):1115\u20131148, 2020. 2, 4, 5, 18, 19, 22\n\n[14] A. Dabholkar, S. Murthy, and D. Zagier. Quantum black holes, wall crossing, and mock modular forms.\nCambridge Monographs in Mathematical Physics, to appear:151 pages, 2019. 4, 18\n\n[15] S. Ehlen. CM values of regularized theta lifts and harmonic weak Maass forms of weight 1. Duke Math.\nJ., 166(13):2447\u20132519, 2017. 2, 3\n\n[16] A. Erde\u0301lyi, W. Magnus, F. Oberhettinger, and F. Tricomi. Tables of integral transforms. Vol. I.\nMcGraw\u2013Hill Book Company, Inc., New York\u2013Toronto-London, 1954. Based, in part, on notes left\nby Harry Bateman. 14\n\n[17] J. Funke and J. Millson. Spectacle cycles with coefficients and modular forms of half-integral weight. In\nArithmetic Geometry and Automorphic Forms, in honor of Stephen S. Kudla. Higher Eduction Press\nand International Press, 2011. 22\n\n[18] E. Hecke. Zur Theorie der elliptischen Modulfunktionen. Math. Ann., 97(1):210\u2013242, 1927. 4\n[19] F. Hirzebruch and D. Zagier. Intersection numbers of curves on Hilbert modular surfaces and modular\n\nforms of Nebentypus. Invent. Math., 36:57\u2013113, 1976. 1, 3\n[20] Y. Li. Average CM-values of higher Green\u2019s function and factorization. preprint, arXiv:1812.08523,\n\n2018. 2\n[21] M. Viazovska. Petersson inner products of weight-one modular forms. J. Reine Angew. Math., 749:133\u2013\n\n159, 2019. 3\n[22] D. Zagier. Nombres de classes et formes modulaires de poids 3/2. C.R. Acad. Sci. Paris (A), 281:883\u2013\n\n886, 1975. 3\n[23] D. Zagier. Ramanujan\u2019s mock theta functions and their applications (after Zwegers and Ono-\n\nBringmann). Number 326, pages Exp. No. 986, vii\u2013viii, 143\u2013164 (2010). 2009. Se\u0301minaire Bourbaki.\nVol. 2007/2008. 1, 24\n\n[24] S.P. Zwegers. Mock \u03b8-functions and real analytic modular forms. Contemp. Math., 291:269\u2013277, 2001.\n23\n\n[25] S.P. Zwegers.Mock Theta Functions. Proefschrift Universiteit Utrecht, 2002. Thesis (Ph.D.)\u2013Universiteit\nUtrecht. 1\n\nFachbereich Mathematik, Technische Universita\u0308t Darmstadt, Schlossgartenstrasse 7, D\u2013\n\n64289 Darmstadt, Germany\n\nEmail address : li@mathematik.tu-darmstadt.de\n\nETH Zu\u0308rich Mathematics Dept., Ra\u0308mistrasse 101, CH-8092 Zu\u0308rich, Switzerland\n\nEmail address : mschwagen@ethz.ch\n\n\n\t1. Introduction\n\t2. Preliminaries\n\t2.1. Modular forms for the Weil representation\n\t2.2. Unary theta series and the eta function\n\t2.3. Regularized inner product and pairing.\n\n\t3. Theta lifts\n\t3.1. Theta lifts on isotropic lattices of signature (1,n)\n\t3.2. Theta lifts on anisotropic lattices of signature (1,1)\n\n\t4. Proof of Theorem 1.1\n\t5. Proof of Theorem 1.5 and Example 1.6\n\t6. Explicit constructions of mock modular forms\n\t6.1. Mock modular forms of weight 12\n\t6.2. Mock modular forms of weight 32\n\n\tReferences\n\n"
"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDoes Continual Learning = Catastrophic Forgetting?\n\nAnh Thai, Stefan Stojanov, Zixuan Huang, Isaac Rehg, James M. Rehg\nGeorgia Institute of Technology\n\n{athai6,sstojanov,zixuanh,isaacrehg,rehg}@gatech.edu\n\nAbstract\n\nContinual learning is known for suffering from catas-\ntrophic forgetting, a phenomenon where earlier learned\nconcepts are forgotten at the expense of more recent sam-\nples. In this work, we challenge the assumption that con-\ntinual learning is inevitably associated with catastrophic\nforgetting by presenting a set of tasks that surprisingly do\nnot suffer from catastrophic forgetting when learned con-\ntinually. We provide evidence that these reconstruction-\ntype tasks exhibit positive forward transfer and that single-\nview 3D shape reconstruction improves the performance\non learned and novel categories over time. We provide\nthe novel analysis of knowledge transfer ability by look-\ning at the output distribution shift across sequential learn-\ning tasks. Finally, we show that the robustness of these\ntasks leads to the potential of having a proxy representa-\ntion learning task for continual classification. The code-\nbase, dataset and pre-trained models released with this ar-\nticle can be found at https://github.com/rehg-lab/CLRec.\n\n1. Introduction\nIn continual learning (CL), a stream of incrementally-\n\narriving inputs is processed without access to past data.\nA key challenge is to avoid catastrophic forgetting [30]\u2014\nlarge negative backward transfer (BWT) [22], which arises\nif previously-learned representations are degraded by more\nrecent exposures. Substantial effort has been made to com-\nbat forgetting [10, 12, 21, 52], and it has come to exemplify\ncontinual learning. However, past works have explored a\nsurprisingly limited set of tasks, with an almost exclusive\nfocus on classification.\n\nIn this work, we demonstrate the surprising finding that\na broad set of continual reconstruction tasks, including 3D\nshape reconstruction, 2.5D sketch estimation and 2D im-\nage reconstruction, do not exhibit catastrophic forgetting\n(Sec. 4). In fact, we show that these tasks exhibit mini-\nmal negative backward transfer and even positive backward\ntransfer without the use of any heuristics or strategies to pre-\nvent forgetting. To the best of our knowledge, we are the\n\nfirst to provide an extensive study of CL for reconstruction-\ntype tasks. Our findings suggest that the challenge of catas-\ntrophic forgetting, established by prior work on CL classifi-\ncation, does not in fact characterize all CL problems.\n\nIn addition to mitigating negative BWT, another essential\ncharacteristic of successful CL models is achieving positive\nforward transfer (FWT). Positive FWT [22] arises when a\nmodel\u2019s representation, trained on a sequence of prior tasks,\nis beneficial for a future learning task. This is an impor-\ntant property because it can enable CL methods to leverage\nshared representations, a common property of batch learn-\ning methods (e.g. learning low-level visual features in im-\nage classification) and a goal of transfer learning. Collec-\ntively, BWT and FWT characterize the effectiveness of CL\nmethods in evolving feature representations incrementally.\n\nRecently, the GDumb (classifier) baseline [26] has called\nthe role of FWT in CL into question. GDumb is an episodic\nrepresentation learner, which maintains a set of evolving\nexemplar memory but reinitializes the feature representa-\ntion and trains it from scratch during each learning ex-\nposure1, eliminating the possibility of FWT. GDumb was\nshown to out-perform other state-of-the-art methods which\nwere designed to achieve positive FWT, highlighting the\ntension between the goals of achieving positive FWT and\navoiding negative BWT. These prior findings for classifica-\ntion beg the question of whether achieving positive FWT\nis beneficial for CL reconstruction. We demonstrate that\ncontinuously-updated representations lead to improved per-\nformance and positive FWT for a variety of reconstruction\ntasks (see Sec. 5).\n\nA limitation of prior work on knowledge transfer (i.e.\nFWT and BWT) is that the findings depend upon the use\nof a specific CL method. A basic question is whether we\ncan find algorithm-agnostic characterizations of CL tasks\nthat might shed light on the surprising behavior of contin-\nual reconstruction. We define a measure of the output distri-\nbution shift across learning exposures and demonstrate that\nsmall distribution shifts across sequential tasks lead to bet-\nter knowledge transfer and improvements in BWT and FWT\n\n1We use the term learning exposure to refer to each new increment of\ndata, i.e. the learner\u2019s next \u201dexposure\u201d to the concepts being learned.\n\nar\nX\n\niv\n:2\n\n10\n1.\n\n07\n29\n\n5v\n3 \n\n [\ncs\n\n.L\nG\n\n] \n 8\n\n D\nec\n\n 2\n02\n\n1\n\n\n\nInput Rep.\u2192 Output Rep. Reconstruction Tasks\n\n2D\u2192 2D\nImage auto-encoding (Fig. 2c)\n\nSingle-view Silhouette Pred. (Fig. 2b)\n\n2D\u2192 2.5D\nSingle-view Depth Pred. (Fig. 2a)\n\nSingle-view Surface Normals Pred. (Fig. 2a)\n2D\u2192 3D Single-view Image 3D Shape Rec. (Figs. 1a, 1b)\n\n2.5D\u2192 3D Single-view Depth 3D Shape Rec. (Figs. 1a, 1b)\n3D\u2192 3D Single-object Pointcloud 3D Shape Rec. (Fig. 1c)\n\nTable 1. Summary of the reconstruction tasks we evaluate that\ndemonstrate robustness to catastrophic forgetting. There are 5\ntypes of tasks based on the input to output representation mapping.\n\n(Sec. 6). This gives us the ability to \u201cforecast\u201d the perfor-\nmance of a supervised CL task in a way that is agnostic to\nthe algorithm and backbone architecture design. We believe\nthese are the first results to demonstrate the feasibility of ap-\nproximating the difficulty of a CL task without performing\ncomputationally expensive model training.\n\nAs a means to further investigate the relationship be-\ntween continual reconstruction and categorization, we\ndemonstrate that continual single-view 3D shape recon-\nstruction can serve as an effective proxy task for classifi-\ncation. Specifically, a continuously-trained shape represen-\ntation (without any class label supervision) is effective for\ncontinual image classification given only a small exemplar\nbudget (Sec. 7). In summary, this paper makes the following\ncontributions:\n\n\u2022 The novel finding that some continual reconstruction\ntasks (Tbl. 1) do not suffer from catastrophic forgetting\n(Sec. 4).\n\n\u2022 That these continual reconstruction tasks demonstrate\npositive forward transfer and the ability of single-view\n3D shape reconstruction to generalize to novel classes\nunseen during training (Sec. 5).\n\n\u2022 Novel analysis of knowledge transfer ability in CL\ndemonstrates that smaller output distribution shift\nacross learning exposures leads to better knowledge\ntransfer in CL (Sec. 6)\n\n\u2022 Using single-view 3D shape reconstruction as a proxy\ntask for classification results in a competitive CL\nmethod given a limited exemplar budget (Sec. 7).\n\n2. Related Work\nOur work is most closely-related to four bodies of prior\n\nwork: 1) CL works outside of the image classification\nparadigm (relevant to our findings on CL for reconstruc-\ntion), 2) Analysis of CL (relevant to our output distribu-\ntion shift analysis), 3) Generalization ability of models for\nsingle image 3D shape reconstruction (relevant to our in-\nvestigation of generalization ability of CL single-view 3D\nshape reconstruction models), and 4) CL for classification\ntask (relevant to our proxy representation task findings).\nCL of Non-Classification Tasks. We are the first to in-\nvestigate and demonstrate that a set of CL tasks is in-\n\ntrinsically robust to catastrophic forgetting. While most\nprior CL works have addressed image classification, a few\nprior works have addressed various other tasks: Aljundi et\nal. [3] studied the problem of actor face tracking in video,\nwhile [1,7,23,25] explored image segmentation. Shmelkov\net al. [34] and Liu et al. [20, 40] investigated incremental\nobject detection while [18, 44] learned image generation.\nElhoseiny et al. [11] examined continual fact learning by\nutilizing a visual-semantic embedding. Wang et al. [41]\nstudied CL of camera localization given input RGB image\nwhile Cai et al. [5] explored online CL of geolocalization\nwith natural distribution shift in the input that occurs over\nreal time. Others [2,13,48] focused on reinforcement learn-\ning task.\n\nMost closely related to our work is Yan et al. [50] that in-\nvestigated continual learning of scene reconstruction. Simi-\nlar to our work, they employed implicit shape representation\n(signed-distance-field) to represent 3D scenes. In contrast,\nthis work aimed to continually reconstruct the input scene\ngiven a stream of depth images from different views. The\ninput distribution shift in this setting is the shift between one\nview of the scene to another and the objective is to produce\na smooth representation of the same input scene observed\nover time. Our work on the other hand, explores CL of re-\nconstruction task in the context of visual classes, which is\nmore challenging since the underlying semantic in the in-\nputs changes over time. Note that all of these CL works\nreported challenges with catastrophic forgetting commen-\nsurate with the classification setting.\nAnalysis of Continual Learning. Our analysis of the be-\nhavior of CL tasks is most closely related to the body of\nworks that analyzes general dynamics of CL [14,39]. While\nVerwimp et al. [39] examined the benefits and drawbacks of\nrehearsal methods in CL, Knoblauch [14] showed that opti-\nmal CL algorithms solve an NP-HARD problem and require\nperfect memory. Specifically, optimal parameters \u03b8t for\neach new task must lie in the intersection of SAT of all tasks\nlearned up to t. Perfect memory refers to the ability to ap-\nproximate the parameters that optimize all seen tasks. This\napproach explains the merit of employing memory replay in\nCL instead of regularization-based approaches. While [17]\ndiscussed the different concept drift in CL, our analysis fo-\ncuses more on the output distribution shift that can be used\nas a means to understand the knowledge transfer ability of\nvarious CL tasks.\nGeneralization in Batch-Mode 3D Shape Reconstruc-\ntion. Our analysis of the generalization ability of CL 3D\nsingle-view shape reconstruction task in Sec. 5 is based on\nprior works that investigate the ability of single image 3D\nshape reconstruction models to generalize to unseen shape\ncategories in batch mode [33, 37, 53]. We are the first to\nprovide generalization analysis of these models in the CL\nsetting, utilizing the 3-DOF VC approach which has been\n\n\n\n(a) (b) (c)\n\nFigure 1. (a) Performance of shape reconstruction methods with 2D and 2.5D inputs when presented with a single exposure for each\ncategory from all 55 categories of ShapeNetCore.v2, 5 classes/exposure (b) repeated exposures case on ShapeNet13 with 10 repeated\nexposures, 2 classes/exposure (c) single exposure case on ShapeNetCore.v2 of shape methods with 3D inputs. Backward transfer is\nreported in parenthesis. Catastrophic forgetting does not happen to any of the algorithms in any case.\n\nshown to learn a more general shape representation than the\nobject-centered (OC) approach.\nCL for Classification. Our work on a reconstruction-based\nproxy task for CL classification (Sec. 7) is unique, but it\nis peripherally-related to other CL works which explore al-\nternative classification losses or forms of supervision. We\nshare with Yu et al. [51] the use of the nearest-class-mean\n(NCM) classification rule. We use NCM for classification\nbased on a latent shape representation trained without class\nsupervision, while Yu et al. use NCM for classification in\nan embedding layer which is trained with ground-truth class\nlabels. Another related work by Rao et al. [29] performs un-\nsupervised CL in a multi-task setting where the boundaries\nbetween tasks are unknown. In contrast, our unsupervised\ntraining paradigm utilizes single-view 3D shape reconstruc-\ntion as a proxy task.\n\n3. Problem Formulation\nSupervised Reconstruction Tasks. The objective of these\ntasks is to learn the mapping function f\u03b8 : X \u2192 Y over\nthe observed the data {(xi, yi)}\n\nN\ni=1 \u223c D. For example,\n\nsingle-view 3D shape reconstruction aims to output the 3D\nshape of the object represented in the input image while\ndepth map reconstruction predicts the depth values of the\nscene/object given in the input. Note that x and y can be\ndifferent depending on the specific reconstruction task con-\nsidered. Generally, the desired function f is a composition\nof two functions f = D \u25e6 E. The encoder E extracts the\nfeature representation from the input x, followed by the de-\ncoderD that produces the output y from the encoded feature\nrepresentation.\nContinual Learning of Reconstruction. In this setting, at\neach learning exposure t, the learning model observes the\ndata {(x(t)i , y\n\n(t)\ni )}\n\nNt\ni=1 \u223c Dt indexed by t \u2208 {1, 2 . . . , T}\n\nand learns to optimize the parameters \u03b8t of the function\nf\u03b8t : Xt \u2192 Yt by minimizing the supervised loss L(\u03b8t) =\nEDt [`(y\n\n(t), f\u03b8t(x\n(t)))] where `(\u00b7, \u00b7) is some loss function as-\n\nsociated with the specific reconstruction task. We employ\nthe notion of single exposure to refer to the standard contin-\nual learning paradigm where data is introduced sequentially\nand never revisited while repeated exposures refers to the\nparadigm introduced in Stojanov et al. [35] where data can\n\nbe revisited after being learned. In this setting, each visual\nclass occurs a fixed number of times (e.g. 10 repetitions) in\nrandom order2. Note that in this work, we assume that each\nDt is defined over a set ofMt visual categories {C\n\n(k)\nt }\n\nMt\nk=1.\n\n3\n\nTraining. During training, the learning model does not\nhave access to previously seen dataD1:t\u22121. We optimize the\nparameters \u03b8t of the function f continuously at each learn-\ning exposure upon observing the data stream Dt. Specifi-\ncally, the learned parameters \u03b8t\u22121 at exposure t \u2212 1 serve\nas the initialization parameters for the model at exposure t,\nwhich we referred to as continuous representation learning.\nThis is the standard SGD training that has been shown to\nsuffer from catastrophic forgetting in prior works. With-\nout any further heuristics such as additional losses, external\nmemory or other methods employed, this technique is re-\nferred to as finetuning strategy [19].\nEvaluation. At test time, the model is evaluated on the test\nsplit of all known categories. Specifically, at each learning\nexposure t we compute the average accuracy of all classes\nseen up to t. Specifically, Acct =\n\n1\nNt\n\n\u2211Nt\ni=1 acc\n\n(t)\ni where\n\nNt is the number of classes seen up to exposure t and acc\n(t)\ni\n\nis the accuracy of class i after learning exposure t. Plotting\nthe average accuracy at all learning exposures results in the\nlearning curve of the CL model (e.g. Fig. 1a). Note that\naccuracy metrics reported for all the tasks are in range [0, 1].\n\nWe further report backward and forward transfer met-\nrics [22] in addition to the average performance curve at\neach learning exposure. Specifically, backward transfer\n(BWT) measures the average change in performance in the\nlast learning exposure w.r.t when the concepts are first in-\ntroduced and forward transfer (FWT) indicates the aver-\nage change in performance between the random initializa-\ntion and the performance of the learning exposure right be-\nfore the concepts are introduced. Note that while BWT is\nbounded in [\u22121, 1], FWT depends on the random initializa-\ntion performance on each dataset. A more successful CL\nlearner will demonstrate higher BWT and FWT.\nPositioning CL Reconstruction. Considering the three\ncontinual learning scenarios [38], CL reconstruction is most\n\n2Details discussed in the Supplement.\n3Organizing shapes into categories allows us to characterize how new\n\nshape concepts are introduced during learning.\n\n\n\n(a) (b) (c)\n\nFigure 2. (a) Results for 2.5D estimation. Performance in terms of thresholding accuracy (\u03b4 = 1.25) for depth prediction and thresholding\ncosine distance (\u03b4 = 0.9) for surface normals (b) IoU of silhouette prediction model (c) SSIM of image autoencoding. Backward transfer\nis reported in parenthesis. The performance of CL models increases over time and approaches batch in 2D reconstruction task.\n\nclosely related to Domain-IL scenario. In both settings, the\nlearning objective (e.g. depth value prediction) is the same\nacross learning exposures, but the input data distribution\nchanges over time (from one set of visual classes to an-\nother). This breaks the i.i.d.-sampling assumption present in\nthe SGD optimization procedure where each training mini-\nbatch is sampled i.i.d. from the entire data distribution. This\npresents the same main challenges faced by other CL tasks\nsuch as classification, object detection, and segmentation.\n\n4. Reconstruction Tasks Do Not Suffer from\nCatastrophic Forgetting\n\nWe identify 5 types of reconstruction tasks based on their\ninput and output properties, as listed in Tbl. 1. Our key find-\ning is that CL tasks of each of these five types do not suffer\nfrom catastrophic forgetting. It is important to emphasize\nthat the \u201ccontinual learning\u201d algorithm used in this section\nis the simple finetuning strategy specified in Sec. 3 that is\nknown to perform poorly for CL classification task. Specif-\nically, we do not need to utilize additional losses, external\nmemory, or other methods to achieve good continual learn-\ning performance.\n\nNote that different categories of shapes exhibit signifi-\ncant domain shift that poses significant challenges to con-\ntinual learning. For example, the categories \u201cchair\u201d and\n\u201cbowl\u201d in ShapeNet define very different 3D data distribu-\ntions with no parts in common. From this point of view,\nit is quite surprising that we do not observe forgetting for\nsuch continual reconstruction tasks. We therefore organize\nshapes by category in constructing our learning exposures,\nso that the category label is a means to characterize the do-\nmain shift between successive exposures.\n\nOur findings for learning 3D shape reconstruction,\n2.5D prediction, and 2D reconstruction are presented in\nSecs. 4.1, 4.2, and 4.3 respectively. We report the average\naccuracy at each learning exposure as described in Sec. 3\nand backward transfer for all the experiments.\n\n4.1. Single Object 3D Shape Reconstruction\n\nWe first present reconstruction tasks where the output\nrepresentation is in 3D. Specifically, the goal of the desired\nfunction f is to produce a 3D surface representation of the\nsingle object present in the input. We examine implicit con-\ntinuous 3D representations such as signed-distance-fields\n\n(SDF) and continuous occupancies since they were iden-\ntified to achieve superior performance in the batch set-\nting [24, 37, 49].\nApproach. We utilize SDFNet [37] and OccNet [24] as\nbackbone architectures for CL. We train both methods with\nthe 3-DOF VC representation (varying in azimuth, elevation\nand camera tilt) from [37], which was shown to give the\nbest generalization performance.4 We also train with OC\nrepresentation for SDF representation, in which the model\nis trained to output the shape in the canonical pose. We\nexamine the behavior of these models given different input\nrepresentations: 2D where inputs are single-view RGB im-\nages, 2.5D where inputs are ground truth depth and normals\nmaps and 3D where inputs are sparse 3D pointclouds.\nDatasets & Metric. We train on all 55 classes of ShapeNet-\nCore.v2 [8]5 (52K instances) with 5 classes per exposure\nfor the single exposure case, and on the largest 13 classes of\nShapeNetCore.v2 (40K meshes), denoted as ShapeNet13,\nwith 2 classes per exposure for the repeated exposure case.\nNote that ShapeNetCore.v2 is currently the largest shape\ndataset with category labels and ShapeNet13 is the standard\nsplit for 3D shape reconstruction. Each exposure is gener-\nated from all of the samples from the training split of each\ncategory currently present.6 Following prior works in shape\nreconstruction [36, 37, 49] we report the average FS@1 at\neach learning exposure. We use SDFNet as the batch refer-\nence. All models are trained from random initialization.\nResults. The results are shown in Figs. 1a, 1b and 1c for\nsingle and repeated exposures on all single object 3D shape\nreconstruction settings (last 3 rows of Tbl. 1). For single\nexposure with 2D and 2.5D inputs (Fig. 1a), all algorithms\nmaintain their accuracy over time and even exhibit a slight\nupward trend of increasing accuracy while for 3D inputs\n(Fig. 1c) the performance significantly increases over time\nand is on par with batch. Note that we conducted 3 runs and\nthe results converge to the same conclusion with an average\nstd of 0.02 at each learning exposure. All models including\nthe model trained with OC representation do not suffer from\ncatastrophic forgetting as evidenced by the minimal nega-\n\n4SDFNet with 3-DOF VC is the current SOTA for single image 3D\nshape reconstruction.\n\n5We provide more details our dataset use and licensing in Supp.\n6For instance, if chair and table are present in the current learning ex-\n\nposure, the model will be trained on all chairs and tables in the respective\ntraining splits.\n\n\n\ntive and even positive backward transfer. This is surprising\nsince we are not taking any steps to ameliorate catastrophic\nforgetting and each learning exposure presents a significant\ndomain shift, as the learner must incorporate information\nabout the shape of a new class of objects. Since SDFNet\nand OccNet differ significantly in shape representation and\nare trained with different losses (L1 loss for SDFNet and\nBCE loss for OccNet) this finding possibly reflects a basic\nproperty of the shape reconstruction problem rather than the\ninductive biases of a particular model.\n\nIn the repeated exposures setting (Fig. 1b), the perfor-\nmance of both architectures when trained with 3-DOF VC\nimproves significantly over time, and eventually performs\non par with the batch learner.7 These models achieve sig-\nnificant positive BWT which indicates that catastrophic for-\ngetting is mitigated. Unlike the experiments in [35], which\nshowed similar asymptotic behavior for classification accu-\nracy, these results were obtained without exemplar memory\nor other heuristics. Note that SDFNet trained with OC does\nnot show a significant increase as 3-DOF VC over time.\nThis complements the finding in [37] that training with 3-\nDOF VC results in a more robust feature representation.\n\n4.2. Single-view 2.5D Sketches Prediction\n\nThe task in Sec. 4.1 requires the model to infer the global\n3D structure of each object. In this section we investi-\ngate the related task of estimating depth and surface nor-\nmals (2.5D) from RGB input images in the single exposure\ncase (Tbl. 1, second row). We adopt the U-ResNet18-based\nMarrNet [45] architecture, with an ILSVRC-2014 [32] pre-\ntrained ResNet18 for the image encoder. We evaluate depth\nprediction using the commonly used thresholding accu-\nracy [15, 28]. For normals prediction, we report the ac-\ncuracy based on the cosine distance threshold between the\npredicted and ground truth surface normals [43]8. Fig. 2a\ndemonstrates that single exposure 2.5D prediction does not\nsuffer catastrophic forgetting as the accuracy increases over\ntime. These findings further extend the 3D shape recon-\nstruction results from Fig. 1.\n\n4.3. 2D Reconstruction\n\nThe tasks in Secs. 4.1 and 4.2 require the model to solve\na challenging 2D to 3D inference problem. We conduct ad-\nditional experiments on continual 2D to 2D mapping that\nincludes learning to segment foreground/background given\nan RGB input image and image autoencoding. For silhou-\nette prediction, we utilize U-ResNet18-based MarrNet [45]\narchitecture train with BCE loss. We report the average IoU\nat each learning exposure as in Fig. 2b which demonstrates\nthat single exposure silhouette prediction does not suffer\n\n7The 65 learning exposures (x axis) in Fig. 1b result from 13 ShapeNet\nclasses divided by 2 classes per exposure with 10 repetitions each.\n\n8More details in Supplement\n\ncatastrophic forgetting (minimal negative backward trans-\nfer). In fact we observe that the IoU increases over time.\n\nFor image autoencoding, we present results in Fig. 2c.\nWe use a randomly initialized shallow architecture with 4\nconv. layers followed by max pooling for the encoder where\nthe bottle-neck feature vector has dimension 16\u00d72\u00d72. We\nexperiment on CIFAR-100 [16] (size 32\u00d732) with one class\nper exposure and use SSIM [42] scaled to range [0, 1] as the\naccuracy metric. SSIM increases over time and eventually\nreaches batch performance. This is yet more evidence for\nthe robustness of continual reconstruction.\n\n4.4. Discussion of CL Reconstruction & Limitations\n\nWe have identified (for the first time) a set of continual\nreconstruction tasks that do not suffer from forgetting, as\nevidenced by the results in Figs. 1 and 2. These models\nwere trained with standard SGD, without exemplar mem-\nory or other heuristics. One point of contact between clas-\nsification and reconstruction is that both sets of tasks ben-\nefit significantly from repeated exposures (see Fig. 1b). In\nSec. 5 we demonstrate the value of continuously-updated\nrepresentations on both seen and novel classes.\n\nWe now briefly discuss two potential limitations of our\nwork. First, our reconstruction experiments, with the ex-\nception of image autoencoding, all use synthetic 3D ob-\nject models as opposed to real-world images. However, we\npoint out that 3D shape reconstruction on synthetic images\n(with 2D and 2.5D inputs) is still a very challenging com-\nputational problem, e.g. the SOTA result on ShapeNet13\nis an FS@1 of 0.5 out of a maximum of 1.0 [37]. This in\nturn raises the second possible limitation, that the lack of\nforgetting may be tied in part to the fact that the models are\nnot yet able to achieve very high accuracy. More accurate\nmodels might be more closely tuned to the data distribu-\ntion in each exposure, increasing the potential for domain\nshift. While this might be true for 3D shape reconstruction\nfrom 2D and 2.5D inputs, the 3D shape reconstruction from\n3D inputs, 2.5D and 2D reconstruction tasks achieve a high\nlevel of accuracy, which provides a counterpoint to the ar-\ngument. In Sec. 6 we provide one explanation for these ob-\nservations. We hope this work will encourage the commu-\nnity to conduct additional investigations into this intriguing\nphenomenon.\n\nNegative societal impacts. Training CL models is com-\nputationally expensive since the models are trained to con-\nvergence for multiple learning exposures. This can be miti-\ngated by algorithm improvements that allow models to learn\nfaster. Keeping exemplar memory for replay might violate\nthe privacy policies for sensitive data, which can be ad-\ndressed by generative methods.\n\n\n\nFigure 3. (a) Performance on seen classes of GDumb and GSmart in 3D shape reconstruction with 2.5D inputs on ShapeNet13 with 1K\nexemplars (3.7% of training data). GSmart outperforms GDumb by a significant margin. (b) Generalization performance to 42 unseen\ncategories of ShapeNetCore.v2 of GDumb, GSmart and C-SDFNet. Generalization ability of GSmart and C-SDFNet increases over time\nwhile constantly staying low for GDumb demonstrates the benefit of continuous representation learning. Note that all models are trained\nwith 3-DOF VC approach.\n5. Forward Transfer in CL Single-view 3D Re-\n\nconstruction\n\nIn this section, we discuss the ability of the learn-\ning model to propagate useful representations learned in\nthe past to current and future learning exposures (positive\nFWT). We focus our analysis on the challenging problem\nof single-view 3D shape reconstruction. We first demon-\nstrate that continuous representation learning is beneficial\nas we observe significantly stronger performance compared\nto episodic representation learning for this task. We fur-\nther note that positive FWT is obtained, as evidenced by\nthe accuracy improvement on seen and novel classes over\ntime. While generalization to unseen classes has been stud-\nied extensively in the batch setting of single-view 3D shape\nreconstruction, and has been identified to be a significantly\nchallenging problem [37,53], we are the first to analyze this\nbehavior in a continual learning setting.\n\nGDumb [26] is an episodic representation learner, de-\nsigned to test the hypothesis that there is no value in contin-\nuous representation learning. Specifically, at each learning\nexposure, the model is randomly reinitialized and trained\nfrom scratch on the exemplar set which ensures that a sub-\nset of data from all previous learning exposures is avail-\nable. This approach surprisingly achieves competitive per-\nformance at classification. We hypothesize that in con-\ntrast to this observation, continuous representation learn-\ning improves the performance in single-view 3D shape re-\nconstruction due to the feasibility of positive FWT. In or-\nder to test this hypothesis, we design GSmart, an algorithm\nthat continuously trains the feature representation instead\nof reinitializing the weights at each learning exposure as in\nGDumb. See the supplement for details. Note that both\nmethods are trained on the same sized exemplar set ran-\ndomly selected at each learning exposure.\n\nWe conduct our experiments on ShapeNet13 with sin-\ngle exposure and 1 shape class per learning exposure. We\nchoose K = 1000 (3.7% of total training data) to be the ex-\nemplar set size and evaluate the performance of the models\non all learned classes (Sec. 3). In Fig. 3a we observe that the\n\nperformance of GSmart improves over time and eventually\nexceeds that of GDumb by 0.15 FS@1. This significant gap\nhighlights the benefit of continuous representation learning\nacross each learning exposure.\n\nWe further investigate the ability of these models to gen-\neralize to novel categories. We evaluate GDumb, GSmart\nand continual SDFNet (C-SDFNet) (Sec. 4.1) on a held out\nset of 42 classes of ShapeNetCore.v2 with 50 instances for\neach category (Fig. 3b). All algorithms perform poorly on\nthe unseen classes after the initial learning exposures, which\ndemonstrates that it is significantly challenging to general-\nize to novel categories after learning on only a few classes.\nHowever, the performance of C-SDFNet and GSmart im-\nproves over time as more classes are learned while GDumb\nremains low. This illustrates benefit of continuous represen-\ntation learning as a useful feature that aids generalization\nand improves the performance on novel classes over time.\nWe note that positive FWT is also observed in other recon-\nstruction tasks (see Tbl. 2).\n\n6. Analysis of Knowledge Transfer Ability\nOur findings in Secs. 4 and 5 have highlighted the signif-\n\nicance of knowledge transfer in CL reconstruction. While\nBWT and FWT quantify the knowledge transfer during CL,\nthey require training and evaluating computationally expen-\nsive CL models.9 Furthermore, these measures only re-\nflect the performance of specific CL algorithms and do not\nspeak to a CL task in general. In this section, we attempt to\ngain more insight into knowledge transfer given a task and\na dataset in an algorithm-agnostic manner, by focusing on\nchanges in the output distribution. We use this approach to\nfurther analyze the benefit of exemplar memory in classifi-\ncation. We begin by stating a key hypothesis connecting the\noutput distribution to CL task knowledge transfer ability.\nHypothesis: Given a supervised task T with input and out-\nput distributions X and Y , at each learning exposure t, the\n\n9Training and evaluating 3D shape reconstruction from 3D inputs on\nShapeNetCore.v2 takes 3 days on two NVIDIA GeForce RTX 2080Ti\nGPUs. On the other hand, computing output distribution distance only\ntakes \u2248 45 minutes which is two orders of magnitude more efficient\n\n\n\nTask Mean Dist. \u2193 BWT \u2191 FWT \u2191\nSil Pred. 0.075 -0.003 0.836\n\nVC 3D Shape Rec. 0.077 -0.123 0.105\nDepth Pred. 0.084 -0.136 0.094\n\nOC 3D Shape Rec. 0.116 -0.220 0.090\nClassification 1 -1 -0.077\n\nTable 2. The relationship between the mean output distribution\ndistance across learning exposures and BWT and FWT for differ-\nent CL tasks. Lower is better for distribution distance while higher\nis better for BWT and FWT. Lower distance leads to better knowl-\nedge transfer (higher BWT and FWT).\n\nmodel observes Xt \u223c X and Yt \u223c Y . We hypothesize that\nif the distance of the conditional distribution Yt|Xt between\neach learning exposure becomes smaller, knowledge (back-\nward and forward) transfer increases for any CL method.\n\nWe now present the intuition behind our formulation. Let\nD be some dataset consisting of two parts D1 and D2 that\nare independently generated. During batch training we op-\ntimize the parameters \u03b8 by minimizing the loss\n\n\u03b8 = argmin\n\u03b8\nL(\u03b8) = \u2212 log p(D|\u03b8) (1)\n\nSince D = D1 \u222a D2 and D1 and D2 are independent, Eq. 1\nbecomes\n\n\u03b8 = argmin\n\u03b8\nL(\u03b8) = \u2212 log p(D1|\u03b8)\u2212 log p(D2|\u03b8)\n\n= \u2212 log p(Y1|X1, \u03b8)\u2212 log p(Y2|X2, \u03b8)\n\nDuring continual learning when D1 and D2 are learned\nsequentially, we optimize L1(\u03b81) = \u2212 log p(D1|\u03b81) and\nL2(\u03b82) = \u2212 log p(D2|\u03b82) separately, which leads to a sub-\noptimal solution for L(\u03b8). When the distance between the\nconditional distributions Y1|X1 and Y2|X2 is small, it is\nmore likely that the optimal parameters \u03b81 for L1 coincides\nwith the optimal parameters \u03b82 for L2 and hence the joint\nparameters \u03b8 that optimize the batch training model.\nAnalysis. In this section we demonstrate the empirical evi-\ndence for the earlier hypothesis. Note that in all of the fol-\nlowing analyses, the inputXt is defined to be a visual object\ncategory.\n\nDistribution Distance Metric. We use the first Wasser-\nstein distance metric (EMD) to quantify the distance be-\ntween two output distributions. EMD was introduced by\nRubner et al. [31] to measure the structural similarity be-\ntween distributions. In contrast to other statistical measure-\nments like KL divergence or Chi-squared statistics, EMD\ncan be used to measure the similarity between both continu-\nous and discrete distributions with different supports. Given\ndistributions u and v, we define\n\nd(u, v) = inf\n\u03c0\u2208\u0393(u,v)\n\n\u222b\nR\u00d7R\n|x\u2212 y|d\u03c0(x, y)\n\nand express the distance between two learning exposures t\nand t\u2032 as\n\nD(t, t\n\u2032\n) =\n\n1\n\n|S|\n\n\u222b\ns\u2208S\n\nd(ut, ut\u2032)ds (2)\n\n0.5 0.6 0.7 0.8 0.9 1.0\nOutput Distribution Distance\n\n0.8\n\n0.7\n\n0.6\n\nB\nW\n\nT\n\nBWT Num Exemplars\n\n0\n\n50\n\n100\n\nN\num\n\n E\nxe\n\nm\npl\n\nar\nsAnalysis of Output Distribution Shift on CIFAR-100\n\nFigure 4. The relationship between the output distribution shift\nacross learning exposures, the number of exemplars per class and\nBWT on CIFAR-100. Experiments are run for 0, 20, 40, 60, 80,\n100 exemplars/class. Larger exemplar set size leads to smaller\noutput distribution distance and higher BWT.\nwhere ut and ut\u2032 are the output distributions at exposures\nt and t\u2032 respectively and S is the support set of ut and ut\u2032 .\nWe now analyze the output distribution shift for different\ncontinual learning tasks.\n\n3D Shape Reconstruction. In this setting, the output\nY SDFt represents the ground truth SDF values for the support\nset S consisting of 3D coordinates. We first select 1000 3D\npoints uniformly in a unit grid of resolution 1283. For each\nshape class, we randomly sample 1000 objects. Each 3D\npoint qi defines a distribution of SDF values within a shape\nclass P (t)qi = P(Y SDFt |qi, Xt). From Eq. 2, the final output\ndistribution distance between each shape class is\n\nD(t, t\n\u2032\n) =\n\n1\n\nNq\n\nNq\u2211\ni=1\n\nd(P\n(t)\nqi\n, P\n\n(t\u2032)\nqi\n\n)\n\nwhere Nq is the number of 3D points. We present the re-\nsults for both coordinate representations (OC and 3-DOF\nVC) described in Sec. 4.1.\n\n2.5D Depth Prediction and 2D Silhouette Prediction. In\nthis setting, Y pixt represents the value of each pixel of the\ninput Xt (depth value and binary value for depth and sihou-\nette pred. respectively). The support set S is the set of 2D\npixel coordinates. Each pixel pi then defines a distribution\nof pixel values within a class P (t)pi = P(Y\n\npix\nt |pi, Xt). The\n\noutput distribution distance between each class is\n\nD(t, t\n\u2032\n) =\n\n1\n\nNp\n\nNp\u2211\ni=1\n\nd(P\n(t)\npi\n, P\n\n(t\u2032)\npi\n\n)\n\nwhere Np is the number of pixels. For depth prediction,\nwe first center crop the input images. For each class we\nrandomly sample 800 objects and for each image sample\n1000 pixels uniformly.\n\nClassification. The output Yt represents the ground truth\nclass labels of the inputXt. The output distribution for each\nclass is then P (t) = P(Yt = c|Xt). Different from the\nreconstruction tasks, the output distribution between each\nlearning exposure does not share a support set. We assume\nthat the class labels are sequentially incremented integers\nfor each new class in the sequence. The final output distri-\nbution distance is computed as D(t, t\u2032) = d(P (t), P (t\n\n\u2032))\nSmall output distribution shift is associated with im-\nproved knowledge transfer. We first compute the output\n\n\n\n0 2 4 6 8 10 12\nLearning Exposures\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\nA\ncc\n\nur\nac\n\ny\n\nClassification Performance on ShapeNet13 with Single Exposure\nProxy Rep Learning\nImageNet Pretrained\n\nClassifier w Exemplr\nGDumb\n\nBatch\n\nFigure 5. Performance of proxy representation learning task\n(Proxy Rep Learning) in continual learning of classification on\nShapeNet13 with RGB input. Given a limited exemplar bud-\nget, 3D shape reconstruction proxy representation learning outper-\nforms ImageNet pretrained features and classification baselines.\n\ndistribution distance as described above for each task and\ncompare it with the resulting BWT and FWT. To verify the\neffectiveness of the proposed method and to ensure fairness\nwe continually train each task using the finetuning strategy\non ShapeNet13 from 2D RGB input images with 1 class per\nlearning exposure and report the average output distribution\ndistance and the BWT and FWT metrics. Tbl. 2 shows that\nour hypothesis holds as the small output distribution dis-\ntance is associated with higher BWT and FWT. It can also\nbe seen that classification has a significantly larger output\ndistribution distance compared to reconstruction tasks, and\nexhibits significantly reduced FWT and BWT.\nEffect of exemplar set size on output distribution shift\nWe apply this analysis technique to gain insight into the\neffectiveness of replay methods commonly used to avoid\ncatastrophic forgetting in classification. We design our ex-\nperiment on CIFAR-100 with 1 class per learning exposure.\nWe employ randomly initialized ResNet34 and vary the ex-\nemplar set size from 0 to 100 exemplars/class. Fig. 4 illus-\ntrates that larger exemplar set size associates with smaller\nconditional output distribution shift which results in im-\nprovement in BWT.\n\n7. Proxy Task for Continual Classification\nThe robustness of representation learning and the abil-\n\nity to transfer knowledge between learning exposures in\nsingle-view 3D shape reconstruction begs the question of\nwhether it could be used as a proxy task to improve class-\nIL classification [38]10. We test that hypothesis here via\na simple approach: We train a 3D reconstruction model,\nSDFNet VC on RGB images continually as in Sec. 4.1, and\nat inference time we extract the feature from its image en-\ncoder with a forward pass. We maintain an exemplar set of\n20 images/class with class labels randomly sampled from\nthe training dataset We do not use the labels for training.\nInstead, we use the extracted representation to do nearest-\nclass-mean (NCM) classification with the exemplars at test-\ning time. Specifically, the mean feature of each class is first\ncomputed from the exemplar set. Then test samples are as-\n\n10In this setting, the learning model is required to discriminate all\nlearned classes.\n\nsigned the label of the closest mean feature via cosine dis-\ntance. We decide to utilize NCM as a classifier instead of\ntraining a fully-connected layer with cross-entropy loss, due\nto the fact that the exemplar set size is small (< 1% of the\ntraining data) and it has been shown that linear classifier\ntrained with CE loss tends to overfit significantly when the\ndataset is imbalanced [6, 46].\n\nWe conduct experiments with ShapeNet13 with one class\nper exposure. We first show that the feature representa-\ntion learned by the single-view 3D shape reconstruction\ntask is discriminative despite not having access to ground\ntruth labels during training. We compare the performance\nof the proxy classifier against an ImageNet pretrained fea-\nture representation model. Specifically, we extract the fea-\nture from the ImageNet pretrained ResNet18 via a forward\npass and use NCM as the classifier with the same exemplar\nset size as the proxy classifier. Fig. 5 shows evidence that\nshape features are more beneficial for continual classifica-\ntion than the rich discriminative feature representation from\nImageNet. We further compare the proxy classifier against\ntwo classification baselines: GDumb [26] and a standard\nclassifier trained continually with cross entropy loss and the\nsame exemplar set, denoted as Classifier with Exemplars.\nFig. 5 shows that the 3D shape proxy classifier outperforms\nthe GDumb and Classifier with Exemplars on ShapeNet13.\nThis demonstrates that a significant amount of discrimina-\ntive information is encoded in the continual shape represen-\ntation and suggests that it may be beneficial to explore other\nproxy tasks as a means to improve CL classification.\n\n8. Conclusion\nWe have identified a set of CL reconstruction tasks, in-\n\ncluding 3D shape reconstruction, 2.5D sketch estimation,\nand 2D image reconstruction, that do not exhibit catas-\ntrophic forgetting. Hence, the answer to the question we\npose in our title \u201cDoes continual learning = catastrophic for-\ngetting?\u201d is in general \u201cNo,\u201d despite the central role of for-\ngetting in prior research on continual classification. We fur-\nther show that reconstruction tasks benefit from continuous\nrepresentation training and exhibit positive forward transfer.\nIn addition, the continual version of the challenging single-\nview 3D shape reconstruction task demonstrates improve-\nments in performance over time on both seen and novel cat-\negories. We are the first to investigate the generalization\nability of single-view 3D shape reconstruction models in\nthe context of CL.\n\nWe provide a novel algorithm-agnostic means to charac-\nterize the knowledge transfer performance of CL tasks via\noutput distribution shift analysis. We show that reduction\nin shift is associated with increased knowledge transfer. We\nlink reconstruction and classification by showing that fea-\nture representations from reconstruction can be effective for\nCL of classification under a limited exemplar budget. Our\n\n\n\nfindings point to a need to enlarge the space of CL tasks and\ndevelop algorithm-agnostic measures of CL performance.\n\n9. Acknowledgement\nWe would like to thank Miao Liu and Meera Hahn\n\nfor the helpful discussion. This work was supported by\nNIH R01-MH114999 and NSF Award 1936970. This pa-\nper is dedicated to the memory of Chengming (Julian)\nGu.\n\nReferences\n[1] Plop: Learning without forgetting for continual semantic seg-\n\nmentation. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2021. 2\n\n[2] David Abel, Dilip Arumugam, Lucas Lehnert, and Michael\nLittman. State abstractions for lifelong reinforcement learn-\ning. In International Conference on Machine Learning, pages\n10\u201319, 2018. 2\n\n[3] Rahaf Aljundi, Klaas Kelchtermans, and Tinne Tuytelaars.\nTask-free continual learning. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), June 2019. 2\n\n[4] Blender Online Community. Blender - a 3D modelling and\nrendering package. Blender Foundation, Blender Institute,\nAmsterdam. 12\n\n[5] Zhipeng Cai, Ozan Sener, and Vladlen Koltun. Online con-\ntinual learning with natural distribution shifts: An empiri-\ncal study with visual data. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV), pages\n8281\u20138290, October 2021. 2\n\n[6] Francisco M Castro, Manuel J Marin-Jimenez, Nicolas Guil,\nCordelia Schmid, and Karteek Alahari. End-to-end incremen-\ntal learning. In Proceedings of the European Conference on\nComputer Vision (ECCV), pages 233\u2013248, 2018. 8\n\n[7] Fabio Cermelli, Massimiliano Mancini, Samuel Rota Bulo,\nElisa Ricci, and Barbara Caputo. Modeling the background\nfor incremental learning in semantic segmentation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), June 2020. 2\n\n[8] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat\nHanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis\nSavva, Shuran Song, Hao Su, et al. Shapenet: An information-\nrich 3d model repository. arXiv preprint arXiv:1512.03012,\n2015. 4\n\n[9] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin\nChen, and Silvio Savarese. 3d-r2n2: A unified approach for\nsingle and multi-view 3d object reconstruction. In European\nconference on computer vision, pages 628\u2013644. Springer,\n2016. 12\n\n[10] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah\nParisot, Xu Jia, Ales Leonardis, Gregory Slabaugh, and Tinne\nTuytelaars. A continual learning survey: Defying forgetting in\nclassification tasks. arXiv preprint arXiv:1909.08383, 2019.\n1\n\n[11] Mohamed Elhoseiny, Francesca Babiloni, Rahaf Aljundi,\nMarcus Rohrbach, Manohar Paluri, and Tinne Tuytelaars. Ex-\nploring the challenges towards lifelong fact learning. In Asian\n\nConference on Computer Vision, pages 66\u201384. Springer, 2018.\n2\n\n[12] Xisen Jin, Junyi Du, and Xiang Ren. Gradient based mem-\nory editing for task-free continual learning. arXiv preprint\narXiv:2006.15294, 2020. 1\n\n[13] Christos Kaplanis, Murray Shanahan, and Claudia Clopath.\nContinual reinforcement learning with complex synapses.\narXiv preprint arXiv:1802.07239, 2018. 2\n\n[14] Jeremias Knoblauch, Hisham Husain, and Tom Diethe. Opti-\nmal continual learning has perfect memory and is np-hard. In\nInternational Conference on Machine Learning, pages 5327\u2013\n5337. PMLR, 2020. 2\n\n[15] Tobias Koch, Lukas Liebel, Friedrich Fraundorfer, and\nMarco Korner. Evaluation of cnn-based single-image depth\nestimation methods. In Proceedings of the European Con-\nference on Computer Vision (ECCV) Workshops, pages 0\u20130,\n2018. 5, 13\n\n[16] Alex Krizhevsky, Vinod Nair, and Geoffrey\nHinton. The CIFAR-100 Dataset. online:\nhttps://www.cs.toronto.edu/ kriz/cifar.html, 2014. 5\n\n[17] Timothe\u0301e Lesort, Massimo Caccia, and Irina Rish. Under-\nstanding continual learning settings with data distribution drift\nanalysis. arXiv preprint arXiv:2104.01678, 2021. 2\n\n[18] Timothe\u0301e Lesort, Hugo Caselles-Dupre\u0301, Michael Garcia-\nOrtiz, Andrei Stoian, and David Filliat. Generative models\nfrom the perspective of continual learning. In 2019 Interna-\ntional Joint Conference on Neural Networks (IJCNN), pages\n1\u20138. IEEE, 2019. 2\n\n[19] Zhizhong Li and Derek Hoiem. Learning without forget-\nting. IEEE transactions on pattern analysis and machine in-\ntelligence, 40(12):2935\u20132947, 2017. 3\n\n[20] Xialei Liu, Hao Yang, Avinash Ravichandran, Rahul\nBhotika, and Stefano Soatto. Multi-task incremental learning\nfor object detection, 2020. 2\n\n[21] Yaoyao Liu, Yuting Su, An-An Liu, Bernt Schiele, and\nQianru Sun. Mnemonics training: Multi-class incremental\nlearning without forgetting. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 12245\u201312254, 2020. 1\n\n[22] David Lopez-Paz and Marc\u2019Aurelio Ranzato. Gradient\nepisodic memory for continual learning. In Advances in neu-\nral information processing systems, pages 6467\u20136476, 2017.\n1, 3\n\n[23] Andrea Maracani, Umberto Michieli, Marco Toldo, and\nPietro Zanuttigh. Recall: Replay-based continual learning\nin semantic segmentation. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV), pages\n7026\u20137035, October 2021. 2\n\n[24] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-\nbastian Nowozin, and Andreas Geiger. Occupancy networks:\nLearning 3d reconstruction in function space. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 4460\u20134470, 2019. 4, 12, 13\n\n[25] Umberto Michieli and Pietro Zanuttigh. Incremental learn-\ning techniques for semantic segmentation. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision\n(ICCV) Workshops, Oct 2019. 2\n\n\n\n[26] Ameya Prabhu, Philip HS Torr, and Puneet K Dokania.\nGdumb: A simple approach that questions our progress in\ncontinual learning. In ECCV, 2020. 1, 6, 8, 12, 14\n\n[27] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.\nPointnet: Deep learning on point sets for 3d classification and\nsegmentation. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, pages 652\u2013660, 2017. 12\n\n[28] Michael Ramamonjisoa and Vincent Lepetit. Sharpnet: Fast\nand accurate recovery of occluding contours in monocular\ndepth estimation. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision (ICCV) Workshops,\nOct 2019. 5\n\n[29] Dushyant Rao, Francesco Visin, Andrei Rusu, Razvan Pas-\ncanu, Yee Whye Teh, and Raia Hadsell. Continual unsuper-\nvised representation learning. In Advances in Neural Informa-\ntion Processing Systems, pages 7647\u20137657, 2019. 3\n\n[30] Anthony Robins. Catastrophic Forgetting, Rehearsal and\nPseudorehearsal. Connection Science, 7(2):123\u2013146, 1995.\n1\n\n[31] Yossi Rubner, Carlo Tomasi, and Leonidas J Guibas. The\nearth mover\u2019s distance as a metric for image retrieval. Inter-\nnational journal of computer vision, 40(2):99\u2013121, 2000. 7\n\n[32] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, et al. Imagenet large scale\nvisual recognition challenge. International journal of com-\nputer vision, 115(3):211\u2013252, 2015. 5\n\n[33] Daeyun Shin, Charless C Fowlkes, and Derek Hoiem. Pixels,\nvoxels, and views: A study of shape representations for single\nview 3d object shape prediction. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages\n3061\u20133069, 2018. 2\n\n[34] Konstantin Shmelkov, Cordelia Schmid, and Karteek Ala-\nhari. Incremental learning of object detectors without catas-\ntrophic forgetting. In Proceedings of the IEEE International\nConference on Computer Vision, pages 3400\u20133409, 2017. 2\n\n[35] Stefan Stojanov, Samarth Mishra, Ngoc Anh Thai, Nikhil\nDhanda, Ahmad Humayun, Chen Yu, Linda B Smith, and\nJames M Rehg. Incremental object learning from contiguous\nviews. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 8777\u20138786, 2019.\n(Oral, Best Paper Finalist). 3, 5\n\n[36] Maxim Tatarchenko, Stephan R Richter, Rene\u0301 Ranftl,\nZhuwen Li, Vladlen Koltun, and Thomas Brox. What do\nsingle-view 3d reconstruction networks learn? In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 3405\u20133414, 2019. 4, 13\n\n[37] Anh Thai, Stefan Stojanov, Vijay Upadhya, and James M\nRehg. 3d reconstruction of novel object shapes from single\nimages. arXiv preprint arXiv:2006.07752, 2020. 2, 4, 5, 6,\n12, 13\n\n[38] Gido M Van de Ven and Andreas S Tolias. Three scenar-\nios for continual learning. arXiv preprint arXiv:1904.07734,\n2019. 3, 8\n\n[39] Eli Verwimp, Matthias De Lange, and Tinne Tuytelaars. Re-\nhearsal revealed: The limits and merits of revisiting samples in\ncontinual learning. In Proceedings of the IEEE/CVF Interna-\n\ntional Conference on Computer Vision (ICCV), pages 9385\u2013\n9394, October 2021. 2\n\n[40] Jianren Wang, Xin Wang, Yue Shang-Guan, and Abhinav\nGupta. Wanderlust: Online continual object detection in the\nreal world. In Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV), pages 10829\u201310838,\nOctober 2021. 2\n\n[41] Shuzhe Wang, Zakaria Laskar, Iaroslav Melekhov, Xiaotian\nLi, and Juho Kannala. Continual learning for image-based\ncamera localization. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 3252\u20133262,\n2021. 2\n\n[42] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P\nSimoncelli. Image quality assessment: from error visibility to\nstructural similarity. IEEE transactions on image processing,\n13(4):600\u2013612, 2004. 5\n\n[43] Ziyun Wang, Volkan Isler, and Daniel D Lee. Surface hof:\nSurface reconstruction from a single image using higher order\nfunction networks. In 2020 IEEE International Conference on\nImage Processing (ICIP), pages 2666\u20132670. IEEE, 2020. 5\n\n[44] Chenshen Wu, Luis Herranz, Xialei Liu, Joost van de Wei-\njer, Bogdan Raducanu, et al. Memory replay gans: Learning\nto generate new categories without forgetting. In Advances\nin Neural Information Processing Systems, pages 5962\u20135972,\n2018. 2\n\n[45] Jiajun Wu, Yifan Wang, Tianfan Xue, Xingyuan Sun, Bill\nFreeman, and Josh Tenenbaum. Marrnet: 3d shape recon-\nstruction via 2.5 d sketches. In Advances in neural informa-\ntion processing systems, pages 540\u2013550, 2017. 5, 13, 14\n\n[46] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye,\nZicheng Liu, Yandong Guo, and Yun Fu. Large scale in-\ncremental learning. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 374\u2013382,\n2019. 8\n\n[47] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva,\nand Antonio Torralba. Sun database: Large-scale scene recog-\nnition from abbey to zoo. In 2010 IEEE Computer Soci-\nety Conference on Computer Vision and Pattern Recognition,\npages 3485\u20133492. IEEE, 2010. 12\n\n[48] Ju Xu and Zhanxing Zhu. Reinforced continual learning. In\nAdvances in Neural Information Processing Systems, pages\n899\u2013908, 2018. 2\n\n[49] Qiangeng Xu, Weiyue Wang, Duygu Ceylan, Radomir\nMech, and Ulrich Neumann. Disn: Deep implicit surface net-\nwork for high-quality single-view 3d reconstruction. 2019. 4\n\n[50] Zike Yan, Yuxin Tian, Xuesong Shi, Ping Guo, Peng Wang,\nand Hongbin Zha. Continual neural mapping: Learning an\nimplicit scene representation from sequential observations. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), pages 15782\u201315792, October 2021.\n2\n\n[51] Lu Yu, Bartlomiej Twardowski, Xialei Liu, Luis Herranz,\nKai Wang, Yongmei Cheng, Shangling Jui, and Joost van de\nWeijer. Semantic drift compensation for class-incremental\nlearning. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), June 2020.\n3\n\n\n\n[52] Junting Zhang, Jie Zhang, Shalini Ghosh, Dawei Li, Ser-\nafettin Tasci, Larry Heck, Heming Zhang, and C-C Jay Kuo.\nClass-incremental learning via deep model consolidation. In\nThe IEEE Winter Conference on Applications of Computer Vi-\nsion, pages 1131\u20131140, 2020. 1\n\n[53] Xiuming Zhang, Zhoutong Zhang, Chengkai Zhang,\nJoshua B Tenenbaum, William T Freeman, and Jiajun Wu.\nLearning to Reconstruct Shapes from Unseen Classes. In Ad-\nvances in Neural Information Processing Systems (NeurIPS),\n2018. 2, 6\n\n\n\nThis supplementary material document is structured as\nfollows: In Sec. A we describe the training data in more de-\ntail; In Sec. B we provide details on the CL algorithms used\nin the paper, their training implementation details and eval-\nuation metrics; In Section C we further explain the repeated\nexposures setting.\n\nA. Datasets\nA.1. ShapeNetCore.v2\n\nDatasets: ShapeNetCore.v2 consists of 55 categories with\n52K CAD models. This is the current largest 3D shape\ndataset with category labels. Many prior works in 3D\nshape reconstruction [9, 24] utilized a subset of 13 largest\ncategories\u2014ShapeNet13, which consists of approximately\n40K 3D instances. Tbl. 3 lists the 13 categories and the\nnumber of samples in each category. For ShapeNet13, we\nuse the standard train/val/test split from prior shape recon-\nstruction works [9, 24]. We sample 100 objects/category\nfrom the test split for evaluation in the repeated expo-\nsures case. For the remaining 42 classes in ShapeNet-\nCore.v2, we split randomly with proportion 0.7/0.1/0.2 for\ntrain/val/test splits. In the single exposure case on all\nclasses of ShapeNetCore.v2, we randomly sample 30 ob-\njects/category for testing. For evaluating novel category\ngeneralization ability, we sample 50 objects from the 42\nclasses. The license of ShapeNetCore.v2 is specified in\nhttps://shapenet.org/terms.\nRendering: We render 25 views of RGB images, ground\ntruth silhouette, depth and surface normal maps with resolu-\ntion 256\u00d7256 for each object. Following [37], we generate\ndata using Cycles ray-tracing engine in Blender [4] with 3\ndegree-of-freedom, varying camera azimuth \u03b8 \u2208 [0, 360\u25e6],\nelevation \u03c6 \u2208 [\u221250\u25e6, 50\u25e6] and tilt. For experiments with\nRGB images as inputs, we render with varying light, specu-\nlar surface reflectance and random backgrounds from SUN\nScenes [47].\nSDF Point Sampling Strategy: For 3D shape reconstruc-\ntion, training 3D points are sampled more densely close to\nthe surface of the mesh. Following [37], we sample half\nof the training points within a distance of 0.03 to the sur-\nface, 30% with distance in the range [0.03, 0.1] and 20% in\nthe range [0.1, 1.1]. To train and evaluate OccNet, we ob-\ntain mesh occupancy values by binary masking 1{sdf \u2264 i}\nwhere i is the isosurface value.\n\nA.2. CIFAR-100\n\nThis is a standard image dataset consisting of 100 cate-\ngories with 500 training and 100 testing samples for each\ncategory. Each image is of resolution 32\u00d732. In our exper-\niment for classification baselines with repeated exposures,\n60 categories are chosen randomly from 100 categories,\nwhich we denote as CIFAR-60.\n\nID Name Num samples\n02691156 airplane 4045\n02828884 bench 1813\n02933112 cabinet 1571\n02958343 car 3532\n03001627 chair 6778\n03211117 display 1093\n03636649 lamp 2318\n03691459 loudspeaker 1597\n04090263 rifle 2373\n04256520 sofa 3173\n04379243 table 8436\n04401088 telephone 1089\n04530566 watercraft 1939\n\nTotal 39,757\n\nTable 3. Statistics of ShapeNet13.\n\nB. Description of Algorithms\nB.1. Single Object 3D Shape Reconstruction\n\n(Secs. 4,5)\n\nArchitecture: We adapt SDFNet [37] and OccNet [24] with\nResNet-18 encoder for continual training with 2D and 2.5D\ninputs and SDFNet with PointNet [27] encoder for 3D in-\nput. Specifically, the architecture consists of an encoder ini-\ntialized with random weights and a point module which are\nmultiple blocks of fully-connected layers with ReLU activa-\ntion. Conditional Batch Normalization is used as applying\nan affine transformation on the output of the point module,\nconditioned on the feature vector produced by the encoder.\n\nGDumb For CL 3D Shape. We employ SDFNet with\nResNet-18 encoder as the backbone architecture and fol-\nlow the training procedure of GDumb for classification\ntask [26]. Specifically, we randomly select an exemplar set\nof size K = 1000 (\u2248 3.7% of the training data), equally\ndivided for all the seen categories at each learning expo-\nsure. We initialize the learning model randomly to train\nfrom scratch on the selected exemplar set at each learning\nexposure.\n\nGSmart. Different from GDumb for CL 3D shape, we\ncontinuously update the representation at each learning ex-\nposure. Please see Algs. 1,2,3 for the pseudo code of CL\nalgorithms evaluated in Sec. 5 in the main text.\nLoss function: SDFNet uses L1 loss as the loss function,\nwith high weights for points close to the surface. Specifi-\ncally,\n\nL(s, s\u0302) =\n\n{\n|s\u2212 s\u0302|, if |s| > 0.01\n4|s\u2212 s\u0302|, otherwise\n\nwhere s is the ground truth SDF value and s\u0302 is the predicted\nSDF value.\n\nhttps://shapenet.org/terms\n\n\nAlgorithm 1: GDumb for CL 3D Shape\nInput: Batch training procedure\n\nSDFNet(\u03b8,Dtrain,Dval) that returns the\ntrained parameters \u03b8 and the performance of\nthe trained model on Dval\n\nData: (RGB image, 3D coordinates, SDF values)\npair datasets Dtrain = \u222aTi=1D\n\ntrain\ni ,\n\nDval = \u222aTi=1D\nval\ni\n\nDefine: ` : weighted L1 loss\n1 init\n2 Exemplar set: C = {}\n3 foreach learning exposure t in 1, 2, . . . , T do\n4 \u03b8 \u2190 RANDOM INIT(\u03b8)\n5 C \u2190 SELECT RANDOM(C \u222a Dtraint )\n6 \u03b8t, acct \u2190 SDFNet(\u03b8, C,Dvalt )\n7 end\n\nResult: (acc1, acc2, . . . accT )\n\nAlgorithm 2: GSmart\nInput: Batch training procedure\n\nSDFNet(\u03b8,Dtrain,Dval) that returns the\ntrained parameters \u03b8 and the performance of\nthe trained model on Dval\n\nData: (RGB image, 3D coordinates, SDF values)\npair datasets Dtrain = \u222aTi=1D\n\ntrain\ni ,\n\nDval = \u222aTi=1D\nval\ni\n\nDefine: ` : weighted L1 loss\n1 init\n2 Exemplar set: C = {}\n3 foreach learning exposure t in 1, 2, . . . , T do\n4 \u03b8 \u2190 \u03b8t\u22121\n5 C \u2190 SELECT RANDOM(C \u222a Dtraint )\n6 \u03b8t, acct \u2190 SDFNet(\u03b8, C,Dvalt )\n7 end\n\nResult: (acc1, acc2, . . . accT )\n\nOccNet uses Binary Cross Entropy (BCE) loss on each\ninput 3D point. Specifically,\n\nL(p, p\u0302) = \u2212p log p\u0302\u2212 (1\u2212 p) log(1\u2212 p\u0302)\n\nwhere p \u2208 {0, 1} is the ground truth binary value and p\u0302\nis the predicted probability of whether a point is inside or\noutside the mesh.\nMesh generation: We use MISE, an algorithm that hier-\narchically extracts the mesh isosurface introduced by [24]\nto generate the predicted mesh. Instead of generating the\nSDF/occupancy values for all the points uniformly sampled\nin the cube, MISE starts from a lower resolution and hier-\narchically determines the voxels that contain the mesh to\nsubdivide until the desired resolution is reached. We adapt\n\nAlgorithm 3: C-SDFNet\nInput: Batch training procedure\n\nSDFNet(\u03b8,Dtrain,Dval) that returns the\ntrained parameters \u03b8 and the performance of\nthe trained model on Dval\n\nData: (RGB image, 3D coordinates, SDF values)\npair datasets Dtrain = \u222aTi=1D\n\ntrain\ni ,\n\nDval = \u222aTi=1D\nval\ni\n\nDefine: ` : weighted L1 loss\n1 foreach learning exposure t in 1, 2, . . . , T do\n2 \u03b8 \u2190 \u03b8t\u22121\n3 \u03b8t, acct \u2190 SDFNet(\u03b8,Dtraint ,Dvalt )\n4 end\n\nResult: (acc1, acc2, . . . accT )\n\nMISE to work on both SDF and occupancy values.\nMetric: Following [36, 37], we use F-Score at 1% as our\nmain evaluation metric. We first sample 300K and 100K\npoints respectively on the surface of the predicted mesh (S1)\nand ground truth mesh (S2). The metric is computed as the\nfollowing\n\nFS@1 =\n2 \u00b7 prec@1 \u00b7 rec@1\nprec@1 + rec@1\n\nwhere prec@1 is the precision at 1%, which measures the\nportion of points from S1 that lie within a threshold 0.01\nto the points from S2 (in the case where the mesh is nor-\nmalized to fit in a unit cube) and rec@1 is the recall at 1%,\nwhich measures the portion of points from S2 that lie within\na threshold 0.01 to the points from S1.\n\nB.2. Single-view 2.5D Sketches Prediction (Sec. 4)\n\nArchitecture: We adapt the 2.5D sketch estimation from\nMarrNet [45] for continual training. The backbone architec-\nture for MarrNet is a U-ResNet18 with the ResNet18 image\nencoder initialized with ILSVRC-2014 pre-trained weights.\nLoss functions: We use MSE as the loss function for depth\nand normals prediction. Specifically,\n\nMSE(I, I\u0302) =\n1\n\nK \u00d7K\n\nK\u2211\ni,j\n\n\u2016I(i, j)\u2212 I\u0302(i, j)\u201622\n\nwhere I and I\u0302 are the ground truth and predicted images\nrespectively.\nMetrics: For depth prediction, we report threshold accu-\nracy: percentage of yi such that\n\nmax\n\n(\nyi\ny?i\n,\ny?i\nyi\n\n)\n< \u03c3\n\nwhere yi and yi? are the predicted and ground truth depth\nvalues at pixel i and \u03c3 is the threshold. In our evaluation,\nwe use \u03c3 = 1.25 as in [15].\n\n\n\nFor normals, we report cosine distance threshold as the\nmain metric. We first convert the RGB values of the normal\nmap into 3D vectors\n\nn = 2\n\n\uf8eb\n\uf8edc\u2212\n\n\uf8ee\n\uf8f00.50.5\n0.5\n\n\uf8f9\n\uf8fb\n\uf8f6\n\uf8f8\n\nwhere n and c =\n\n\uf8ee\n\uf8f0rg\n\nb\n\n\uf8f9\n\uf8fb are the normal and color vectors re-\n\nspectively. Cosine distance threshold accuracy is then com-\nputed as \u2329\n\nn\n\u2016n\u20162\n\n,\nn?\n\n\u2016n?2\u2016\n\n\u232a\n> \u03c3\n\nwhere n and n? are predicted and ground truth normals. We\nset \u03c3 = 0.9.\n\nB.3. 2D Reconstruction (Sec. 4)\n\nB.3.1 Silhouette Prediction\n\nWe use MarrNet [45] as the backbone architecture for\ncontinual training with BCE loss function. We report\nIntersection-over-Union for silhouette prediction as the\nmetric. Specifically,\n\nIoU(I, I\u0302) =\n|I \u2229 I\u0302|\n|I \u222a I\u0302|\n\nB.3.2 Image Autoencoding\n\nArchitecture: We implement a shallow network with 4\nconv. layers, each followed by a max pooling layer which\nwe termed ConvAutoEncoder. Each conv. layer has 16\nchannels and the dimension of the bottle-neck feature vector\nis 16\u00d7 2\u00d7 2. The network is randomly initialized.\nLoss function: We train ConvAutoEncoder with MSE loss\nfor each pixel, defined as\n\nL(I, I\u0302) =\n1\n\nK \u00d7K \u00d7 3\n\n3\u2211\nc=1\n\nK\u2211\ni,j\n\n\u2016I(i, j, c)\u2212 I\u0302(i, j, c)\u201622\n\nwhere K is the size of the input image and c = {1, 2, 3} is\nthe 3 input channels (red, green, blue).\nMetric: We use SSIM scaled to range [0, 1] as the main\nevaluation metric for the image autoencoding experiment.\nSpecifically, given two image windows x and y of the same\nsize N \u00d7N the original SSIM metric is computed as\n\nSSIM(x, y) =\n(2\u00b5x\u00b5y + c1)(2\u03c3xy + c2)\n\n(\u00b52x + \u00b5\n2\ny + c1)(\u03c3\n\n2\nx + \u03c3\n\n2\ny + c2)\n\nwith \u00b5x, \u00b5y be the averages of x and y respectively,\n\u03c32x, \u03c3\n\n2\ny, \u03c3xy are the variances of x, y and the covariance of x\n\nand y respectively, c1, c2 are constants to avoid dividing by\n0 in the denominator.\n\nB.4. Classification Baselines (Sec. 7)\n\nGDumb [26] is an algorithm that randomly selects exem-\nplars and performs training on the exemplar set only. At\neach learning exposure, the model is trained from scratch\non the exemplar set, in which each category is represented\nwith the same number of samples. GDumb utilizes the stan-\ndard cross-entropy loss and classifies using the network out-\nputs. We used our PyTorch implementation of GDumb with\nResNet18 initialized randomly as the feature extractor.\nClassifier with Exemplars is a simple baseline where we\ntrain a standard classifier with cross-entropy loss contin-\nually. At each learning exposure, the learning model is\ntrained on the current training data combined with the ran-\ndomly selected exemplar set without any further heuristics.\nSimilar to GDumb, we use randomly initialized ResNet18\nas the feature extractor.\nImageNet Pretrained is the baseline we use to highlight\nthat the feature space learned by CL single-view 3D shape\nmodel from RGB image without ground truth label is dis-\ncriminative. For each new class, we randomly select the\nexemplar set from the training data. At test time, we first\nextract the feature representation from the ILSVRC-2014\npretrained ResNet18 for each test sample. We then perform\nNCM to predict the label using the exemplar set.\n\nC. Further Explanation for Repeated Expo-\nsures Setting\n\nIn the repeated exposure setting, each class occurs a fixed\nnumber of times (e.g. 10 repetitions) in random order. For\nexample, in the case of 50 classes repeated 10 times, we\nwould first generate 500 learning exposures, and then per-\nform a random permutation to obtain the order seen by the\nlearner. As a result, classes repeat in complex and highly-\nvariable patterns. Note that even though classes repeat, each\nlearning exposure still contains only a single class (or a\nsmall number), thereby preserving the domain shift between\nexposures that makes CL challenging.\n\n\n"
"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA new type of 4D Hybrid Chaos Systems \n\n \n\nReza Parvaz \n\nDepartment of Mathematics, University of Mohaghegh Ardabili, 56199-11367 Ardabil, Iran. \n\nrparvaz@uma.ac.ir \n\n \n\nAbstract \n\nIn this paper a new type of chaotic system based on sin and logistic systems is introduced. Also the \n\nbehavior of this new system is studied by using various tests. The results of these tests indicate the \n\nappropriate behavior for the proposed new system. \n\n \n\nKey words: Chaos system, Logistic map, Hybrid system  \n\n1-Introduction \n\nIn recent years, the use of chaotic functions has been considered due to its structure. These types of \n\nsystems are used in image encryption. Also, due to the weaknesses that classical functions have, the use \n\nof combination method to improve this group of functions is considered. In this article, we present a four-\n\ndimensional chaos system that can be used to create different four-dimensional chaos systems. This \n\nsystem can be used in various algorithms, including the use of image encryption algorithm. It should be \n\nnoted that all calculations in this article have been done using MATLAB software \n\n \n\n2.   4D Hybrid Chaos Systems \n\n2.1 Structure of the Proposed Hybrid System \n\n     In this section, more details about the proposed new hybrid chaos systems based on Tent, Sin and \n\nLogistic maps are given. The general structure of the proposed chaos system has been given in Fig. 1. The \n\ncombination parts in the proposed system based on Tent, Sin and Logistic maps are shown in Fig. 2. The \n\nmathematics formulae for each of the parts can be written as follows relations. \n\nFirst combination part (\ud835\udc34\ud835\udc65): \n\n\n\n\ud835\udc65\ud835\udc56+1 =\n\n{\n \n \n\n \n \ud835\udefc1\n\n\ud835\udc65\ud835\udc531\n\ud835\udc65\ud835\udc5c\ud835\udc391\n\n\ud835\udc65(\ud835\udc5f, \ud835\udc65\ud835\udc56) + \ud835\udc541\n\ud835\udc65(\ud835\udc4e1) + \u210e1\n\n\ud835\udc65 ( \n(\ud835\udefd1\n\n\ud835\udc65 \u2212 \ud835\udc5f)\ud835\udc67\ud835\udc56\n2\n\n)\ud835\udc5a\ud835\udc5c\ud835\udc51 1,                     \ud835\udc67\ud835\udc56 < 0.5,\n\n\ud835\udefc2\n\ud835\udc65\ud835\udc532\n\n\ud835\udc65\ud835\udc5c\ud835\udc392\n\ud835\udc65(\ud835\udc5f, \ud835\udc65\ud835\udc56) + \ud835\udc542\n\n\ud835\udc65(\ud835\udc4e1) + \u210e2\n\ud835\udc65 ( \n\n(\ud835\udefd2\n\ud835\udc65 \u2212 \ud835\udc5f)(1 \u2212 \ud835\udc67\ud835\udc56)\n\n2\n)\ud835\udc5a\ud835\udc5c\ud835\udc51 1,           \ud835\udc67\ud835\udc56 \u2265 0.5.\n\n \n\nSecond combination part (\ud835\udc34\ud835\udc66): \n\n\ud835\udc66\ud835\udc56+1 =\n\n{\n \n \n\n \n \ud835\udefc1\n\n\ud835\udc66\n\ud835\udc531\n\ud835\udc66\n\ud835\udc5c\ud835\udc391\n\n\ud835\udc66\n(\ud835\udc5f, \ud835\udc66\ud835\udc56) + \ud835\udc541\n\n\ud835\udc66\n(\ud835\udc4e2) + \u210e1\n\n\ud835\udc66\n( \n(\ud835\udefd1\n\n\ud835\udc66\n\u2212 \ud835\udc5f)\ud835\udf09\ud835\udc65\n2\n\n)\ud835\udc5a\ud835\udc5c\ud835\udc51 1,                     \ud835\udf09\ud835\udc65 < 0.5,\n\n\ud835\udefc2\n\ud835\udc66\n\ud835\udc532\n\ud835\udc66\n\ud835\udc5c\ud835\udc392\n\n\ud835\udc66\n(\ud835\udc5f, \ud835\udc66\ud835\udc56) + \ud835\udc542\n\n\ud835\udc66\n(\ud835\udc4e2) + \u210e2\n\n\ud835\udc66\n( \n(\ud835\udefd2\n\n\ud835\udc66\n\u2212 \ud835\udc5f)(1 \u2212 \ud835\udf09\ud835\udc65)\n\n2\n)\ud835\udc5a\ud835\udc5c\ud835\udc51 1,           \ud835\udf09\ud835\udc65 \u2265 0.5.\n\n \n\nThird combination part (\ud835\udc34\ud835\udc67): \n\n\ud835\udc67\ud835\udc56+1 =\n\n{\n \n \n\n \n \ud835\udefc1\n\n\ud835\udc67\ud835\udc531\n\ud835\udc67\ud835\udc5c\ud835\udc391\n\n\ud835\udc67(\ud835\udc5f, \ud835\udc67\ud835\udc56) + \ud835\udc541\n\ud835\udc67(\ud835\udc4e3) + \u210e1\n\n\ud835\udc67 ( \n(\ud835\udefd1\n\n\ud835\udc67 \u2212 \ud835\udc5f)\ud835\udf09\ud835\udc66\n\n2\n)\ud835\udc5a\ud835\udc5c\ud835\udc51 1,                     \ud835\udf09\ud835\udc66 < 0.5,\n\n\ud835\udefc2\n\ud835\udc67\ud835\udc532\n\n\ud835\udc67\ud835\udc5c\ud835\udc392\n\ud835\udc67(\ud835\udc5f, \ud835\udc67\ud835\udc56) + \ud835\udc542\n\n\ud835\udc67(\ud835\udc4e3) + \u210e2\n\ud835\udc67 ( \n\n(\ud835\udefd2\n\ud835\udc67 \u2212 \ud835\udc5f)(1 \u2212 \ud835\udf09\ud835\udc66)\n\n2\n)\ud835\udc5a\ud835\udc5c\ud835\udc51 1,           \ud835\udf09\ud835\udc66 \u2265 0.5.\n\n \n\nFourth combination part (\ud835\udc34\ud835\udc64): \n\n\ud835\udc64\ud835\udc56+1 =\n\n{\n \n \n\n \n \ud835\udefc1\n\n\ud835\udc64\ud835\udc531\n\ud835\udc64\ud835\udc5c\ud835\udc391\n\n\ud835\udc64(\ud835\udc5f, \ud835\udc64\ud835\udc56) + \ud835\udc541\n\ud835\udc64(\ud835\udc4e4) + \u210e1\n\n\ud835\udc64 ( \n(\ud835\udefd1\n\n\ud835\udc64 \u2212 \ud835\udc5f)\ud835\udf09\ud835\udc67\n2\n\n)  \ud835\udc5a\ud835\udc5c\ud835\udc51 1,                     \ud835\udf09\ud835\udc67 < 0.5,\n\n\ud835\udefc2\n\ud835\udc64\ud835\udc532\n\n\ud835\udc64\ud835\udc5c\ud835\udc392\n\ud835\udc64(\ud835\udc5f, \ud835\udc64\ud835\udc56) + \ud835\udc542\n\n\ud835\udc64(\ud835\udc4e4) + \u210e2\n\ud835\udc64 ( \n\n(\ud835\udefd2\n\ud835\udc64 \u2212 \ud835\udc5f)(1 \u2212 \ud835\udf09\ud835\udc67)\n\n2\n)\ud835\udc5a\ud835\udc5c\ud835\udc51 1,           \ud835\udf09\ud835\udc67 \u2265 0.5,\n\n \n\nWhere  \ud835\udc4e1: = {\ud835\udc5f, \ud835\udc65\ud835\udc56 , \ud835\udc66\ud835\udc56 , \ud835\udc67\ud835\udc56, \ud835\udc64\ud835\udc56},  \ud835\udc4e2 = {\ud835\udc5f, \ud835\udc65\ud835\udc56 , \ud835\udc65\ud835\udc56+1, \ud835\udc66\ud835\udc56 , \ud835\udc67\ud835\udc56 , \ud835\udc64\ud835\udc56},  \ud835\udc4e3 \u2254 {\ud835\udc5f, \ud835\udc65\ud835\udc56, \ud835\udc65\ud835\udc56+1, \ud835\udc66\ud835\udc56 , \ud835\udc66\ud835\udc56+1, \ud835\udc67\ud835\udc56 , \ud835\udc64\ud835\udc56}, \ud835\udc4e3 \u2254\n\n{\ud835\udc5f, \ud835\udc65\ud835\udc56 , \ud835\udc65\ud835\udc56+1, \ud835\udc66\ud835\udc56 , \ud835\udc66\ud835\udc56+1, \ud835\udc67\ud835\udc56 , \ud835\udc67\ud835\udc56+1, \ud835\udc64\ud835\udc56}, and \ud835\udf09\ud835\udf0f = \ud835\udf0f\ud835\udc56 \ud835\udc5c\ud835\udc5f \ud835\udf0f\ud835\udc56+1, for \ud835\udf0f = \ud835\udc65, \ud835\udc66, \ud835\udc67. \ud835\udc39\ud835\udf0d\n\ud835\udf0f, for \ud835\udf0f = \ud835\udc65, \ud835\udc66, \ud835\udc67, \ud835\udc64 , \ud835\udf0d = 1,2, can be \n\nconsidered as Sin or Logistic maps. \ud835\udefc\ud835\udf0d\n\ud835\udf0f, \ud835\udefd\ud835\udf0d\n\n\ud835\udf0f, for \ud835\udf0f = \ud835\udc65, \ud835\udc66, \ud835\udc67, \ud835\udc64 , \ud835\udf0d = 1,2, are arbitrary number in \ud835\udc45, and \ud835\udc54\ud835\udf0d\n\ud835\udf0f, \u210e\ud835\udf0d\n\n\ud835\udf0f \n\nare considered as arbitrary sufficiently smooth functions.  In the proposed system, the best feature of the \n\ndifferent chaos maps as Tent, Sin and Logistic maps have been improved by using Composition and \n\ntransfer operator. In the following, the basic properties of the hybrid system have been studied. \n\n \n\n  \n\n \n\n\n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n  \n\n \n\n \n\n \n\nIn the subsections in order to study hybrid system, the following cases have been considered. \n\nFigure 1. The structure of the proposed chaotic system.  \n\nFigure 2. The structure of the combination parts in the proposed system. \n\nchaotic system.  \n\n\n\nCase i:    {\ud835\udefc1\n\ud835\udc65, \ud835\udefc2\n\n\ud835\udc65 , \ud835\udefc1\n\ud835\udc66\n, \ud835\udefc2\n\n\ud835\udc66\n, \ud835\udefc1\n\n\ud835\udc67, \ud835\udefc2\n\ud835\udc67, \ud835\udefc1\n\n\ud835\udc64 , \ud835\udefc2\n\ud835\udc64} = {1,16,10,20,10,20,10,20}, {\ud835\udefd1\n\n\ud835\udc65, \ud835\udefd2\n\ud835\udc65 , \ud835\udefd1\n\n\ud835\udc66\n, \ud835\udefd2\n\n\ud835\udc66\n, \ud835\udefd1\n\n\ud835\udc67, \ud835\udefd2\n\ud835\udc67, \ud835\udefd1\n\n\ud835\udc64 , \ud835\udefd2\n\ud835\udc64} =\n\n{6,2,50,30,50,30,50,30}, {\ud835\udf09\ud835\udc65, \ud835\udf09\ud835\udc66, \ud835\udf09\ud835\udc67} = {\ud835\udc65\ud835\udc56, \ud835\udc66\ud835\udc56 , \ud835\udc67\ud835\udc56}, \ud835\udc531\n\ud835\udc65(\ud835\udc5d) = cosh(\ud835\udc5d) , \ud835\udc532\n\n\ud835\udc65(\ud835\udc5d) = cot(\ud835\udc5d) , \ud835\udc531\n\ud835\udc66\n(\ud835\udc5d) =\n\n\ud835\udc531\n\ud835\udc67(\ud835\udc5d) = \ud835\udc531\n\n\ud835\udc64(\ud835\udc5d) = \ud835\udc5d, \ud835\udc532\n\ud835\udc66\n(\ud835\udc5d) = \ud835\udc532\n\n\ud835\udc64(\ud835\udc5d) = sin(\ud835\udf0b\ud835\udc5d) , \ud835\udc532\n\ud835\udc67(\ud835\udc5d) = exp(\ud835\udf0b\ud835\udc5d),    \ud835\udc541\n\n\ud835\udc65(\ud835\udc4e1) = 15 tanh(\ud835\udc5f\ud835\udc65\ud835\udc56 + \ud835\udc67\ud835\udc56) +\n\nsin(\ud835\udc64\ud835\udc56) + 12 cos(\ud835\udc5f\ud835\udc65\ud835\udc56),   \ud835\udc542\n\ud835\udc65(\ud835\udc4e1) = \u2212 7\ud835\udc5f\ud835\udc66\ud835\udc56 + exp(1 + 2\ud835\udc64\ud835\udc56) + \ud835\udc67\ud835\udc56 + 7 log(\ud835\udf0b\ud835\udc5f\ud835\udc65\ud835\udc56),   \ud835\udc541\n\n\ud835\udc66\n(\ud835\udc4e2) = 2 tan(\ud835\udc5f\ud835\udc65\ud835\udc56 +\n\n\ud835\udc66\ud835\udc56 + 2\ud835\udc67\ud835\udc56 +\ud835\udc64\ud835\udc56),  \ud835\udc542\n\ud835\udc66\n(\ud835\udc4e2) =\ud835\udc67\ud835\udc56 +\ud835\udc64\ud835\udc56 + 14exp(20\ud835\udc5f\ud835\udc65\ud835\udc56), \ud835\udc541\n\n\ud835\udc67(\ud835\udc4e3) =2 tan(\ud835\udc5f\ud835\udc65\ud835\udc56 + \ud835\udc66\ud835\udc56) + \ud835\udc64\ud835\udc56 + \ud835\udc67\ud835\udc56,  \ud835\udc542\n\ud835\udc67(\ud835\udc4e3) =\n\n14 exp(20\ud835\udc5f\ud835\udc65\ud835\udc56 +\ud835\udc64\ud835\udc56) + sin(\ud835\udc67\ud835\udc56), \ud835\udc541\n\ud835\udc64(\ud835\udc4e4) = 2 tan(\ud835\udc5f\ud835\udc65\ud835\udc56 + \ud835\udc66\ud835\udc56 + \ud835\udc67\ud835\udc56) + \ud835\udc64\ud835\udc56, \ud835\udc542\n\n\ud835\udc64(\ud835\udc4e4) = 14 exp(20\ud835\udc5f\ud835\udc65\ud835\udc56 +\ud835\udc64\ud835\udc56) +\n\n\ud835\udc67\ud835\udc56 , \u210e1\n\ud835\udc65(\ud835\udc5d) = sin(2\ud835\udc5d), \u210e2\n\n\ud835\udc65(\ud835\udc5d) = 4\ud835\udc5d, \u210e2\n\ud835\udc66\n(\ud835\udc5d) = cot(\ud835\udc5d),  \u210e1\n\n\ud835\udc66\n(\ud835\udc5d) = \u210e1\n\n\ud835\udc67(\ud835\udc5d) = \u210e1\n\ud835\udc64(\ud835\udc5d) = exp(2\ud835\udc5d), \u210e2\n\n\ud835\udc67(\ud835\udc5d) =\n\n\u210e2\n\ud835\udc64(\ud835\udc5d) = cot(4\ud835\udc5d) . \n\nCase ii:   {\ud835\udefc1\n\ud835\udc65, \ud835\udefc2\n\n\ud835\udc65 , \ud835\udefc1\n\ud835\udc66\n, \ud835\udefc2\n\n\ud835\udc66\n, \ud835\udefc1\n\n\ud835\udc67, \ud835\udefc2\n\ud835\udc67, \ud835\udefc1\n\n\ud835\udc64 , \ud835\udefc2\n\ud835\udc64} = {7,2,4,4,3,5,5,5}, {\ud835\udefd1\n\n\ud835\udc65, \ud835\udefd2\n\ud835\udc65, \ud835\udefd1\n\n\ud835\udc66\n,  \ud835\udefd2\n\n\ud835\udc66\n, \ud835\udefd1\n\n\ud835\udc67, \ud835\udefd2\n\ud835\udc67, \ud835\udefd1\n\n\ud835\udc64, \ud835\udefd2\n\ud835\udc64} =\n\n {1,2,3,3,1,2,2,2}, {\ud835\udf09\ud835\udc65 , \ud835\udf09\ud835\udc66, \ud835\udf09\ud835\udc67} = {\ud835\udc65\ud835\udc56+1, \ud835\udc66\ud835\udc56+1, \ud835\udc67\ud835\udc56+1}, \ud835\udc531\n\ud835\udc65(\ud835\udc5d) = \ud835\udc532\n\n\ud835\udc64(\ud835\udc5d) = cos(\ud835\udc5d) , \ud835\udc532\n\ud835\udc65(\ud835\udc5d) = \ud835\udc532\n\n\ud835\udc66\n(\ud835\udc5d) =\n\n\ud835\udc532\n\ud835\udc67(\ud835\udc5d) = \ud835\udc531\n\n\ud835\udc64(\ud835\udc5d) = \ud835\udc5d, \ud835\udc531\n\ud835\udc67(\ud835\udc5d) = exp(\ud835\udc5d),     \ud835\udc541\n\n\ud835\udc65(\ud835\udc4e1) = sin(\ud835\udc64\ud835\udc56 + \ud835\udc67\ud835\udc56) + \ud835\udc5f\ud835\udc65\ud835\udc56\ud835\udc66\ud835\udc56 , \ud835\udc542\n\ud835\udc65(\ud835\udc4e1) = sin(\ud835\udc5f\ud835\udc66\ud835\udc56 + \ud835\udc65\ud835\udc56) +\n\nlog(7 + \ud835\udc64\ud835\udc56 + \ud835\udc67\ud835\udc56) ,  \ud835\udc541\n\ud835\udc66\n(\ud835\udc4e2) = \ud835\udc5f\ud835\udc65\ud835\udc56 + \ud835\udc66\ud835\udc56 + exp(\ud835\udc5f\ud835\udc65\ud835\udc56+1) + cos(\ud835\udc67\ud835\udc56 +\ud835\udc64\ud835\udc56) , \ud835\udc542\n\n\ud835\udc66\n(\ud835\udc4e2) = \ud835\udc67\ud835\udc56 \u2212\ud835\udc64\ud835\udc56 +\n\nlog(20\ud835\udc5f\ud835\udc65\ud835\udc56+1 + \ud835\udc65\ud835\udc56) , \ud835\udc541\n\ud835\udc67(\ud835\udc4e3) = cot(\ud835\udc5f\ud835\udc65\ud835\udc56+1 + \ud835\udc66\ud835\udc56+1) + sin(\ud835\udc65\ud835\udc56 +\ud835\udc64\ud835\udc56\ud835\udc67\ud835\udc56) , \ud835\udc542\n\n\ud835\udc67(\ud835\udc4e3) = exp(\ud835\udc65\ud835\udc56 +\ud835\udc64\ud835\udc56 + \ud835\udc67\ud835\udc56) +\nsin(\ud835\udc65\ud835\udc56+1 + \ud835\udc66\ud835\udc56+1), \ud835\udc541\n\n\ud835\udc64(\ud835\udc4e4) = 2 cot(\ud835\udc5f\ud835\udc65\ud835\udc56+1 + \ud835\udc66\ud835\udc56+1 + \ud835\udc67\ud835\udc56+1) + log (\ud835\udc65\ud835\udc56 +\ud835\udc64\ud835\udc56), \ud835\udc542\n\ud835\udc64(\ud835\udc4e4) = exp(\ud835\udc5f\ud835\udc66\ud835\udc56+1 +\n\n\ud835\udc65\ud835\udc56+1 + 2\ud835\udc64\ud835\udc56) + \ud835\udc66\ud835\udc56 + \ud835\udc67\ud835\udc56 , \u210e1\n\ud835\udc65(\ud835\udc5d) = cos(20\ud835\udc5d), \u210e2\n\n\ud835\udc65(\ud835\udc5d) = 5\ud835\udc5d, \u210e1\n\ud835\udc66\n(\ud835\udc5d) = log(4\ud835\udc5d), \u210e2\n\n\ud835\udc66\n(\ud835\udc5d) = cos(6\ud835\udc5d),  \u210e1\n\n\ud835\udc67(\ud835\udc5d) =\n\u210e1\n\ud835\udc64(\ud835\udc5d) = exp(4\ud835\udc5d), \u210e2\n\n\ud835\udc67(\ud835\udc5d) = \u210e2\n\ud835\udc64(\ud835\udc5d) = cos(\ud835\udc5d) . \n\n \n\nAlso, in the case i, {\ud835\udc6d\ud835\udfcf\n\ud835\udc99, \ud835\udc6d\ud835\udfcf\n\n\ud835\udc9b , \ud835\udc6d\ud835\udfd0\n\ud835\udc9b}  are considered as Tent map, and {\ud835\udc6d\ud835\udfd0\n\n\ud835\udc99, \ud835\udc6d\ud835\udfcf\n\ud835\udc9a\n, \ud835\udc6d\ud835\udfd0\n\n\ud835\udc9a\n, \ud835\udc6d\ud835\udfd0\n\n\ud835\udc9b} are considered as Sin \n\nmap. For case ii, {\ud835\udc6d\ud835\udfcf\n\ud835\udc99, \ud835\udc6d\ud835\udfcf\n\n\ud835\udc9b} and {\ud835\udc6d\ud835\udfd0\n\ud835\udc99, \ud835\udc6d\ud835\udfcf\n\n\ud835\udc9a\n, \ud835\udc6d\ud835\udfd0\n\n\ud835\udc9a\n} are considered as Tent and Sin maps, respectively. \n\n2.1 Chaotic Behavior Analysis \n\n      In this subsection same important tests for the proposed chaos system are discussed. One of the \n\nimportant values in the study of the behavior of the chaos system is Lyapunov exponent or Lyapunov \n\ncharacteristic exponent. A \ud835\udc5b-dimensional chaos systems in general have \ud835\udc5b values for Lyapunov exponent. \n\nThere are many methods for calculating this value as [1-3]. The method based on QR algorithm has been \n\nused for obtained Lyaponov exponent in Fig. 3 for the case i. More details about this method can be found \n\nin [3]. The positive or negative values of the resulting values are related with the structure of a chaos \n\nsystem. This relation had been studied in many papers. In [4], the relation has been given as follows \u2018\u2019 In \n\nan n-dimensional dynamical system we have \ud835\udc5b Lyapunov exponents. Each \ud835\udf06\ud835\udc58 represents the divergence of \n\n\ud835\udc58-volume. The sign of the Lyapunov exponents indicates the behavior of nearby trajectories. A negative \n\nexponent indicates that neighboring trajectories converge to the same trajectory A positive exponent \n\nindicates that neighboring trajectories diverge [4]\u2019\u2019. Also the following theorem has been given in [5] for \n\nthis value. \n\nTheorem 2.1 If at least one of the average Lyapunov exponents is positive, then the system is chaotic; if \n\nthe average Lyapunov exponent is negative, then the orbit is periodic and when the average Lyapunov \n\nexponent is zero, a bifurcation occurs. \n\nThe results in Fig. 3 show that all four value of Lyapunov exponent in the proposed system are positive. \n\nThen by using above studies, we can say that in the proposed system in the all neighboring trajectories \n\ndiverge. In order to compare proposed system, the Lyapunov exponent has been compared with 4D \n\nChaotic Laser System [6] in Fig. 3. It is observed that the proposed system has better chaos behavior then \n\nChaotic Laser System. \n\n\n\nAnother tool for study chaotic behavior is bifurcation analysis. In the Fig. 4, the results for the bifurcation \n\nanalysis of the case ii of the proposed system have been shown.  The chaotic attractors can be studied by \n\nthis figure. The attractor for \ud835\udc5f \u2208 (0 ,1.2] is given in the vertical line at that \ud835\udc5f. Also, the cobweb plot (or \n\nVerhulst diagram) for case i have been given in the Fig.s 5. By using this results, it is observed that for the \n\ngiven values, the resulting sequences of the proposed system has chaotic behavior. \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nDistribution is another important factor in chaotic system. One of the reasons for the weakness of the \n\nstatistical attack is nonuniform distribution. The histogram plots of the proposed system for the case i are \n\nFigure 3. Lyapunov exponent values for: (a) Case i, (b) Chaotic system in [6].   \n\nFigure 4. Bifurcation diagram results for the case ii.   \n\n(a) (b) \n\nhttps://en.wikipedia.org/wiki/Chaos_(mathematics)\nhttps://en.wikipedia.org/wiki/Attractor\nhttps://en.wikipedia.org/wiki/Attractor\n\n\ngiven in the Fig. 6. Also, the distribution patterns of the case ii are shown in Fig. 7. By using these results, \n\nit can be seen that the generated sequence of the proposed maps have a flat distribution.  \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n\n\n \n\n \n\n \n\n \n\n \n\nReference  \n\n[1] Wolf, Alan, Jack B. Swift, Harry L. Swinney, and John A. Vastano. Determining Lyapunov exponents \n\nfrom a time series. Physica D: Nonlinear Phenomena 16, no. 3 (1985): 285-317. \n\nFigure 6. Histogram plots of the case i for \ud835\udc5f = 0.5.   \n\nFigure 6. Distribution patterns of the case ii for \ud835\udc5f = 0.4.   \n\n\n\n[2] Sano, Masaki, and Yasuji Sawada. Measurement of the Lyapunov spectrum from a chaotic time series. \n\nPhysical review letters 55, no. 10 (1985): 1082. \n\n[3] Dmitrieva, Lyudmila A., Yuri A. Kuperin, Nikolai M. Smetanin, and German A. Chernykh. Method of \n\ncalculating Lyapunov exponents for time series using artificial neural networks committees In Days on \n\nDiffraction (DD), 2016, pp. 127-132. IEEE, 2016 \n\n[4] Van Opstall, Michael. Quantifying Chaos in Dynamical Systems with Lyapunov Exponents. Furman \n\nUniversity Electronic Journal of Undergraduate Mathematics 4, no. 1 (1998): 1-8. \n\n[5] Lynch, Stephen. Dynamical systems with applications using Matlab\u00ae. Springer, 2014. \n\n[6] Natiq, Hayder, Mohamad Said, Nadia Al-Saidi, and Adem Kilicman. Dynamics and complexity of a \nnew 4d chaotic laser system. Entropy 21, no. 1 (2019): 34. \n\n \n\n \n\n \n\n \n\n \n\n\n"
